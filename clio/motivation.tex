
%\input{tbl-system-cmp}

\section{Goals and Related Works}
\label{sec:motivation}

Resource disaggregation 
% is a notion to 
separates different types of resources into different pools,
each of which can be independently managed and scaled.
Applications can allocate resources from any node in a resource pool, resulting in tight resource packing. %regardless of where other types of resources sit.
Because of these benefits, % its manageability, independent scaling, and efficient resource utilization benefits,
many datacenters have adopted the idea of disaggregation, often at the storage 
layer~\cite{FACEBOOK-BRYCECANYON,FB1,SnowFlake-NSDI20,AMAZON-S3,AMAZON-EBS,Pangu,FC-SAN-book}.
%layer~\cite{FACEBOOK-BRYCECANYON,FB1,SnowFlake-NSDI20,Decibel-NSDI17,AMAZON-S3,AMAZON-EBS,Klimovic18-ATC,RackScaleDisaggregation}.
%As a recent success story, Alibaba listed their storage disaggregation as one of the five reasons
%that enable them to serve the peak load of 544,000 orders per second on the 2019 Single's Day~\cite{Ali-SinglesDay}.
With the success of disaggregated storage, % and the fast developments in datacenter network speed,
researchers in academia and industry have also sought ways to disaggregate memory
(and persistent memory)
\cite{Lim09-disaggregate,FireBox-FASTKeynote,IntelRackScale,Lim12-HPCA,Shan18-OSDI,Shan17-SOCC,RAMCloud,Tsai20-ATC,AIFM,FastSwap,InfiniSwap,Semeru,Nitu18-EUROSYS}.
%For example, even though storage disaggregation has been well understood and widely deployed~\cite{SnowFlake-NSDI20,Pangu,FACEBOOK-BRYCECANYON,FB1,XXX},
Different from storage disaggregation,
\md\ needs to achieve at least an order of magnitude higher performance and it should offer a byte-addressable interface.
Thus, \md\ poses new challenges and requires new designs.
%demands much more stringent performance targets, which in turn has new implications on how scalability and low cost can be achieved.
%Moreover, the memory interface is very different from a storage interface, requiring new designs in \md's abstraction and metadata management.
%Different from distributed memory such as distributed shared memory~\cite{Shasta,Bennett90-PPOPP,Fleisch89-SOSP,Keleher92-ISCA,Nelson15-ATC},
%memory disaggregation, or {\em \md}, has a clear notion of function separation where the memory pool hosts memory
%and the compute pool executes applications.
This section discusses the requirements of \md\ and why existing solutions cannot fully meet them.
%This section, Table~\ref{tbl-system-cmp}, and Figure~\ref{fig-arch} put \sys\ in the perspective of two main existing memory disaggregation approaches.
%architecture of disaggregated memory.

\subsection{\md\ Design Goals}
\label{sec:requirements}
In general, \md\ has the following features, some of which are hard requirements while others are desired goals.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Hosting large amounts of memory with high utilization.}
To keep the number of memory devices and total cost of a cluster low,
each \MN\ should host hundreds GBs to a few TBs of memory that is expected to be close to fully utilized.
To most efficiently use the disaggregated memory, we should allow applications to create and access {\em disjoint} memory regions of arbitrary sizes at \MN.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Supporting a huge number of concurrent clients.}
To ensure tight and efficient resource packing,
%To fully exploit the resource-utilization benefit of disaggregation, 
we should allow many (\eg, thousands of) client processes running on tens of \CN{}s to access and share an \MN.
This scenario is especially important for new data-center trends like serverless computing and microservices where applications run as large amounts of small units.
%Thus, disaggregated memory should scale with client servers, client processes, memory size, and memory regions.
%each client process should be able to access large amounts of memory and many disjoint memory regions 
%scale with client servers, client processes, memory size, memory regions

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low-latency and high-throughput.}
We envision future systems to have a new memory hierarchy, where disaggregated memory is larger and slower than local memory but still faster than storage.
Since \md\ is network-based, a reasonable performance target of it is to match the state-of-the-art network speed,
\ie, 100\Gbps\ throughput (for bigger requests) and sub-2\mus\ median end-to-end latency (for smaller requests).
%When used as dynamically allocated memory, disaggregated memory should deliver low latency so as not to slow down application execution.
%When it is used as (cache of) data store, throughput is more important, 
%and accesses to disaggregated memory should reach network line rate.
 
\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low tail latency.}
%Many workloads in datacenters have tight SLOs. 
Maintaining a low tail latency is important in meeting service-level objectives (SLOs) in data centers.
%Even though accesses to \MN{}s are much rarer than local memory accesses at \CN{}s (\eg, because \CN{}s cache remote data locally), %Although \CN{}s could cache data at their local memory to reduce the amount of remote memory accesses,
Long tails like RDMA's 16.8\ms\ remote memory access can be detrimental to applications that are short running (\eg, serverless computing workloads) or have large fan-outs or big DAGs
(because they need to wait for the slowest step to finish)~\cite{taillatency}.
%Within a rack, the network wire delay is short (~XXX\mus) and stable.
%Other factors such as memory access delays, software delays, and network queueing delays are the major factors in the tail.
%One of the obstacles to adopting disaggregated memory is the fear for unpreditable performance and much worse tail latency than local memory. %, \eg, during network congestion
%In addition to network unstability, we found that memory systems could also contribute to long tails with today's RDMA-based systems (\S\ref{sec:}).
%Having predictable, bounded performance and low tail latency would be one key to the successful adoption of memory disaggregation in data centers,
%where SLAs are important.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Protected memory accesses.}
%When deployed in datacenters, it is important to protect disaggregated memory from undesired or malicious accesses.
As an \MN{} can be shared by multi-tenant applications running at \CN{}s, % and applications from different tenants over the network,
we should properly isolate memory spaces used by them.
Moreover, to prevent buggy or malicious clients from reading/writing arbitrary memory at \MN{}s, we should not allow the direct access of \MN{}s' physical memory from the network and \MN{}s should check the access permission.
%we should properly protect and isolate memory accesses to \MN{}s
%(\eg, by not allowing \CN{}s to directly access \MN{} physical memory over the network).
%and should not allow \CN{}s to directly access physical memory at \MN{}s.
%they should have at least the same level of safety guarantees as today's normal (local) memory
%and potentially also guard against new security threasts such as side channel attacks over the network~\cite{Tsai19-Security}.
%when shared across side channel attacks~\cite{Meltdown,Spectre}

\if 0
\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Computation offloading support.}
Accessing remote memory over the network is inevitably slower than accessing local memory over the memory bus, esp. on latency.
Caching remote memory locally is one way to avoid network delays.
However, \CN{}s are intended to host many applications but with limited local memory, making it hard to cache all the data.
A viable solution is to shift some computation to \MN{}s, which could save \CN{} cache space, reduce \CN{}'s CPU utilization, and reduce network bandwidth consumption.
Others have identified the benefits of computation offloading to remote memory and disaggregated storage as well~\cite{XXX}.
\fi

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low cost.}
A major goal and benefit of resource disaggregation is cost reduction.
A good \md\ system should have low {\em overall} CapEx and OpEx costs.
Such a system thus should not 1) use expensive hardware to build \MN{}s, 
2) consume huge energy at \MN{}s,
and 3) add more costs at \CN{}s than the costs saved at \MN{}s.
%1) means that server-based \MN\ designs are not ideal, and a hardware-based \MN\ design should make careful choice to keep the hardware resource consumption low;
%2) implies that CPU-based \MN\ solutions are likely to be too power hunger, and low-power solutions that run too slow cannot work either;
%and 3) implies that 
%Previous work~\cite{Shan18-OSDI} has shown that \md\ could improve memory resource utilization by around 50\% 
%(\ie, a \md\ cluster only needs to host half of the memory compared to a non-disaggregated cluster).
%This means that %1) a \md\ system should aim to have close-to-full utilization of its memory and have minimal memory waste, and 2) 
%building and running an \MN\ should not double the cost of hosting memory, as such a \md\ system would cost even more than no disaggregation.
%Using a server to build an \MN\ is thus not a good option, 
%since a server box
%and its CPU
%costs more than the DRAM it hosts.
%Another cost-related goal of \md\ is to have as much memory used by applications as possible (\ie, minimal memory wastes).
%However, if adding an \MN{} to host a certain amount of memory doubles the CapEx and OpEx cost, there will be no cost saving.
%This indicates that 
%Although disaggregated memory pool can be built with regular servers, 
%(in fact, most of today's disaggregated memory solutions~\cite{AIFM,FarMem,InfiniSwap,Semeru} are server-based),
%it is much cheaper to build and run standalone disaggregated memory devices without a server box or a CPU.


\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Flexible.}
With the fast development of datacenter applications, hardware, and network, a sustainable \md\ solution should be flexible and extendable,
for example, to support high-level APIs like pointer chasing~\cite{AIFM,Aguilera-FarMemory},
to offload some application logic to memory devices~\cite{AIFM,StRoM},
or to incorporate different network transports~\cite{Homa,NDP,TONIC} and congestion control algorithms~\cite{swift-sigcomm,1RMA,hpcc-sigcomm19}.

\if 0
\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Ease of deployment and management.}
To ease the adoption of \md, it is desirable to have a solution 
that needs no changes to existing datacenter infrastructure such as network switches.
It is also desirable to have a \md\ solution that is easy to manage and configure.
\fi
%
%\stepcounter{reqs}
%\boldpara{R\arabic{reqs}: Ease of management.}
%A major reason behind the successful adoption of disaggregated storage at production scale and the traction of disaggregation in general
%is its ease of management, as it is more flexible to manage disaggregated resource pools independently at where the resource is.
%Memory disaggregation should also follow the practice by managing the disaggregated memory pool.
%Datacenter operators can then configure disaggregated memory pool (\eg, adding replication, migrating data across memory nodes)
%without affecting other pools.


%\stepcounter{reqs}
%\boldpara{R\arabic{reqs}: Easy and cheap to deploy.}
%opportunity
%asymetric
%application aware of disaggregated memory

\subsection{Server-Based Disaggregated Memory}
%RDMA-Based and Messaging-Based Disaggregated Memory}
\label{sec:rdma}

\md\ research so far has mainly taken a server-based approach by using regular servers as \MN{}s~\cite{InfiniSwap,FastSwap,Semeru,Shan18-OSDI,AIFM,zombieland,FaRM},
usually on top of RDMA.
%which are connected to \CN{}s with RDMA or TCP~\cite{}.
%The second approach, taken by most disaggregated and remote memory solutions,
%is using a virtual memory abstraction provided by host server at \MN{}s
%and build their own layer on top of RDMA~\cite{InfiniSwap,FastSwap,Semeru} or TCP~\cite{AIFM}.
%many server-based disaggregated memory systems,
%uses the abstraction provided by a network layer 
%They use RDMA~\cite{InfiniSwap,FastSwap,Semeru} or TCP~\cite{AIFM} as the communication layer.
%As discussed in \S\ref{sec:intro}, 
The common limitation of these systems is their reliance on a host server and the resulting CPU energy costs, both of which violate \textbf{R6}.
%RDMA and TCP's symmetric architecture and connection-based, reliable transports are ill fit for memory disaggregation.
%Each of them also has their own problems.
%\md\ solutions that use TCP or similar transports incur high performance overhead because of costly network stack and memory copy, violating \textbf{R3}.
%other messaging-based disaggregated memory systems first copy application objects to messages at \CN{}s
%and then from messages to memory locations at \MN{}s.
%These systems not only incur the performance overhead of memory copies 
%but also require intensive CPU cycles to run the transport layer,
%making them unfit for memory disaggregation.
%These memory copies not only add performance overhead but also 
%RDMA is a high-speed, zero-copy, low CPU-utilization network technology 
%that has been adopted by several major datacenters~\cite{Microsoft,Alibaba}.

\ulinebfpara{RDMA} is what most server-based \md\ solutions are based on, with some using RDMA for swapping memory between \CN{}s and \MN{}s~\cite{InfiniSwap,FastSwap,Semeru} and some using RDMA for explicitly accessing \MN{}s~\cite{AIFM,zombieland,FaRM}.
%is more efficient than TCP-like transports, % avoids memory copying and can bypass CPU and OS kernel for most operations.
%but it still relies on the host server to run a virtual memory system,
Although RDMA has low average latency and high throughput, it has a set of scalability and tail-latency problems.
% is not a scalable or low-cost way to build disaggregated memory systems
%because of its reliance on the host CPU, MMU, and OS to run a virtual memory system.
%a host server box (CPU, MMU, and OS).
%RDMA's main issue when used for memory disaggregation is its reliance on host CPU, MMU, and OS
%and its limited scalability.
%RDMA relies on a host server's virtual memory system to manage the memory in this server.
%(\eg, for virtual and physical memory allocation, address translation, and page fault handling).
%Thus, \MN{}s can only be operated with a host server box.

A process ($P_M$) running at an \MN\ needs to allocate memory in its virtual memory address space 
and {\em register} the allocated memory (called a memory region, or MR) with the RDMA NIC (RNIC).
The host OS and MMU set up and manage the page table that maps $P_M$'s virtual addresses ({\em VA}s) to physical memory addresses ({\em PA}s).
%Client applications use $P_M$'s virtual memory address ($P_M{\text -}VA$) and MR information to perform RDMA read/write.
%On the \CN-side, there needs to be an extra level of indirection to first translate application abstraction to the $P_M{\text -}VA$ and MR combination.
%On the \MN-side, RNICs rely on host OS and MMU to manage page tables that map $P_M{\text -}VA$ to $PA$ (physical memory address).
To avoid always accessing host memory for address mapping, RNICs cache page table entries (PTEs),
but when more PTEs are accessed than what this cache can hold, RDMA performance degrades significantly (Figure~\ref{fig-pte-mr} and \cite{FaRM,Tsai17-SOSP}).
Similarly, RNICs cache MR metadata and incur degraded performance when the cache is full. 
Thus, RDMA has serious performance issues with either large memory (PTEs) or many disjoint memory regions (MRs), violating \textbf{R1}.
Moreover, RDMA uses a slow way to support on-demand allocation: the RNIC interrupts the host OS for handling page faults.
From our experiments, a faulting RDMA access is 14100\x\ slower than a no-fault access (violating \textbf{R4}).
%Page faults happen at the initial accesses to allocated virtual memory addresses, 
%causing long tails (violating \textbf{R4}).
%Slow initial accesses have a big impact on applications like serverless computing, which run short but very frequently.
%to handle page faults, which is extremely slow (14100\x\ slower than a no-fault access from our experiments).

To mitigate the above performance and scalability issues, most RDMA-based systems today~\cite{FaRM,Tsai17-SOSP} 
preallocate a big MR with huge pages and pin it in physical memory.
This results in inefficient memory space utilization and violates \textbf{R1}.
Even with this approach, there can still be a scalability issue (\textbf{R2}),
as RDMA needs to create at least one MR for each protection domain (\ie, each client).

In addition to problems caused by RDMA's memory system design, reliable RDMA, the mode used by most \md\ solutions, suffers from a connection queue pair (QP) scalability issue, also violating \textbf{R2}.
Finally, today's RNICs violate \textbf{R7} because of their rigid one-sided RDMA interface and the close-sourced, hardware-based transport implementation.
Solutions like 1RMA~\cite{1RMA} and IRN~\cite{IRN} mitigate the above issues by either onloading part of the transport back to software or proposing a new hardware design.

\ulinebfpara{LegoOS}~\cite{Shan18-OSDI}, our own previous work, is a distributed operating system designed for resource disaggregation.
Its \MN{} includes a virtual memory system that maps VAs of application processes running at \CN{}s to \MN\ PAs. \sys's \MN{} performs the same type of address translation.
%also advocates for managing memory at where memory is (called mComponent in LegoOS).
% is the only disaggregated memory system that adopts this abstraction and manages memory at \MN{}s.
However, LegoOS emulates \MN\ devices using regular servers and we built its virtual memory system in software,
which has a stark difference from a hardware-based virtual memory system. 
%In fact, we started our virtual memory design from LegoOS's but ended up finding 
%that none of its design or implementation fit a hardware environment. 
For example, LegoOS uses a thread pool that handles incoming memory requests by looking up a hash table for address translation and permission checking.
This software approach is the major performance bottleneck in LegoOS (\S\ref{sec:results}),
violating \textbf{R3}.
%have to redesign every factor of it.
%still relies on host CPU and cannot be easily adapted to a hardware-based virtual memory system.
Moreover, LegoOS %has no computation offloading support, 
%and it 
uses RDMA for its network communication hence inheriting its limitations.

\subsection{Physical Disaggregated Memory}
\label{sec:pdm}

One way to build \md\ without a host server is to treat it as raw, physical memory,
%\ie, client servers send read/write request with physical memory address, and the memory node directly read/write to that address without any address mapping.
a model we call {\em \pdm}.
The \pdm\ model has been adopted by a set of coherent interconnect proposals~\cite{Genz-citation,CXL-citation},
HPE's Memory-Driven Computing project~\cite{HP-TheMachine,THEMACHINE-HOTOS,HP-MODC-POSTER,THEMACHINE-WEB}.
A recent disaggregated hashing system~\cite{race-atc21} and our own recent work on disaggregated key-value systems~\cite{Tsai20-ATC} also adopt the \pdm\ model and emulate remote memory with regular servers.
%two RDMA-based software systems~\cite{Tsai17-SOSP,Tsai20-ATC} 
%(these systems use the OS kernel to register physical memory directly, which is a special RDMA configuration).
To prevent applications from accessing raw physical memory,
these solutions add an indirection layer at \CN{}s in hardware~\cite{Genz-citation,CXL-citation} or software~\cite{Tsai20-ATC,race-atc21}
to map client process VAs or keys
to \MN\ PAs. 

There are several common problems with all the \pdm\ solutions.
First, because \MN{}s in \pdm\ are raw memory, \CN{}s need multiple network round trips to access an \MN\ 
for complex operations like pointer chasing and concurrent operations that need synchronization~\cite{Tsai20-ATC}, violating \textbf{R3} and \textbf{R7}.
Second, \pdm\ requires the client side to manage disaggregated memory.
For example, \CN{}s need to coordinate with each other or use a global server~\cite{Tsai20-ATC} to perform tasks like memory allocation.
Non-\MN-side processing is much harder, performs worse compared to memory-side management (violating \textbf{R3}), and could even result in higher overall costs because of the high computation added at \CN{}s (violating \textbf{R6}).
%and when some memory needs to be migrated from one \MN\ to another (\eg, for load balancing), 
%all the \CN{}s that have mapped this memory need to update their mappings.
%Such complex client-side memory management defeats the purpose and benefits of disaggregation (\textbf{R5}).
Third, exposing physical memory makes it hard to provide security guarantees (\textbf{R5}),
as \MN{}s have to authenticate that every access is to a legit physical memory address belonging to the application.
%\MN{}s have to trust that \CN{}s will never access beyond their allocated physical memory regions. 
%coordination across client servers (\eg, through a global controller~\cite{Tsai20-ATC} or a distributed consensus system).
%Fourth, it is difficult to build compute offloads on physical memory and unsafe to run them without memory protection.
Finally, all existing \pdm\ solutions require physical memory pinning at \MN{}s, causing memory wastes and violating \textbf{R1}.
%it is also unclear how page faults will be handled.

In addition to the above problems, none of the coherent interconnects or HPE's Memory-Driven Computing have been fully built.
When they do, they will require new hardware at all endpoints and new switches. 
Moreover, the interconnects automatically make caches at different endpoints coherent, which could cause performance overhead that is not always necessary (violating \textbf{R3}).

Besides the above \pdm\ works, there are also proposals to include some processing power in between the disaggregated memory layer and the computation layer.
soNUMA~\cite{soNUMA} is a hardware-based solution that scales out NUMA nodes by extending each NUMA node with a hardware unit that services remote memory accesses.
Unlike \sys\ which physically separates \MN{}s from \CN{}s across generic data-center networks, soNUMA still bundles memory and CPU cores, and it is a single-server solution.
Thus, soNUMA works only on a limited scale (violating \textbf{R2}) and is not flexible (violating \textbf{R7}).
MIND~\cite{mind:sosp21}, a concurrent work with \sys, proposes to use a programmable switch for managing coherence directories and memory address mappings between compute nodes and memory nodes.
Unlike \sys\ which adds processing power to every \MN, MIND's single programmable switch has limited hardware resources and could be the bottleneck for both performance and scalability.


%Recently, new cache-coherent interconnects like Gen-Z~\cite{Genz-citation} and CXL~\cite{CXL-citation}
%have been proposed to connect heterogeneous components in a rack~\cite{Genz-citation,CXL-v2} or in a server~\cite{CXL-citation,CCIX,OpenCAPI}.
%for single-machine or rack scale memory accesses.
%They usually expose a global memory space (that covers memory and accelerator memory) to applications running at the compute (client) 
%side and guarantee cache coherence among different compute and memory components.

\if 0
\subsection{Takeaway}
The fundamental issues of server-based approaches such as RDMA systems are the monetary cost of a host server and the inherent performance and scalability limitations caused by the way NICs interact with the host server's virtual memory system.
Although \pdm\ solutions solve the first (cost) issue, they introduce more performance, security, and management problems
because \MN{}s become too ``dumb'' and low level when removing its processing power altogether.
%overhead of root cause why RDMA is unfit for \md\ is that it is designed to work around a host server,
%and traditional servers' virtual memory system is not designed for \md.
%Contrary to server-based \md\ solutions, \pdm\ is another extreme where there is no processing at all at \MN{}s.
%The root cause of \pdm's various performance, security, and management problems is exactly this: 
%\MN{}s are too ``dumb'' and low level.

%From the above discussion, we find that both server-based \md\ and raw physical \md\ have their limitations.
Server-based \MN{}s and \MN{}s with no processing power are two extreme approaches of building \MN{}s.
This work tries to find a sweet spot in the middle by building a hardware-based \md\ solution that has the right amount of processing power at \MN{}s.
%for the right type of \MN{} management system.
%To mitigate RDMA's various issues for \md, one could try to improve RDMA's hardware design or add a software layer to work around RDMA's problems.
%This paper takes a different approach by starting
Furthermore, we take a clean-slate approach by starting from the requirements of \md\
and designing a \md-native system.
%LegoOS is the only existing work that also proposes a .
%Unfortunately, LegoOS still uses server software and the RDMA network to emulate its hardware design, leaving all the research questions we ask in this paper unanswered.
\fi

\if 0
\subsection{Our Proposal: Hardware-Based Virtual Disaggregated Memory Device}

From the above discussion, we find that both server-based \md\ and raw physical \md\ have their limitations.
This work tries to find a sweet spot in the middle by building a hardware-based \md\ solution that has the right amount of processing power at \MN{}s
used to build the right type of \MN{} management system.
\fi

%\subsection{Active Disaggregated Memory}
%\label{sec:offload}



\if 0
\subsection{Requirements of Memory Disaggregation}
Many applications can benefit from disaggregated memory, and their usages can be categorized into two types:
as dynamically allocated memory space (\eg, as part of a heap~\cite{Semeru}, as a swap space~\cite{InfiniSwap}) during the execution of an application,
and as the cache of a data store~\cite{XXX} or the data store itself~\cite{RAMCloud}.
In general, memory disaggregation has the following requirements.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low cost and easy deployment.}
Although disaggregated memory pool can be built with regular servers 
(in fact, most of today's disaggregated memory solutions~\cite{AIFM,FarMem,InfiniSwap,Semeru} are server-based),
it is much cheaper to build and run standalone disaggregated memory devices without a server box or a CPU.
To ease the adoption of disaggregated memory in current datacenters, ideally, there should be no hardware change or addition at client servers.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low-latency and high-throughput.}
Accesses to disaggregated memory are over the network, which is inevitably slower than accessing local memory.
Despite this nature, memory disaggregation should still offer attractive performance, and by offering more (but slower) memory,
overall application performance could be better than with just local memory.
Specifically, when used as dynamically allocated memory, disaggregated memory should deliver low latency so as not to slow down application execution much.
When it is used as (cache of) data store, throughput is more important, 
and accesses to disaggregated memory should reach network line rate.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low tail latency and predictable performance.}
One of the obstacles to adopting disaggregated memory is the fear for unpreditable performance and much worse tail latency than local memory. %, \eg, during network congestion
In addition to network unstability, we found that memory systems could also contribute to long tails with today's RDMA-based systems (\S\ref{sec:}).
Having predictable, bounded performance and low tail latency would be one key to the successful adoption of memory disaggregation in datacenters,
where SLAs are important.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Scale with clients and memory.}
To fully exploit the resource-utilization benefit of disaggregation, we should allow many client servers and client processes running on them to access and share a memory device.
At the same time, each memory device could host hundreds GBs to few TBs of memory that is expected to be close to fully utilized (and thus bringing down the number of 
memory devices and total cost of a cluster).
Furthermore, we should have no restriction on how applications use the memory, which could result in many disjoint memory regions.
Thus, disaggregated memory should scale with client servers, client processes, memory size, and memory regions.
%each client process should be able to access large amounts of memory and many disjoint memory regions 
%scale with client servers, client processes, memory size, memory regions

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Independent resource pool management.}
A major reason behind the successful adoption of disaggregated storage at production scale and the traction of disaggregation in general
is its ease of management, as it is more flexible to manage disaggregated resource pools independently at where the resource is.
Memory disaggregation should also follow the practice by managing the disaggregated memory pool.
Datacenter operators can then configure disaggregated memory pool (\eg, adding replication, migrating data across memory nodes)
without affecting other pools.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Secure access.}
When deployed in datacenters, it is important to protect disaggregated memory from undesired or malicious accesses.
Disaggregate memory can be shared by many client servers and applications over the network.
It should provide at least the same level of safety guarantees as today's normal (local) memory
and potentially also guard against new security threasts such as side channel attacks over the network~\cite{Tsai19-Security}.
%when shared across side channel attacks~\cite{Meltdown,Spectre}

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Efficient, easy, and flexible computation offloading.}
%\boldpara{R\arabic{reqs}: Extendible.}
%With the fast development of datacenter applications, hardware, and network, a good memory disaggregation solution should be flexible and extendible,
for example, to support more advanced APIs like pointer chasing~\cite{StRoM,AIFM},
to offload some application logic to memory devices~\cite{AIFM,XXX},
or to plug different network transports~\cite{Homa,NDP,pFabric,XXX} and congestion control algorithms~\cite{Swift,XXX}.

%\stepcounter{reqs}
%\boldpara{R\arabic{reqs}: Easy and cheap to deploy.}
%opportunity
%asymmetric
%application aware of disaggregated memory
\fi


\if 0
%The reason behind the \pdm\ model's issues is its physical abstraction of disaggregated memory.
%Thus, we believe that disaggregate memory should expose a \textit{virtualized} memory abstraction,
%and memory management and address mapping should happen at the memory pool, not the compute pool (client servers).
%By hiding the physical nature of disaggregated memory under a virtual abstraction,
%disaggregated memory can have the same benefits as what traditional virtual memory system provide, 
%such as on-demand allocation (paging) and protected accesses across different applications.
%By performing memory management at the memory node, we can avoid orchestration across client servers
%and transparently perform tasks like replication and migration.
%Both \sys\ and RDMA offer a virtualized disaggregated (remote) memory abstraction.
%However, the virtualized view of them are completely different, and as we will show next, RDMA has many issues when used for memory disaggregation.
%As we will show shortly, RDMA cannot meet these requirements and is an ill fit for memory disaggregation.


%This section discusses related topics 
%and motivates our work. %existing proposals and attempts to build remote memory
%and motivates programmable hardware-based disaggregated memory.
%We also cover related works in this section.

\subsection{Non-Disaggregated Remote Memory}
\label{sec:remotemem}

The idea of using non-local machine's memory dates back to the early 90s when research in 
distributed shared memory (DSM)~\cite{Shasta,Bennett90-PPOPP,Fleisch89-SOSP,Keleher92-ISCA} 
and distributed operating systems~\cite{Sprite88,Amoeba90} boomed.
Recent years have seen a rise of new interests in this idea because of datacenter network improvements~\cite{Aguilera17-SOCC}.
Several RDMA-based distributed memory systems have been proposed %in recent years
including Grappa~\cite{Nelson15-ATC}, FaRM~\cite{FaRM}, and Hotpot~\cite{Shan17-SOCC}.
%There are mainly two forms of using non-local (remote) machine's memory.
These systems use a machine's memory for both applications running locally and on remote machines.
Each machine manages its own memory and runs application processes.
Similar to traditional DSM systems, these systems require complex and costly consistency protocols for concurrent accesses.
Moreover, % important, %memory in these systems is not disaggregated 
they lack the benefits of disaggregation. % (see below).
%The second type reserves some memory space exclusively for remote accesses.
%To use these systems in a cluster setting, another distributed software system is usually needed on top of them.

\subsection{Resource Disaggregation}
\label{sec:disaggregation}

Resource disaggregation is a notion to separate different types of resources into different pools,
each of which can be independently managed and scaled.
Applications can allocate resources from any available node in a resource pool, resulting in tight resource packing. %regardless of where other types of resources sit.
Because of these benefits, % its manageability, independent scaling, and efficient resource utilization benefits,
many datacenters have adopted the idea of disaggregation, often at the storage 
layer~\cite{FACEBOOK-BRYCECANYON,FB1,SnowFlake-NSDI20,AMAZON-S3,AMAZON-EBS,Ali-SinglesDay}.
%layer~\cite{FACEBOOK-BRYCECANYON,FB1,SnowFlake-NSDI20,Decibel-NSDI17,AMAZON-S3,AMAZON-EBS,Klimovic18-ATC,RackScaleDisaggregation}.
%As a recent success story, Alibaba listed their storage disaggregation as one of the five reasons
%that enable them to serve the peak load of 544,000 orders per second on the 2019 Single's Day~\cite{Ali-SinglesDay}.

With the success of disaggregated storage and the fast developments in datacenter network speed,
researchers in both academia and industry started to seek ways to disaggregate memory 
(and persistent memory)~\cite{Lim09-disaggregate,FireBox-FASTKeynote,IntelRackScale,Lim12-HPCA,Shan18-OSDI,RAMCloud,Tsai20-ATC}.
Disaggregated memory proposals so far have taken two main approaches.
The first type treats the disaggregated memory layer as a {\em passive} party (raw, physical memory space with no processing power);
the compute pool manages the memory pool and provides high-level services to applications~\cite{HP-TheMachine,Tsai20-ATC,Lim09-disaggregate,Nitu18-EUROSYS}.
%that requires the compute pool for further management and high-level interface, 
The main problem of passive disaggregation is the excessive network RTTs needed when completely separating processing and memory.
% in passive disaggregated memory results in excessive RTTs and performance loss.

The second type (active disaggregated memory) organizes the disaggregated memory pool with regular servers and runs all or part of management software 
at these servers' CPUs~\cite{InfiniSwap,NAMDB,Kalia14-RDMAKV,Aguilera18-ATC}. 
%Recent proposals in this type include RDMA-based remote memory swap system~\cite{InfiniSwap}, remote database system~\cite{NAMDB}, 
%remote key-value store~\cite{Kalia14-RDMAKV}, and remote memory system with file interfaces~\cite{Aguilera18-ATC}.
%hey all run their memory systems on regular machines and use those machines' CPU to manage the memory spaces.
Co-locating management with memory reduces network RTTs.
However, these server-based solutions increase owning and energy cost, with full servers and large amounts of CPU cycles at the memory layer.
Moreover, the number of CPU cores limits the amount of parallel memory requests a memory server can handle.
An alternative way of building active disaggregated memory is to run management software in low-power cores.
However, recent work shows that low-power cores cannot meet the high-bandwidth and low-latency demands of memory systems~\cite{Tsai20-ATC}.

\phdm\ solves the problems of both these two existing disaggregated memory models.
By managing memory and providing distributed memory services in hardware at \MN{}s,
\phdm\ can achieve great performance, low network bandwidth consumption, and low monetary and energy cost.

LegoOS~\cite{Shan18-OSDI} is a distributed OS that takes a {\em partially} active disaggregation approach. 
%designed for resource disaggregation. %, which also advocates for managing resources at the resource nodes.
It runs a virtual memory system at its disaggregated memory pool and exposes a virtual memory interface to the compute pool.
%However, a virtual memory system is all what LegoOS' memory layer runs.
To support higher-level memory services, LegoOS still needs to add a software layer at the compute side.
More important, LegoOS has no real hardware design or implementation.
%runs all management in software on regular servers that are emulated as resource devices.
HPE's ``The Machine'' (Memory-Driven Computing) project~\cite{HP-TheMachine,THEMACHINE-HOTOS,HP-MODC-POSTER,THEMACHINE-WEB}
is another proposal that adopts the partially active disaggregation approach.
It separates a pool of DRAM/NVM from a pool of SoCs with a special interconnect inside a rack. % through a special interconnect~\cite{Genz-citation}. 
Memory instructions running at SoCs are transparently translated to access the DRAM/NVM pool,
and the translation is performed either at the interconnect~\cite{Genz-citation} or at the memory pool.
Similar to LegoOS, the Machine still needs extra software running at the SoC pool to provide high-level memory services.
Moreover, neither its design nor its maturity is known to the public.
%The DRAM/NVM pool provides a global memory address space,
%which requires extra software running at the SoC pool to manage and . % in, and use the memory pool.

\subsection{RDMA and Other Network Technologies}
\label{sec:rdma}

Datacenter network speed has increased dramatically in the past decade.
%Most datacenters now run on 10\Gbps/server or 40\Gbps/server networks~\cite{CatapultV1,Facebook-10GDatacenter},
%with 100\Gbps\ and 200\Gbps\ networks on the horizon~\cite{Facebook-100G,Mellanox200Gbps}.
%Meanwhile, network RTT can be as low as 1\mus\ for a single hop~\cite{Mellanox-Switch}.
High-bandwidth, low-latency networks give rise to a surge of interest in exploring remote or disaggregated 
resources~\cite{FaRM,FireBox-FASTKeynote,Shan18-OSDI,HP-TheMachine}.
%Network's much faster frequency speedup than CPU's~\cite{XXX} also motivated offloading various application and 
%I/O functionalities to hardware.
%Both these hardware trends are in line with what we propose in \phdm.

A high-speed network technology that several major datacenters have adopted~\cite{AZURE-RDMA,ALIBABA-RDMA} is
{\em Remote Direct Memory Access} (RDMA),
%RDMA supports both {\em one-sided} and {\em two-sided} communication to remote memory.
%One-sided RDMA allows one machine to directly read and write another machine's memory without involving the latter's CPU.
%Two-sided RDMA works like traditional messaging, involving both sender and receiver CPU.
which is the communication media adopted by most remote memory systems. % use RDMA as the communication media.
%with one-sided RDMA being used by systems where remote memory are managed by local (compute) machines~\cite{DPM}
%and two-sided RDMA used by systems where each machine manages its own memory~\cite{Kalia14-RDMAKV,Mitchell13-ATC}.
%With the help of one-sided RDMA and other one-sided network technologies~\cite{GenZ},
%several software systems have been proposed to treat 
%Albeit its superior performance, RDMA's limited interface is too restrictive for most applications,
Although several recent proposals of hardware~\cite{StRoM,Aguilera-FarMemory} and software~\cite{Tsai17-SOSP,HyperLoop} extensions to RDMA made it more flexible and performant, 
RDMA is still an ill fit for memory disaggregation.
The RDMA protocol and its implementation are both tightly bound with the processor that co-locates with the memory.
At least a process running on this processor needs to perform virtual memory allocation 
and registration through an OS and then share the allocated virtual memory addresses
with other (client) servers that wish to access the allocated memory. 
Essentially, RDMA's abstraction is a {\em host-server-centric} memory view.
What fits disaggregated memory and what \phdm\ provides is a {\em client-centric} memory view 
where client applications running on a server can directly access remote memory without the need to contact or maintain 
any additional processes or OSs running at the memory side.
Moreover, \phdm\ is a system-level design that has built-in distributed support, while RDMA is just a network layer that needs another software distributed system. 
%a client-side address space in \phdm\ could span multiple \MN{}s. %memory nodes.
%This client virtual address space can also span multiple \MN{}s.

Recently, new cache-coherent interconnects like Gen-Z~\cite{Genz-citation} and CXL~\cite{CXL-citation}
have been proposed for single-machine or rack scale memory accesses.
They expose a global memory space to applications running at the compute (client) 
side and guarantee cache coherence among applications and memory.
They are low-level, physical-layer solutions for accessing disaggregated memory.
With additional software systems, they could be used to build passive disaggregated memory.
\phdm's active disaggregated memory model saves network RTTs and client-side CPU cycles.
Moreover, the need to mainain cache coherence limits the scalability of these interconnects, and they require the replacement of existing datacenter network infrasturcture.
\phdm\ does not have cache-coherence scalability bottlenecks
and builds on Ethernet and can directly be deployed in today's datacenters.
%Rack-scale cache-coherent interconnects like CXL and Gen-Z provide a hardware layer to access disaggregated memory. 
%Software layer needs to be added on top (at the CPU side) to manage the disaggregated memory space.

\subsection{HADM Benefits and Use Cases}

The \phdm\ model offers several benefits over existing proposals. 
%that are key to meeting today's datacenter needs.
Unlike non-disaggregated remote memory, 
\phdm's separate, self-contained disaggregated memory layer is easy to configure, manage, and scale.
Compared to software-based approaches, 
\phdm\ can achieve 
%with hardware-based implementation, remote memory services can achieve
high-bandwidth, low-latency performance 
and sustain large concurrent loads
without paying the high costs of full servers.
Unlike passive disaggregated memory,
%By offering high-level distributed memory services, 
applications can build different remote memory services and/or offload
their near-data computation to the memory pool, 
improving performance and reducing CPU cycles.
%easily use remote memory without 
%worrying about its distributed nature or dealing with low-level interfaces.
%Finally, programmable hardware enables the reuse of the same hardware for different 
%types of memory services, yielding faster and cheaper deployment. %option than ASIC.

Many types of applications can make use of \phdm.
For example, applications or libraries can store large in-memory data structures 
like hash tables and tree indices at \phdm's memory pool and offload operations
like hash lookup and tree traversal to move computation closer to data.
More advanced types of application offloading are also possible, such as data 
processing and data analytics.
Applications can also use the memory layer of \phdm\ as a location to cache storage 
data or to share data across different compute nodes, 
\eg, storage-data caching or shared states for serverless computing systems.
Finally, OS and low-level systems software can build more functionalities like remote swap over \phdm\
and offer applications transparent accesses to remote memory.
See Appendix for further discussion.


\fi


\if 0
\sys\ is also different from previous computation offloading systems in terms of its hardware choice.
Previous solutions use either FPGA or ARM-based SoC to build their smart devices.
%with most of them also include a full NIC on board.
\MN{}s in \sys\ include both FPGA and ARM 
and use them for different planes. %, with the former used for the data plane and the latter used for the control plane.
%\sys\ integrates all the data path, including a lean network stack, on a single FPGA chip,
%while moving all the control plane and metadata operations to the SoC.
In doing so, \sys\ could achieve the best performance, cost, and programmability
combination that fits the need of a remote memory system.
%FPGA has seen an increasing adoption in industry~\cite{XXX} and more research interests~\cite{XXX} in recent years.
%Among the many FPGA-based systems, KV-Direct~\cite{KVDIRECT} and Catapult~\cite{XXX} are most relevant to \sys.
\fi

\if 0
Among all existing computation offloading solutions, three are the most relevant to \sys. 
KV-Direct~\cite{KVDIRECT} is a remote key-value store built on FPGA.
%Similar to \sys, it also integrates a lightweight network stack on the FPGA chip.
Unlike \sys, it targets a non-disaggregated environment and 
relies on the host CPU to perform metadata tasks such as memory allocation.
StRoM~\cite{StRoM} is a recent hardware proposal that includes a full RoCE (RDMA-over-Converged-Ethernet) network stack
and several high-level APIs like pointer chasing on an FPGA.
Although StRoM extends RDMA's restricted APIs to a more feature-rich interface,
it still has the same host-based memory view like RDMA (\S\ref{sec:rdma}).
Moreover, it has no distributed systems support.
Catapult~\cite{Catapult,CatapultV1} is a general-purpose distributed FPGA computation-offloading platform developed and deployed at Microsoft.
%which has been used for network-function offloading~\cite{AccelNet}, deep-learning inference~\cite{Brainwave}, and Bing search~\cite{CatapultV1}.
%Although Catapult is also a distributed FPGA-based platform, 
Different from \sys, Catapult is designed for a non-disaggregated environment 
and targets general computation offloading instead of memory management/services.
\fi


\if 0
Conceptually, the \phdm\ model could be built using any programmable hardware technologies.
For example, a SmartNIC could be extended to a programmable disaggregated memory device by attaching some memory chips to it.
Processor-based SmartNICs usually attach an ARM-based SoC or a small CPU to a NIC~\cite{BLUEFIELD,BROADCOM-SMARTNIC}.
Software can be easily deployed in the processor.
However, our experiments show that the processor cannot meet the performance requirements of high-speed network.
Hardware-based SmartNICs attach an FPGA chip to a NIC~\cite{INNOVA,SOLARFLARE-FPGA}.
\sys's hardware architecture is close to these SmartNICs, both using FPGA as programmable hardware (and thus could easily be modified to run on them).
Differently, \sys\ does not use a separate NIC and integrates the network stack with the memory services, 
saving both monetary cost and the performance overhead of communicating between two chips.
\fi

