\section{Goals and Related Works}
\label{sec:clio:motivation}

Resource disaggregation 
separates different types of resources into different pools,
each of which can be independently managed and scaled.
Applications can allocate resources from any node in a resource pool, resulting in tight resource packing. %regardless of where other types of resources sit.
Because of these benefits, % its manageability, independent scaling, and efficient resource utilization benefits,
many datacenters have adopted the idea of disaggregation, often at the storage 
layer~\cite{FACEBOOK-BRYCECANYON,FB1,SnowFlake-NSDI20,AMAZON-S3,AMAZON-EBS,Pangu,FC-SAN-book}.
With the success of disaggregated storage,
researchers in academia and industry have also sought ways to disaggregate memory
(and persistent memory)
\cite{Lim09-disaggregate,FireBox-FASTKeynote,IntelRackScale,Lim12-HPCA,Shan18-OSDI,hotpot-socc17,RAMCloud,Tsai20-ATC,AIFM,FastSwap,InfiniSwap,Semeru,Nitu18-EUROSYS}.
Different from storage disaggregation,
\md\ needs to achieve at least an order of magnitude higher performance and it should offer a byte-addressable interface.
Thus, \md\ poses new challenges and requires new designs.
This section discusses the requirements of \md\ and why existing solutions cannot fully meet them.

\subsection{\md\ Design Goals}
\label{sec:clio:requirements}
In general, \md\ has the following features, some of which are hard requirements while others are desired goals.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Hosting large amounts of memory with high utilization.}
To keep the number of memory devices and total cost of a cluster low,
each \MN\ should host hundreds GBs to a few TBs of memory that is expected to be close to fully utilized.
To most efficiently use the disaggregated memory, we should allow applications to create and access {\em disjoint} memory regions of arbitrary sizes at \MN.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Supporting a huge number of concurrent clients.}
To ensure tight and efficient resource packing,
%To fully exploit the resource-utilization benefit of disaggregation, 
we should allow many (\eg, thousands of) client processes running on tens of \CN{}s to access and share an \MN.
This scenario is especially important for new data-center trends like serverless computing and microservices where applications run as large amounts of small units.
%Thus, disaggregated memory should scale with client servers, client processes, memory size, and memory regions.
%each client process should be able to access large amounts of memory and many disjoint memory regions 
%scale with client servers, client processes, memory size, memory regions

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low-latency and high-throughput.}
We envision future systems to have a new memory hierarchy, where disaggregated memory is larger and slower than local memory but still faster than storage.
Since \md\ is network-based, a reasonable performance target of it is to match the state-of-the-art network speed,
\ie, 100\Gbps\ throughput (for bigger requests) and sub-2\mus\ median end-to-end latency (for smaller requests).
%When used as dynamically allocated memory, disaggregated memory should deliver low latency so as not to slow down application execution.
%When it is used as (cache of) data store, throughput is more important, 
%and accesses to disaggregated memory should reach network line rate.
 
\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low tail latency.}
%Many workloads in datacenters have tight SLOs. 
Maintaining a low tail latency is important in meeting service-level objectives (SLOs) in data centers.
%Even though accesses to \MN{}s are much rarer than local memory accesses at \CN{}s (\eg, because \CN{}s cache remote data locally), %Although \CN{}s could cache data at their local memory to reduce the amount of remote memory accesses,
Long tails like RDMA's 16.8\ms\ remote memory access can be detrimental to applications that are short running (\eg, serverless computing workloads) or have large fan-outs or big DAGs
(because they need to wait for the slowest step to finish)~\cite{taillatency}.
%Within a rack, the network wire delay is short (~XXX\mus) and stable.
%Other factors such as memory access delays, software delays, and network queueing delays are the major factors in the tail.
%One of the obstacles to adopting disaggregated memory is the fear for unpreditable performance and much worse tail latency than local memory. %, \eg, during network congestion
%In addition to network unstability, we found that memory systems could also contribute to long tails with today's RDMA-based systems (\S\ref{sec:}).
%Having predictable, bounded performance and low tail latency would be one key to the successful adoption of memory disaggregation in data centers,
%where SLAs are important.

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Protected memory accesses.}
%When deployed in datacenters, it is important to protect disaggregated memory from undesired or malicious accesses.
As an \MN{} can be shared by multi-tenant applications running at \CN{}s, % and applications from different tenants over the network,
we should properly isolate memory spaces used by them.
Moreover, to prevent buggy or malicious clients from reading/writing arbitrary memory at \MN{}s, we should not allow the direct access of \MN{}s' physical memory from the network and \MN{}s should check the access permission.
%we should properly protect and isolate memory accesses to \MN{}s
%(\eg, by not allowing \CN{}s to directly access \MN{} physical memory over the network).
%and should not allow \CN{}s to directly access physical memory at \MN{}s.
%they should have at least the same level of safety guarantees as today's normal (local) memory
%and potentially also guard against new security threasts such as side channel attacks over the network~\cite{Tsai19-Security}.
%when shared across side channel attacks~\cite{Meltdown,Spectre}

\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Low cost.}
A major goal and benefit of resource disaggregation is cost reduction.
A good \md\ system should have low {\em overall} CapEx and OpEx costs.
Such a system thus should not 1) use expensive hardware to build \MN{}s, 
2) consume huge energy at \MN{}s,
and 3) add more costs at \CN{}s than the costs saved at \MN{}s.
%1) means that server-based \MN\ designs are not ideal, and a hardware-based \MN\ design should make careful choice to keep the hardware resource consumption low;
%2) implies that CPU-based \MN\ solutions are likely to be too power hunger, and low-power solutions that run too slow cannot work either;
%and 3) implies that 
%Previous work~\cite{Shan18-OSDI} has shown that \md\ could improve memory resource utilization by around 50\% 
%(\ie, a \md\ cluster only needs to host half of the memory compared to a non-disaggregated cluster).
%This means that %1) a \md\ system should aim to have close-to-full utilization of its memory and have minimal memory waste, and 2) 
%building and running an \MN\ should not double the cost of hosting memory, as such a \md\ system would cost even more than no disaggregation.
%Using a server to build an \MN\ is thus not a good option, 
%since a server box
%and its CPU
%costs more than the DRAM it hosts.
%Another cost-related goal of \md\ is to have as much memory used by applications as possible (\ie, minimal memory wastes).
%However, if adding an \MN{} to host a certain amount of memory doubles the CapEx and OpEx cost, there will be no cost saving.
%This indicates that 
%Although disaggregated memory pool can be built with regular servers, 
%(in fact, most of today's disaggregated memory solutions~\cite{AIFM,FarMem,InfiniSwap,Semeru} are server-based),
%it is much cheaper to build and run standalone disaggregated memory devices without a server box or a CPU.


\stepcounter{reqs}
\boldpara{R\arabic{reqs}: Flexible.}
With the fast development of datacenter applications, hardware, and network, a sustainable \md\ solution should be flexible and extendable,
for example, to support high-level APIs like pointer chasing~\cite{AIFM,Aguilera-FarMemory},
to offload some application logic to memory devices~\cite{AIFM,StRoM},
or to incorporate different network transports~\cite{Homa,NDP,TONIC} and congestion control algorithms~\cite{swift-sigcomm,1RMA,hpcc-sigcomm19}.

\subsection{Server-Based Disaggregated Memory}
%RDMA-Based and Messaging-Based Disaggregated Memory}
\label{sec:clio:rdma}

\md\ research so far has mainly taken a server-based approach by using regular servers as \MN{}s~\cite{InfiniSwap,FastSwap,Semeru,Shan18-OSDI,AIFM,zombieland,FaRM},
usually on top of RDMA.
%which are connected to \CN{}s with RDMA or TCP~\cite{}.
%The second approach, taken by most disaggregated and remote memory solutions,
%is using a virtual memory abstraction provided by host server at \MN{}s
%and build their own layer on top of RDMA~\cite{InfiniSwap,FastSwap,Semeru} or TCP~\cite{AIFM}.
%many server-based disaggregated memory systems,
%uses the abstraction provided by a network layer 
%They use RDMA~\cite{InfiniSwap,FastSwap,Semeru} or TCP~\cite{AIFM} as the communication layer.
%As discussed in \S\ref{sec:intro}, 
The common limitation of these systems is their reliance on a host server and the resulting CPU energy costs, both of which violate \textbf{R6}.
%RDMA and TCP's symmetric architecture and connection-based, reliable transports are ill fit for memory disaggregation.
%Each of them also has their own problems.
%\md\ solutions that use TCP or similar transports incur high performance overhead because of costly network stack and memory copy, violating \textbf{R3}.
%other messaging-based disaggregated memory systems first copy application objects to messages at \CN{}s
%and then from messages to memory locations at \MN{}s.
%These systems not only incur the performance overhead of memory copies 
%but also require intensive CPU cycles to run the transport layer,
%making them unfit for memory disaggregation.
%These memory copies not only add performance overhead but also 
%RDMA is a high-speed, zero-copy, low CPU-utilization network technology 
%that has been adopted by several major datacenters~\cite{Microsoft,Alibaba}.

\ulinebfpara{RDMA} is what most server-based \md\ solutions are based on, with some using RDMA for swapping memory between \CN{}s and \MN{}s~\cite{InfiniSwap,FastSwap,Semeru} and some using RDMA for explicitly accessing \MN{}s~\cite{AIFM,zombieland,FaRM}.
%is more efficient than TCP-like transports, % avoids memory copying and can bypass CPU and OS kernel for most operations.
%but it still relies on the host server to run a virtual memory system,
Although RDMA has low average latency and high throughput, it has a set of scalability and tail-latency problems.
% is not a scalable or low-cost way to build disaggregated memory systems
%because of its reliance on the host CPU, MMU, and OS to run a virtual memory system.
%a host server box (CPU, MMU, and OS).
%RDMA's main issue when used for memory disaggregation is its reliance on host CPU, MMU, and OS
%and its limited scalability.
%RDMA relies on a host server's virtual memory system to manage the memory in this server.
%(\eg, for virtual and physical memory allocation, address translation, and page fault handling).
%Thus, \MN{}s can only be operated with a host server box.

A process ($P_M$) running at an \MN\ needs to allocate memory in its virtual memory address space 
and {\em register} the allocated memory (called a memory region, or MR) with the RDMA NIC (RNIC).
The host OS and MMU set up and manage the page table that maps $P_M$'s virtual addresses ({\em VA}s) to physical memory addresses ({\em PA}s).
%Client applications use $P_M$'s virtual memory address ($P_M{\text -}VA$) and MR information to perform RDMA read/write.
%On the \CN-side, there needs to be an extra level of indirection to first translate application abstraction to the $P_M{\text -}VA$ and MR combination.
%On the \MN-side, RNICs rely on host OS and MMU to manage page tables that map $P_M{\text -}VA$ to $PA$ (physical memory address).
To avoid always accessing host memory for address mapping, RNICs cache page table entries (PTEs),
but when more PTEs are accessed than what this cache can hold, RDMA performance degrades significantly (Figure~\ref{fig-pte-mr} and \cite{FaRM,Tsai17-SOSP}).
Similarly, RNICs cache MR metadata and incur degraded performance when the cache is full. 
Thus, RDMA has serious performance issues with either large memory (PTEs) or many disjoint memory regions (MRs), violating \textbf{R1}.
Moreover, RDMA uses a slow way to support on-demand allocation: the RNIC interrupts the host OS for handling page faults.
From our experiments, a faulting RDMA access is 14100\x\ slower than a no-fault access (violating \textbf{R4}).
%Page faults happen at the initial accesses to allocated virtual memory addresses, 
%causing long tails (violating \textbf{R4}).
%Slow initial accesses have a big impact on applications like serverless computing, which run short but very frequently.
%to handle page faults, which is extremely slow (14100\x\ slower than a no-fault access from our experiments).

To mitigate the above performance and scalability issues, most RDMA-based systems today~\cite{FaRM,Tsai17-SOSP} 
preallocate a big MR with huge pages and pin it in physical memory.
This results in inefficient memory space utilization and violates \textbf{R1}.
Even with this approach, there can still be a scalability issue (\textbf{R2}),
as RDMA needs to create at least one MR for each protection domain (\ie, each client).

In addition to problems caused by RDMA's memory system design, reliable RDMA, the mode used by most \md\ solutions, suffers from a connection queue pair (QP) scalability issue, also violating \textbf{R2}.
Finally, today's RNICs violate \textbf{R7} because of their rigid one-sided RDMA interface and the close-sourced, hardware-based transport implementation.
Solutions like 1RMA~\cite{1RMA} and IRN~\cite{IRN} mitigate the above issues by either onloading part of the transport back to software or proposing a new hardware design.

\ulinebfpara{LegoOS}~\cite{Shan18-OSDI}, our own previous work, is a distributed operating system designed for resource disaggregation.
Its \MN{} includes a virtual memory system that maps VAs of application processes running at \CN{}s to \MN\ PAs. \sys's \MN{} performs the same type of address translation.
%also advocates for managing memory at where memory is (called mComponent in LegoOS).
% is the only disaggregated memory system that adopts this abstraction and manages memory at \MN{}s.
However, LegoOS emulates \MN\ devices using regular servers and we built its virtual memory system in software,
which has a stark difference from a hardware-based virtual memory system. 
%In fact, we started our virtual memory design from LegoOS's but ended up finding 
%that none of its design or implementation fit a hardware environment. 
For example, LegoOS uses a thread pool that handles incoming memory requests by looking up a hash table for address translation and permission checking.
This software approach is the major performance bottleneck in LegoOS (\S\ref{sec:clio:results}),
violating \textbf{R3}.
%have to redesign every factor of it.
%still relies on host CPU and cannot be easily adapted to a hardware-based virtual memory system.
Moreover, LegoOS %has no computation offloading support, 
%and it 
uses RDMA for its network communication hence inheriting its limitations.

\subsection{Physical Disaggregated Memory}
\label{sec:clio:pdm}

One way to build \md\ without a host server is to treat it as raw, physical memory,
%\ie, client servers send read/write request with physical memory address, and the memory node directly read/write to that address without any address mapping.
a model we call {\em \pdm}.
The \pdm\ model has been adopted by a set of coherent interconnect proposals~\cite{Genz-citation,CXL-citation},
HPE's Memory-Driven Computing project~\cite{HP-TheMachine,THEMACHINE-HOTOS,HP-MODC-POSTER,THEMACHINE-WEB}.
A recent disaggregated hashing system~\cite{race-atc21} and our own recent work on disaggregated key-value systems~\cite{Tsai20-ATC} also adopt the \pdm\ model and emulate remote memory with regular servers.
%two RDMA-based software systems~\cite{Tsai17-SOSP,Tsai20-ATC} 
%(these systems use the OS kernel to register physical memory directly, which is a special RDMA configuration).
To prevent applications from accessing raw physical memory,
these solutions add an indirection layer at \CN{}s in hardware~\cite{Genz-citation,CXL-citation} or software~\cite{Tsai20-ATC,race-atc21}
to map client process VAs or keys
to \MN\ PAs. 

There are several common problems with all the \pdm\ solutions.
First, because \MN{}s in \pdm\ are raw memory, \CN{}s need multiple network round trips to access an \MN\ 
for complex operations like pointer chasing and concurrent operations that need synchronization~\cite{Tsai20-ATC}, violating \textbf{R3} and \textbf{R7}.
Second, \pdm\ requires the client side to manage disaggregated memory.
For example, \CN{}s need to coordinate with each other or use a global server~\cite{Tsai20-ATC} to perform tasks like memory allocation.
Non-\MN-side processing is much harder, performs worse compared to memory-side management (violating \textbf{R3}), and could even result in higher overall costs because of the high computation added at \CN{}s (violating \textbf{R6}).
%and when some memory needs to be migrated from one \MN\ to another (\eg, for load balancing), 
%all the \CN{}s that have mapped this memory need to update their mappings.
%Such complex client-side memory management defeats the purpose and benefits of disaggregation (\textbf{R5}).
Third, exposing physical memory makes it hard to provide security guarantees (\textbf{R5}),
as \MN{}s have to authenticate that every access is to a legit physical memory address belonging to the application.
%\MN{}s have to trust that \CN{}s will never access beyond their allocated physical memory regions. 
%coordination across client servers (\eg, through a global controller~\cite{Tsai20-ATC} or a distributed consensus system).
%Fourth, it is difficult to build compute offloads on physical memory and unsafe to run them without memory protection.
Finally, all existing \pdm\ solutions require physical memory pinning at \MN{}s, causing memory wastes and violating \textbf{R1}.
%it is also unclear how page faults will be handled.

In addition to the above problems, none of the coherent interconnects or HPE's Memory-Driven Computing have been fully built.
When they do, they will require new hardware at all endpoints and new switches. 
Moreover, the interconnects automatically make caches at different endpoints coherent, which could cause performance overhead that is not always necessary (violating \textbf{R3}).

Besides the above \pdm\ works, there are also proposals to include some processing power in between the disaggregated memory layer and the computation layer.
soNUMA~\cite{soNUMA} is a hardware-based solution that scales out NUMA nodes by extending each NUMA node with a hardware unit that services remote memory accesses.
Unlike \sys\ which physically separates \MN{}s from \CN{}s across generic data-center networks, soNUMA still bundles memory and CPU cores, and it is a single-server solution.
Thus, soNUMA works only on a limited scale (violating \textbf{R2}) and is not flexible (violating \textbf{R7}).
MIND~\cite{mind:sosp21}, a concurrent work with \sys, proposes to use a programmable switch for managing coherence directories and memory address mappings between compute nodes and memory nodes.
Unlike \sys\ which adds processing power to every \MN, MIND's single programmable switch has limited hardware resources and could be the bottleneck for both performance and scalability.