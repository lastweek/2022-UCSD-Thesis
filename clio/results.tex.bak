\section{Results}
\label{sec:results}

\input{fig-throughput-latency}
\input{fig-alloc-breakdown-photo-radix}


We evaluated \sys\ on our local cluster of four \CN{}s and one Xilinx ZCU106 board\footnote{Unfortunately, our process of purchasing and setting up a bigger cluster was significantly delayed because of COVID-19},
all connected to a Mellanox 40\Gbps\ VPI switch.
Each \CN\ server is a Dell PowerEdge R740 equipped with a Xeon Gold 5128 and a 40\Gbps\ Mellanox Connect-X3 NIC,
with two of them also have a Mellanox BlueField SmartNIC~\cite{BlueField}.
We also include results from CloudLab~\cite{CloudLab}, which hosts a newer generation of RNIC (Connect-X5).

We compare the end-to-end performance with our servers and our FPGA prototype 
with industry-grade NICs (ASIC) and well-tuned RDMA-based software systems.
All our results are FPGA-based, which would be improved with ASIC implementation.
Nonetheless, \sys\ significantly outperforms RDMA on scalability and tail latency,
while being similar on other measurements.

\subsection{Basic Microbench Performance}


\boldpara{Scalability.}
We first compare the scalability of \sys\ and RDMA.
Figure~\ref{fig-conn} measures the latency of \sys\ and RDMA as the number of client processes increases.
For RDMA, each process uses its own QP.
As expected, since \sys\ is connectionless, it scales perfectly with the number of processes (\sys\ collections).
RDMA scales poorly with its QP, and the problem persist with newer generations of RNIC,
which is also confirmed by others~\cite{Pythia,Storm}.

Figure~\ref{fig-pte-mr} evaluates the scalability with respect to PTEs and memory regions.
For the memory region test, we register multiple MRs using the same physical memory for RDMA.
For \sys\ (which gets rid of the MR concept), we use multiple collections to share the same memory,
resulting in one PTE per collection.
RDMA's performance starts to degrade when there are more than $2^8$ (local cluster) or $2^{12}$ (CloudLab),
and the scalability wrt MR is worse than wrt PTE.
In fact, RDMA fails to run beyond $2^{18}$ MRs.
In contrast, \sys\ scales well and never fails (at least up to 4\TB\ memory\footnote{This calculation is based on the number of PTEs, while our experiments 
map all of them to a small range of memory as our testbed only has 2\GB\ physical memory.}).
It has two levels of latency: lower latency below $2^4$ for TLB hit and higher above $2^4$ for TLB miss (which results in a constant of one DRAM access).
A real \sysboard\ could use a larger TLB if optimal performance is desired.

These experiments confirm that \textbf{\sys\ can handle thousands of concurrent clients and TBs of memory}.

\boldpara{Latency variation.}
This set of experiment analyzes
Figure~\ref{fig-miss-hit} plots the latency of reading/writing 16\,B data 
when the operation results in a TLB hit, a TLB miss, a first-access page fault, and MR miss (for RDMA only).
RDMA's performance degrades significantly with misses.
Its page fault handling cost is startling high --- 16.8\ms.
We confirm the same effect on CloudLab.
\sys\ only incurs a small TLB miss cost and \textbf{no additional cost of page fault handling}.

We also include an projection of \sys's latency if it was to be implemented using a real ASIC-based \sysboard.
Specifically, we collect the latency breakdown of time spent on the network wire and at \CN, 
number of cycles on FPGA, and time on accessing on-board DRAM.
We maintain the first part, scale the FPGA part to ASIC's frequency (2\,GHz), use DDR access time collected on our server to replace the access time to on-board DRAM (which 
goes through a slow board memory controller).
Our projected read latency is better than RDMA, while write is worse.
We suspect that it is due to Mellanox RNIC's optimization of replying a write before it is written to on-board DRAM, which \sys\ could also adopt.

Figure~\ref{fig-tail-latency} plots the per-request latency CDF of continuously running read/write of 16\,B data.
Clearly, \sys\ has much less latency variation and a much shorter tail than RDMA.

\boldpara{Read/write throughput.}
We measure \sys's throughput by varying the number of concurrent client threads (Figure~\ref{fig-read-write-throughput}).
\sys's asynchronous APIs (default) quickly reach the line rate of the testbed (9.4\Gbps\ maximum throughput measured with raw Ethernet).
Its synchronous APIs could also reach line rate fairly quickly.

Figure~\ref{fig-onboard-throughput} measures the maximum throughput \sys's FPGA implementation could reach without the bottleneck of the board's 10\Gbps\ port.
Both read and write can reach more than 110\Gbps\ when request size is large.
Read throughput is lower than write when request size is smaller.
We discovered that the throughput bottleneck is at a third-party non-pipelined DMA IP 
(which could potentially be improved).

\boldpara{Comparison with other systems.}
We compare \sys\ with native one-sided RDMA, Clover~\cite{Tsai20-ATC}, and HERD~\cite{Kalia14-RDMAKV}.
We ran HERD on both CPU and BlueField (HERD-BF).
%Native RDMA can be considered as a baseline (optimal performance but low-level, restrictive interface).
Clover is a passive disaggregated persistent memory system that we adapted as a passive disaggregated memory system.
HERD is an RDMA-based system that supports a key-value interface with an RPC-like architecture.
%It uses one RDMA read for its read and one RDMA write plus one 
%it can be considered as a software-based active disaggregated memory system. 

\sys's performance is similar to HERD and close to native RDMA.
%\sys's write performance is better than Clover and similar to HERD. %but has a constant overhead over native RDMA.
Clover's write is the worst because of it uses at least 2 RTTs for writes to deliver its consistency guarantees without any processing power at \MN{}s.
HERD-BF's latency is much higher than when HERD runs on CPU
due to the slow communication between BlueField's Connect-X5 chip and ARM processor chip when HERD runs on CPU
due to the slow communication between BlueField's Connect-X5 chip and ARM processor chip..
%\sys's write overhead can be attributed to \fixme{XXX}.

\boldpara{Allocation performance.}
Figure~\ref{fig-alloc-free} shows the performance of \sys's VA and PA allocation and RDMA's MR registration
(ODP represents when RDMA is configured with the On-Demand-Paging mode).
%Physical memory allocation includes the time to perform an allocation with the buddy algorithm and to insert the allocated address into the free page list.
%It is very fast, indicating that our asynchronous free physical page generation could keep up with most workloads' page fault speed.
%Virtual memory allocation and free (measured from client on \CN) are slower,
PA allocation takes less than XXX.
\sys's VA allocation is much faster than RDMA MR registration,
although both get slower with larger allocation/registration size.
%since these operations involve the costly crossing between FPGA and ARM.
%They are also slower with larger sizes, as searching the VMA tree for a big free region takes more time.

\boldpara{Close look at \sysboard\ components.}
To further understand \sys's performance, % and to determine the reason for worse large-read performance,
we profile different parts of \sys's processing for read and write of 4\,B to 1\KB.
\syslib\ adds a very small overhead (250\ns\ in total), 
thanks to our efficient threading model and network stack implementation.
Figure~\ref{fig-lat-break} shows the latency breakdown at \sysboard.
Time to fetch data from DRAM (DDRAccess) and to transfer it over the wire (WireDelay) are the main 
contributor to read latency, especially with large read size.
Both could be largely improved in a real \sysboard\ with better memory controller and higher frequency.
TLB miss (which takes one DRAM read) is the other main part of all the latencies.

\if 0
{\em DDRAccess} in the figure is the time between sending a DDR request to a third-party DDR IP 
and when the IP reports the completion of the request.
DDR access time for read is larger than write because the DDR IP signals the ``commit'' of a write before the data is all written to DRAM,
but the commit of a read is signaled only after the entire data is read, which is one of the reason why large reads are slower.
We do not change this feature of the DDR IP, as it follows our sequentiality and read committed consistency guarantees.
Address translation takes a constant of 20\ns\ for a PTE cache hit.
A PTE cache miss incurs a constant addition of 252\ns, mainly for fetching the PTE from DRAM.
Thanks to our overflow-free hash page table design, this PTE fetch time is bounded.
Note that a page fault does not add {\em any more} time on top of the 252\ns\ when free page lists are not empty,
as it goes through the same fault handling pipeline (which takes 3 cycles).
\sys's pre-processor and XBar account for a constant of 24\ns.
%With a page fault, our pipeline takes 3 cycles more. %another constant addition of XXX\ns.
Our reliable transport layer's request processing accounts for 30--72\ns.
The final number we report (as {\em OtherNet}) accounts for the open-source third-party FPGA IP/UDP stack we use. 
%It adopts stall-and-forward buffer to calculatse the checksum in UDP header for every out-going packet.
\sys\ is blocked by non-\sys\ entities (lower-layer network stacks, physical network port, and physical network) from sending out read/write replies.
This effect is higher with larger reads mainly because sending the third-party IP/UDP stack performs a checksum for the TX path but not the RX path
and that read size dominates the TX path.
\fi
 %out large read replies through the network in our cluster is slower.
%Our \CN-side threading model overhead is around a constant of 0.26\mus\ to 0.3\mus\ (we use a global polling thread to forward requests to client threads).
%These overheads mainly come from laying our network stack on top of an RDMA NIC (and its driver) at \CN{}s.
%They could be avoided or reduced by changing \CN-side's NIC and/or NIC driver.

\input{fig-kv-mv-df-energy}

\subsection{Application Performance}

\boldpara{Image Compression.}
We run a workload where each client (process for RDMA and collection for \sys)
compresses and decompresses 1000 256*256-pixel images with increasing number of concurrently running clients.
Figure~\ref{fig-photo} shows the total run time per client.
\sys's performance stays the same as the number of clients increase.
RDMA's performance does not scale because it requires each client to register a different MR.

\boldpara{Radix Tree.}
Figure~\ref{fig-radix} shows the latency of searching radix trees when varying the tree size. 
RDMA's performance is worse than \sys,
because it requires multiple RTTs to traverse the tree with pointer chasing,
while \sys\ only needs one RTT for each pointer chasing.
Unlike \sys, RDMA's performance also scales poorly.

\boldpara{Key-value store.}
%\subsubsection{Key-Value Store}
We evaluate \syskv\ using the YCSB benchmark~\cite{YCSB} and compare it to Clover, HERD, and HERD-BF (Figure~\ref{fig-kvstore}).
We run two \CN{}s and 8 threads per \CN.
We use 100K total key-value entries and 100K operations per test,
with YCSB's default key-value size of 1\KB. %where the key size is 8 bytes and the value size is 1\KB.
The accesses to keys follow the Zipf distribution ($\theta=0.99$).
We use three YCSB workloads with different {\em get-set} ratios: 
100\% {\em get} (workload C), 5\% {\em set} (B), and 50\% {\em set} (A).
\syskv\ outperforms all the other systems.

To put \sys\ in respective with other existing RDMA-based and FPGA-based key-value stores that we couldn't directly compare with (\eg, close-sourced), we compare 
\syskv's latency results with reported latencies in ~\cite{KVDIRECT}. 
\syskv\ has {\bf lower end-to-end latency than all these existing systems}.

\boldpara{Multi-version data store.}
%\subsubsection{Multi-Version Data Store}
We evaluate \sysmv\ by varying the number of \CN{}s that concurrently access data objects (of 16\,B) on an \MN\ using workloads of 50\% read (of different versions) and 50\% write under uniform and Zipf distribution of objects (Figure~\ref{fig-mvstore}). 
\sysmv's read and write have the same performance, and reading any version has the 
same performance, since we use an array-based version design. 
Running multiple \MN{}s have similar performance and we omit for space.

\boldpara{Data analytics.}
We ran a simple workload which first \texttt{select} rows in a table whose field-A matches a value (\eg, gender is female)
and calculate \texttt{avg} of field-B (\eg, final score) of all the rows.
Finally, it calculates the histogram of the selected rows (\eg, score distribution), which can be presented to the user together with the avg value (\eg, how female students' scores compare to the whole class).
\sys\ executes the first two steps at \MN\ offloads and the final step at \CN,
while RDMA always reads rows to \CN\ and then does each operation.
Figure~\ref{fig-dataframe} plots the total run time as the select ratio decreases (fewer rows selected).
When the select ratio is high, \sys\ and RDMA send similar amount of data across the network,
and as CPU computation is faster than FPGA, \sys's overall performance is worse than RDMA.
When the select ratio is low, \sys\ transfers much less data than RDMA, resulting in its better performance.

%then sends the data to \CN, which shuffles the data and sends the shuffled 
%data back to \MN\ for aggregation.

\subsection{Energy Cost and FPGA Consumption}
%\sys's memory layer has a 2\x\ CapEx saving and 25\x\ energy saving compared to a server-based active disaggregated memory layer.
%A Dell PowerEdge R740 server with a Xeon 5218 processor and 128\GB\ memory (the type in our cluster) costs \$6330~\cite{Dell-R740}.
%The Xilinx ZCU-106 board costs \$2495~\cite{ZCU106}, and 128\GB\ DRAM costs around \$720~\cite{Samsung-32GDRAM}.
%The measured power consumption of our FPGA plus our ARM processor is 6.825\,W (with 43\% used by ARM and the rest by FPGA).
%In comparison, our measured power consumption of our Dell server when running HERD is 175\,W. 
%consumes 175\,W while running HERD (data reported by iDRAC~\cite{dell-idrac}).
We measure the total energy used for running YCSB workloads
by collecting the total CPU (or FPGA) cycles and the Watt of a CPU core~\cite{XXX}, ARM processor~\cite{XXX}, and FPGA (measured).
We omit the energy used by DRAM and NICs in all the calculation. 
Clover, a system that centers its design around low cost, has slightly higher energy than \sys.
Even though there is no processing at \MN{}s for Clover, its \CN{}s use more cycles to process and manage memory.
HERD consumes 1.6\x\ to 3\x\ more energy than \sys, mainly because its CPU overhead at \MN{}s.
Surprisingly, HERD-BF consumes the most energy, even though it is a low-power ARM-based SmartNIC.
This is because its worse performance and longer total runtime.

With our design that is tailored to save FPGA resources, 
\sys\ consumes only 30.3\% LUTs and 31.3\% BRAM of the ZCU-106 FPGA (which is already a very small one).
Within \sys, the network and core-memory modules consume about 1/3 of the above consumption, with the rest being DDR4 IP and interconnects.
%(see Appendix for a detailed break down).
Our efficient implementation of \sys\ leaves most FPGA resources available for application offloads.
%\syskv\ consumes 2.5\% LUTs and 2.6\% BRAM;
%\syskvphys\ consumes 5.7\% LUTs and 2.4\% BRAM.
It indicate that even a small FPGA is sufficient.
%will be able to host multiple memory services on \sys\ at the same time.

