\section{Building Applications on \sys}
\label{sec:app}

We built five applications on top of \sys\ that have different ways of 
splitting computation across \CN\ and \MN\ (from all in \CN\ to all in \MN).

\boldpara{Image compression.}
The first application we built is a simple image compression/decompression utility
that runs purely at \CN\ ({\bf U2}).
Each client of the utility (\eg, a Facebook user) has its own collection of photos, 
stored in two arrays at \MN, one for compressed and one for original.
Because clients' photos need to be protected from each other, we use one process
per client to run the utility.
The utility simply reads a photo from \MN\ using \sysread, compress/decompress it,
and write it back to the other array using \syswrite.
We implemented this utility with XXX C code in XXX developer days.

\boldpara{Radix tree.}
Our second example is a demonstration of how to build a data structure on \sys\
using \sys's extended API ({\bf U2}).
Data-structure-level systems like AIFM~\cite{AIFM} could follow this example to make
(simple) changes to run on \sys.
Specifically, we build a radix tree with link lists and pointers, 
where all nodes in a layer is linked in a list and each node points a link list
of its children.
At \MN, we customized the pointer chasing API \sys\ provides to perform a value comparison
at each chased node and returns either there is a match or the next pointer becomes null. 
Searching the radix tree mainly involves going through layers of nodes and performing
this modified pointer chasing \sys\ API.
We implmeneted the radix tree with XXX C code and XXX lines of modification to \sysboard\ 
in XXX developer days.

\boldpara{Key-value store.}
We built {\em \syskv}, a key-value store that supports concurrent 
create/update/read/delete key-value entries
with atomic write and read committed consistency guarantees.
\syskv\ runs purely at \MN\ as a memory service that its users can access through a key-value interface from \CN{}s ({\bf U4}).
The \syskv\ module has its own address space (assigned by \sysboard\ at startup time).
It stores key-value pair data and a chained hash table for metadata in this virtual 
memory address space and accesses them with \sys\ virtual memory APIs.
We implmeneted \syskv\ with 772 SpinalHDL code in 6 developer days.


\boldpara{Multi-version object store.}
As another example of memory service ({\bf U4}),
we built a multi-version object store ({\em \sysmv}) which lets users on \CN{}s
reading a specific version of an object and appending a new version to an object.
Similar to \syskv, \sysmv\ has its own address space and stores versions of objects
in a per-object array.
Each object also has an object ID and a latest version number.
When a new object is created, \sysmv\ allocates (with \texttt{alloc}) a new array
and writes the virtual memory address of the array into an object ID map (which maps object IDs to their array addresses).
Appending a new version to an object would simply increase the latest version number
and use that as an index to the object array for writing the value.
Reading a version simply reads the corresponding element of the array.
We implmeneted \sysmv\ with 1680 Verilog code in 15 developer days.
 
\boldpara{Simple SQL-like computation.}
Our final example is a simple SQL-like data processing application (\sysdf),
which splits its computation between \CN\ and \MN\ ({\bf U5}).
We perform \texttt{select} and \texttt{aggregate} at \MN\ as two offloads
and \texttt{shuffle} at \CN.
All these modules share the same address space.
A simple workload first selects (filters) data at \MN,
then sends the data to \CN, which shuffles the data and sends the shuffled 
data back to \MN\ for aggregation.
We implmeneted \sysdf\ with XXX SpinalHDL code and XXX C code in XXX developer days.

\if 0
%This section first describes a generic framework for extending \sys\ to other memory services
%and several high-level memory APIs we built.
%It then presents the two extended memory services we built:
%a shared multi-version data store and a distributed key-value store.
%Implementing them took only 15/6 developer days and 1680/772 SLOC respectively,
%which is significantly faster than developing them on raw hardware.

\subsection{Extended Memory Service Framework}
\label{sec:extendedmodule}
On top of \sys's core functionalities, 
we built a generic framework for implementing different memory services.
Each service has a hardware module running at each \MN{}'s FPGA.
This module can be viewed as an independent ``process'' running at the \MN.
It has its own \pid\ and virtual memory address space to access on-board DRAM. 
Different service modules on the same \MN\ are protected from each other and from \sys\ modules.
It can use \sys's virtual memory APIs such as malloc/free, read/write, atomic operations, and high-level APIs like pointer chasing and data migration.
Extended service modules' address spaces are separate and hidden from application process address spaces running at \CN{}s.
Client applications access extended services via the APIs services provide such as key-value set/put.

Each service module is an endpoint connected to the XBar.
% (\pid\ and address space assigned by \sys\ when initializing).
At initialization time, ARM assigns each extended module a new \pid\ and 
a new virtual memory address space.
Service modules' calls to \sys\ APIs are sent to the core memory module and ARM (through the XBar),
which then execute the requests in the same way as those sent from client applications.
%\sys\ assigns each extended FPGA module its own \pid\ and virtual memory address space.
%Each extended memory service can add its own software components at \CN{}s, global controller, 
%and \MN{}s' ARM cores.
%Their hardware components are added to \MN{}s' FPGA as {\em extended modules},
%by \sys\ owner, who is the only party that can reconfigure the FPGA.

We envision \sys\ to be maintained by datacenter/cloud owners and extended services to be written and maintained 
by either datacenters' internal product teams or by cloud users.
Only cloud providers can reconfigure the FPGA and deploy extended service modules to it,
similar to today's cloud FPGA services~\cite{AWS-F1}.
\sys\ thus serves as a trusted layer that performs permission check and protection.

%Extended modules thus can be viewed as different ``processes'' running at an \MN.
%These processes are orthogonal and transparent to application processes running at \CN{}s.
%We separate these two types of processes using the highest bit of the \pid.
%With its own private address space, an extended module can use \sys's virtual memory APIs to allocate and access on-board DRAM.

Building extended modules on \sys's virtual memory interface offers three main benefits.
First, developers do not need to deal with physical memory directly and can write memory-related operations similar to writing software.
They do not need to deal with FPGA-ARM communication either.
Second, \sys\ provides protection across different potentially untrusted extended services.
Third, \sys's virtual memory operation ordering and consistency guarantees make it easier for service modules to implement 
their own concurrent data accesses.
%To allow more deployment options, \sys\ can also assign extended services private physical memory spaces
%and let them manage/access this space on their own.
We also allow extended services to build customized ARM-side and global controller software. 
These optional components can also leverage \sys's on-board framework and distributed operation support.

%With this framework, we built several high-level memory APIs, including pointer dereferencing, 
%array indexing, \tas, and \cas, as extended modules.
%They internally call several \sys\ virtual memory APIs, but appear to clients as single remote APIs.
%Other high-level APIs could be built in a similar way to avoid multiple RTTs.

\if 0
We implemented a distributed key-value store and a distributed multi-version object store.
Implementing them took only 15/6 developer days and 1680/772 SLOC respectively,
which is significantly faster than developing them on raw hardware.
\fi

\subsection{Concurrent Multi-Version Object Store}
\label{sec:mvstore}
In many data store services such as pub-sub systems, multi-version databases, and multi-version file systems,
there is the need to maintain multiple versions of a data object and allow concurrent accesses to them from different clients.
We built such a multi-version object store on \sys\ called {\em \sysmv}.

\sysmv\ hosts a set of objects, each having a unique object ID and one or more versions of data.
\sysmv\ uses its own on-board virtual address space to store its data and most of its metadata. %these data structures.
Writing an object with \sysmv\ always creates a new version (\ie, no overwrite).
Reading to an object can take two forms: reading a specific version or reading the latest version.
\sysmv\ allows concurrent accesses to an object (create, read, write, and delete) 
and guarantees sequential consistency for each object.
\sysmv\ organizes each object as a (dynamically-allocated) array of version data that we call {\em data array}.
Each array element stores a version of the data, with version ID being the index.
%We recycle version IDs in time order.
%\sysmv\ also uses some other data structures to manage objects, 
%\eg, an {\em object ID map} to store data arrays' addresses. %keep track of object IDs and the mapping between these IDs to their version data array.

When the user issues a create object request, \sysmv\ finds a free object ID 
(from a {\em free ID list} maintained in its virtual memory)
and allocates a new data array by issuing an \texttt{alloc} request to ARM.
After the above two (parallel) operations complete, \sysmv\ writes the virtual address of the allocated data array
to the object ID map.
%to the metadata of the new object ID.
To handle a write request, \sysmv\ first reads the address of the object's data array and
(atomically) increments its version number.
It then writes new data into the corresponding array element.
To read a specific version, % (including the latest version), 
\sysmv\ locates the object's data array and simply reads the array element of that version.
To delete an object, \sysmv\ adds the object ID to the free ID list and issues a \texttt{free} request to ARM to free the data array.

Each \sysmv\ user request involves at least two internal \sys\ operations, some of which includes both metadata and data operations.
This compound request pattern makes it tricky to deal with synchronization problems,
as \sysmv\ needs to ensure that no internal \sys\ operation of a later \sysmv\ request could affect the correctness of an earlier \sysmv\ request.
Fortunately, both \sys\ core memory and \sys\ ARM metadata modules guarantee sequential delivery of \sys\ operations.
Since \sysmv\ only issues one \sys\ operation per clock cycle, the ordering that \sys\ modules guarantees
is sufficient to deliver \sysmv's consistency guarantees.
Because of space limits, we omit the details of this discussion.

\subsection{Distributed Key-Value Store}
\label{sec:kvstore}
We built {\em \syskv}, a distributed key-value store on \sys\ that supports concurrent create/update/read/delete key-value entries
with atomic write and read committed consistency guarantees.
Similar to \sysmv, \syskv\ leverages the ordering and atomicity guarantees of the core memory system
to ensure its own consistency guarantees.

\syskv\ uses a chained hash table in on-board DRAM to manage key-value pairs and supports variable sizes of keys and values.
It stores key-value pairs in DRAM outside the hash table
to decouple key/value size from hash table size (\eg, the hash table can be small enough to be stored in FPGA memory if needed).
% key and value sizes are not bounded by the location of the hash table.
Each hash bucket has a chain of {\em slots}. 
Each slot contains information of 7 key-value pairs,
including the virtual address of a key-value pair 
and a small hash value of the key.
This small hash is used to perform a fast lookup filtering.
It is calculated using a secondary hash function that is different from the primary hash function.
%\syskv\ uses its virtual memory space to store key-value pairs and the hash table.

To create a new key-value entry, \syskv\ allocates space for the key-value pair with an \texttt{alloc} call to ARM and writes the key-value data with a \texttt{write}.
It also hashes the key with both the primary and the secondary hash functions.
It then fetches the last slot in the corresponding hash bucket chain.
If that slot is full, \syskv\ allocates another slot (using \texttt{alloc}) and links it to the current chain.
Finally, it inserts the information (address of the newly created key-value pair and the secondary hash value) into the slot.

To perform a read, \syskv\ locates the hash bucket (with the primary hash function) 
and fetches one slot in the bucket chain at a time.
It then compares the secondary hash value of the key to the 7 entries in the slot.
If there is no match, it fetches the next slot in the chain.
Otherwise, with a matched entry, it reads the key-value pair using the address stored in that entry (with a \texttt{read}).
It then compares the full key and returns the value if it is a match (or otherwise, it keeps searching down the chain).
Updating a key-value pair works similarly.


\fi
