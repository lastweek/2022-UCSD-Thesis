\section{\sys\ Design}
\label{sec:design}

This section presents the design of \sys,
including its virtual memory system, network system, and framework for computation offloading.
%\sys\ is .. presents unprecedented challenges
%because it not only impl hardware
%and aims to do so with ...

\subsection{Hardware-Based Virtual Memory System} 
\label{sec:memory}

The main part of \sys\ is a new page-based, on-demand-allocation virtual memory system that runs in \sysboard\ and 
serves applications running at both \CN{}s and \MN{}s.
We choose page-level addressing and on-demand allocation because they are the most 
efficient way to utilize memory space
and the most flexible way to enforce memory protection and sharing.
Although this virtual memory model has been well understood and standardized in modern computer architecture and OS's,
memory disaggregation presents an interesting new challenge:
\textit{how to implement a virtual memory system in hardware}? % so that \MN{}s can be cheaply built and run without a server box?
This challenge is particularly acute because of \sys's low-tail-latency, high-throughput, scalability, and low-cost goals.
Below, we explain how step by step \sys\ achieves these goals.
%Our design not only demonstrates how to build an efficient disaggregated \MN\
%but also illustrates \textit{how to build a major OS subsystem in hardware}.

\ulineitpara{Step 1: Achieving low cost with fast/slow path split.} 
Modern computers disperse the virtual memory system across many hardware and OS components.
%the main CPU pipeline, TLB, MMU, 
%the OS virtual memory allocation module, the OS page fault handler, and the OS swap system.
\textit{Can we simply move everything to a single hardware chip}?
Unfortunately, implementing the entire virtual memory system in hardware will be too costly,
as many virtual memory tasks have complex logic and need to deal with states,
both of which consume huge hardware resources. % and add performance overhead.
Thus, \sysboard\ should still use both hardware and software to implement a virtual memory system.

\input{fig-board}

\textit{Shall we then just follow how today's virtual memory system is split} across the OS and various hardware units?
Unfortunately, with this split, some memory operations will be slow because they involve both hardware and software
(\eg, a page fault is triggered by the MMU, handled by the OS, and finally fulfilled by the MMU again).
%page tables and the TLB are managed 
%this split does not cleanly separate 
%with this split, page fault handling involves both hardware and software.
% would be slow,
%resulting in two issues for \sysboard.
Such slow operations are tolerable in a regular server but present two problems for \sysboard.
First, different from a regular server, \sysboard\ only handles memory operations but needs to 
process one request at each cycle to reach network line rate 
at full load (which is expected to be a common case, as one \MN\ will be shared by many clients).
Second, waiting for a slow (software) operation requires \sysboard\ hardware to maintain a big buffer,
working against our low-cost goal.
%needs on-chip big buffer space (

\if 0
should not split and implement in the way of modern computer systems
the page table and TLB are accessed and managed together by the MMU hardware and OS
based on 
fast path is the only one accessing and updating TLB and page table
when page fault is handled in hardware, hardware can update TLB directly
and there is no need for the costly TLB shootdown
The virtual memory system in modern computers is dispersed across the main CPU pipeline, TLB, MMU, and the OS.
%many hardware and OS components:
%the main CPU pipeline, TLB, MMU, the OS virtual memory allocation module, the OS page fault handler, and the OS swap system.
%At the same time, each of them handles or has to consider other non-memory operations.
%For example, TLB is designed together with CPU data and instruction cache.
%
%\ulineitpara{Observation and challenge.}
Building a virtual memory system at \MN\ for memory disaggregation brings two opportunities:
{\bf O1}) memory requests are the only operations that \MN\ need to deal with,
and {\bf O2}) different virtual memory operations can be tightly integrated in the same hardware.
There is one new challenge as well: simply implementing everything in hardware
will be too costly, % (both in performance and in monetary), 
as many virtual memory tasks have complex logic and need to deal with states,
both of which consume huge hardware resources. % and add performance overhead.
\fi


%\ulinepara{Solution.}
Based on these insights, we propose a new hardware/software split of virtual memory functionalities
based on their speed and resource (\eg, buffer space) demands.
Thus, the resulting hardware is a fast path, and the software is a slow path.
%a design that splits \sys's virtual memory system 
%into a fast, hardware path and a slow, software path 
%based on the performance requirements and hardware resource demands of different functionalities.
The fast path ensures that {\em all} data memory operations can be processed at line rate and with minimal delay.
Different from today's computers, \sys's fast hardware path includes a page fault handler, in addition to a TLB, a TLB manager, and a DRAM accessing unit.
It integrates all these functionalities in a single pipeline.   
This pipeline only performs two tasks, reading/writing a virtual memory address (application requests)
and fetching/updating a PTE (internal operation).
%which is different from how modern computers split its virtual memory system across hardware and OS.
%The fast path only performs two tasks, reading/writing a virtual memory address (application requests) 
%and fetching/updating a page table entry (PTE) (internal operation).
%Different from traditional virtual memory system which requires the cooperation of CPU, MMU, and OS to perform 
%these tasks, 
%\sys\ fast path integrates minimal but all the necessary functionalities for these tasks into one (efficient) pipeline,
%All the read/write requests will go through 
%including a TLB that caches hot PTEs, a TLB manager that performs TLB replacement, 
%a PTE fetcher, a page fault handler, 
%and a memory access unit that accesses DRAM (for both PTEs and data).
We move all complex and/or stateful functionalities to the slow software path,
including virtual memory space (de)allocation, physical memory (de)allocation,
and various set up tasks. 
%We defer the detailed description of \sys's fast and slow paths implementation to \S\ref{sec:impl}.
%We move the complex, stateful, and non-deterministic allocation logic (both virtual and physical memory allocation)

One caveat that results from the separation is the synchronization between the fast path and the slow path.
The first synchronization problem happens when the slow path needs to update the page table (\eg, when handling a \free).
%between slow-path generated PTE changes (update or delete) and fast-path TLB.
If the change is applied directly to the page table in DRAM, then the TLB will be inconsistent.
%Today's computer relies on the OS to perform costly~\cite{XXX} TLB shootdowns for TLB consistency.
%Following our design of only having the fast path managing TLB,
To solve this problem, we adopt a simple principle: the fast path is the only unit accessing and changing TLB and the page table.
%We use a different approach where
Thus, the slow path hands over its PTE change requests to the fast path, 
which performs both the TLB change and the PTE change in its pipeline in a way that is consistent for inflight fast-path operations.
Another synchronization problem happens when as the slow path is handling a virtual memory operation like \free, 
the fast path handles a read/write request to the same address.
A potential method that could be implemented in the hardware (in fact, our strawman approach) 
is to compare each new request with all the inflight requests and block it when a conflict is detected.
This approach would require hardware resources to maintain inflight request information.
Instead, we opt for a client-side software approach (Step 4 below) to avoid this cost.
%We solve this problem with a simple technique:
%before sending a request to either the fast path or the slow path, 
%we use a \textit{command pre-processor} to detect if a fast-path (slow-path) request
%is destined to the same virtual memory address as an inflight slow-path (fast-path) request.
%If so, we simply blocks the request until the other request completes.

\ulineitpara{Step 2: Achieving network line rate and low tail latency with deterministic fast path and decoupled slow operations.}
To achieve network line rate for data operations,
%Since our fast path is where data operations happen, 
%our goal is to make it run as fast as network line rate to achieve the best possible end-to-end data-path throughput.
our design principle is to make the fast path performance {\em deterministic} 
so that we can plan out every stage of the pipeline to avoid stalling and process one data unit in each cycle.
A major technical hurdle is the handling of first-access page faults, which requires PA allocation.
%in achieving deterministic fast-path performance is on-demand allocation.
%On-demand allocation delays the allocation of physical page for a virtual memory page until when the virtual page is accessed for the first time (\ie, page fault time).
As PA allocation is a stateful operation (states to keep track of free PAs), it belongs to the slow path.
If page fault handling always has to wait for the slow on-demand allocation,
\sys\ memory performance will have long tail latency, and the fast path will have non-deterministic performance.
Our idea is to decouple the generation and the consumption of physical memory addresses
using an asynchronous communication method between the slow path and the fast path.
Specifically, we use an \textit{async buffer} between the slow and the fast paths.
The slow path keeps generating free physical memory addresses to always keep the buffer full.
During page faults, the fast path immediately grabs a pre-generated address from this buffer without any delay.
Note that even though a single PA allocation operation has a non-trivial delay, 
the throughput of generating PAs and filling the async buffer is higher than network line rate.
Thus, the fast path can always find free PAs in the async buffer.
Our implementation of \sysboard\ takes a constant of \textbf{three cycles to handle a page fault}.


\ulineitpara{Step 3: Achieving scalability with new page table design.} %Page table with bounded size and single-read access time.}
A target usage model and benefit of memory disaggregation is for many client applications to share a memory device.
Thus, our virtual memory system design should scale well with the number of collections. %clients (\ie, the number of \sys\ address spaces).
We propose a new {\em overflow-free} hash-based page table design that bounds the size of PTEs to physical memory size 
and \textbf{address translation to at most one DRAM access}.
Specifically, we store {\em all} PTEs (from all collections) in a single hash table 
whose size is proportional to the physical memory size of a \sysboard.
The hash table has enough entries to cover the entire physical memory (\eg, \fixme{XXX \MB\ for 1\TB\ physical memory and XXX page size}).
The hash value of a VA and its collection ID is used as the index to determine which hash bucket the corresponding PTE goes to.
To lookup the page table, we always fetch
% compute its hash value (using the {\em lookup3}~\cite{lookup3-wiki} hash function) 
the entire bucket (with all its slots) with a single DRAM read.

Normally, a hash table with fixed slots will have an overflow problem because of hash conflicts.
We use a novel technique to {\em proactively avoid} hash overflows at VA allocation time.
To handle an \alloc, the allocator in the slow path keeps trying available VA ranges until there is one 
where each virtual page in it goes to a hash bucket that still has empty slots.
%This allocation process is carried out in the slow path, while the page table lookup is performed by the hardware pipeline of a \sys\ device.
We trade potential retry overhead at allocation time (at slow path) for better run-time performance and simpler hardware design (at fast path).
% core-memory implementation.
This overhead is manageable because 1) each retry is fast with our implementation (\S\ref{sec:impl}), 
2) we employ huge pages, which means fewer pages need to be allocated, 
3) we choose a hash function that has very low collision rate~\cite{lookup3-wiki},
and 4) the \sys\ 64-bit virtual address space is huge.

\ulineitpara{Putting Steps 1-3 together.}
We illustrate how the key parts of \sysboard\ work using a simple example of allocating some memory and writing to it.
The first step (\alloc) is handled by the slow path, which allocates a VA range by finding an available set of slots in the hash page table.
The slow path forwards the new PTEs to the fast path, which inserts them to the page table.
At this point, the PTEs are invalid.
This VA range is returned to the client.
When the client performs the first write, the request will go to the fast path.
There will be a TLB miss, followed by a fetch of the PTE.
Since the PTE is invalid, the page fault handler will be triggered,
which fetches a free PA from the async buffer and establishes the valid PTE.
It will then (in parallel) execute the write and insert the new PTE to the TLB.
%\boldpara{Efficient support for on-demand allocation.}
%Overall, with our design, our implementation of \sysboard\ takes a constant of \textbf{three cycles for page faults}
%and at most \textbf{one DRAM access for address translation}.


\ulineitpara{Step 4: Delivering memory ordering at CN and synchronization at MN.}
As explained in \S\ref{sec:usage}, by default, \sys\ runs the release order
with dependency checks of requests targeting same VAs.
%provides two modes of operations:
%synchronous operations that are strictly ordered and asynchronous operations that follow release order.
We enforce this ordering at \CN{}s (in \syslib) instead of at \MN{}s for two reasons.
First, enforcing ordering at \MN{}s requires more hardware resources to maintain states.
Second, even if we enforce ordering at \MN{}s, network reordering would still break end-to-end ordering guarantees,
and we opt for relaxing network ordering guarantees to make \sys\ scalable and low cost (\S\ref{sec:network}).
%To enforce ordering for asynchronous operations at the client side, 
Specifically, \syslib\ keeps track of all inflight requests and matches every new request's VA
to the inflight ones. 
If a WAR, RAW, or WAW dependency is detected, \syslib\ blocks the new request until the conflicting request finishes.
When \syslib\ sees a \release\ operation, it waits until all inflight requests return or time out.
%For synchronous operations, \syslib\ only returns when a request gets a response, effectively achieving strict ordering.

We implemented synchronization primitives like \tas\ and \fence\ at \sysboard,
because they need to work across threads and collections.
Before a request enters either the fast or the slow paths, 
\sysboard\ checks if it is an atomic primitive or a \fence.
For the former, \sysboard\ will block future atomic primitives until the current one completes.
For the latter, \sysboard\ will block all future requests until all inflight ones complete.
Inflight synchronization primitives are the ``states'' that \sysboard\ needs to maintain for the entire memory system.
As these operations are not frequent, the hardware resources for maintaining these states are minimal.
%that users can use to customize their synchronization and consistency semantics. 
%For example, \fence\ flushes all the requests in a collection that are issued before it,
%which can be used to implement release consistency.
%\tas\ can be used to build critical sections on shared data.
%help users perform inter-thread and inter-process synchronization.

\ulineitpara{Step 5: Supporting future distributed extension with inter-MN chunk movement.}
The \sys\ system we present in this paper is a single-\MN\ system. 
We include an inter-\MN\ {\em chunk migration} operation that is crutial in extending \sys\ to a distributed-\MN\ setting 
and in supporting \MN\ memory over-commitment.
\sys\ can move a chunk of {\em virtual memory} space from a sender-\MN\ to a receiver-\MN.
To perform the movement, \syslib\ pauses sending new requests to the chunk at the sender-\MN.
The sender-\MN\ waits until all the inflight requests finish
and migrates all this chunk's metadata (\eg, virtual memory permission information) and data to the receiver-\MN.
The receiver-\MN\ writes the data to its memory and establishes new PTEs with new PAs.
Afterwards, the sender-\MN\ notifies all \CN{}s that have mapped this chunk about the new \MN\ address.
This movement operation is useful for distributed tasks like load balancing.
It can also be used to migrate or swap data when an \MN's physical memory is close to full.


%\stepcounter{principle}
%\ulinebfpara{Principle \arabic{principle}:}
%\textit{Efficient space usage and performance can both be achieved by integrating virtual memory system in the hardware pipeline}.

\subsection{Network System}
\label{sec:network}

Different from general-purpose network communication where each endpoint can be both the sender (or requester) and the receiver (or responder),
\MN{}s in a disaggregated memory system only respond to requests sent by \CN{}s,
and these requests are all memory-related operations.
Based on these observations, we design a new network system that exploits the unique features of memory disaggregation
and does so without the need to change any existing data-center network infrastructure.
Our main idea is to shift network tasks to the client side software
and to customize it to memory disaggregation.
With the following steps, \sys\ network system achieves scalability, low latency, and end-to-end reliability,
and it is completely {\em stateless} and {\em bufferless} at \MN{}s.
Both \CN{}s and \MN{}s only need standard Ethernet physical and link layers in the hardware.
Thus, \CN{} servers can continue using regular Ethernet-based NICs, and \MN{}s can be built with low cost.
%and only include L1 and L2 in hardware (regular Ethernet NIC at client servers and a standard Ethernet MAC module at memory device hardware).

\ulineitpara{Step 1: Eliminating connections with timeout-based RPC communication.}
%Our insight is that we {\em can} achieve reliability without the need for connection or {\em any} states at the memory devices
%by exploiting the {\em asymmetric} nature of memory disaggregation.
%and perform {\em all} reliable transport operations at the client side in software (\syslib).
Applications running at \CN{}s directly initiate \sys\ APIs to an \MN\ without any connections.
%Since all \sys\ operations are in the RPC style initiated by client servers, \ie, sending read/write requests and getting data/response back,
\syslib\ uses the response of each \sys\ request as the ACK and matches it to the request using a request ID.
We rely on standard link-layer mechanisms to correct and mitigate packet corruption~\cite{FEC}.
We treat uncorrectable corruption (either detected by \CN\ or by \MN) as request failure and retry the request.
If \syslib\ does not receive a response in a \texttt{TIMEOUT} period (\eg, because of packet loss in either the sending or receiving direction), % or detects packet corruption,
it will also retry the entire request.

\if 0
Network errors can happen when a packet is corrupted, loss, or reordered.
We use link-/physical-layer error correction like Forward Error Correction and retries that are already available in InfiniBand~\cite{MellanoxOFED} \fixme{and Ethernet???}
to deal with packet corruption, similar to prior practices~\cite{RAIL-NSDI17,FaSST}.
As the link layer does not (\fixme{or rarely?}) drop packets, packet loss is mainly due to buffer overflows in the network.
We control (minimize) packet loss with our client-side, software congestion/flow control mechanism (\S\ref{sec:cc}).
In the rare case of packet loss (either in a \sys\ request or its response), the client-side \syslib\ will fail after a request timeout
and retry the entire \sys\ request.
This leaves us with packet reordering, which is rare in non-multi-path network, but can still happen.
\fi

\ulineitpara{Step 2: Eliminating \MN-side request states by ordering packets (only) at \CN{}s.}
Step 1 uses request retry to deal with packet corruption, which handles one type of network reliability issue.
Another issue is packet reordering, for example, due to data-center multipath routing~\cite{ECMP}. %which can happen at the link layer.
Enforcing ordering above the link layer normally requires maintaining states (\eg, request ID) at both sender and receiver side (\eg, in the transport layer).
However, our goal is to eliminate states completely at \MN{}s.
Our approach is to deal with packet reordering only at \CN{}s in \syslib.

Specifically, \syslib\ splits a request that is bigger than MTU (mainly write requests) into several link-layer packets
and attaches a \sys\ header to each packet, which includes sender-receiver addresses, a request ID, and request OP information.
An \MN{} can thus treat each packet independently.
It executes packets whenever they arrive, even not in the sending order.
We intentionally relax the ordering guarantee at the network layer
because 1) memory requests do not always require strict ordering as described in \S\ref{sec:usage},
%, and \syslib\ performs 
%inter-request ordering as described in Step 4 in \S\ref{sec:memory},
2) intra-request packets can be executed out of order without breaking our memory ordering guarantees,
and 3) relaxed ordering allows performance optimization and less hardware resources at \MN{}s.
When receiving multiple link-layer packets belonging to the same request response, 
\syslib\ reassembles them before delivering to the application.


\ulineitpara{Step 3: Controlling congestion and incast at client software.}
Steps 1 and 2 focus on handling packet corruption and reordering.
Our final step is to minimize packet loss with congestion control and flow control.
Our design principle is to perform them at the client side to keep \MN{}s stateless.
%We make two insights here.
%First, with our design and implementation, a \sysboard's internal throughput is always higher than its port bandwidth.
%Thus, there will be no buffer overflow in a \sys\ memory device (\ie, it is faster than the network), 
%and there is no \MN-side incast problem and no need for \MN-side flow control.
We exploit the fact that \CN{}s know the sizes of both requests and expected responses
to control congestion on both the outgoing and incoming directions at \CN{}s.
We use a simple delay-based, reactive congestion control mechanism at \syslib.
Specifically, \syslib\ uses two types of congestion windows: outgoing and incoming.
For each \CN, there is one outgoing window per \MN\ that is shared by all collections sending to the \MN.
It controls how many outstanding requests can be made (across collections) to an \MN.
There is one incoming window per collection that controls how many outstanding expected responses there can be for each collection.
\syslib\ only sends a request out when both windows have slots.
We change the window sizes based on delay in a standard Additive Increase Multiplicative Decrease (AIMD) manner.
%client-side controlled congestion control
%reactive, delay-based, window based 
%solicitation window for client-side incast

\subsection{Computation Offloading}
\label{sec:offload}

One of the reasons why building computation offloading is hard is the lack of system support.
\sys\ takes a step forward by providing the full virtual memory support to offloads and 
by offering frameworks that ease the interaction between hardware and software and between \CN\ and \MN.
\sys\ users can write and deploy application offloads both in programmable hardware (FPGA)
and in software.
\sys\ offers the system support in the following steps.

\ulineitpara{Step 1: Easier offload development and safer execution with DVMA virtual memory abstraction.}
An offload can either claim the same process address space as its \CN-side counterparts
or has an address space by itself.
The execution of each offload is protected in the same way as \CN-side applications.
\sys\ offers offloads the same set of virtual memory APIs as regular \CN-side applications.
Developers thus do not need to deal with low-level physical memory.

\ulineitpara{Step 2: Reducing network round trips with extended APIs.}
Network communication between \CN{}s and \MN{}s is the main performance overhead in memory disaggregation.
Such communication can happen when applications at \CN{}s access memory at \MN{}s or communicate with offloads at \MN{}s.
To reduce network round trips for both cases,
\sys\ offers a small set of extended APIs like pointer chasing so that a high-level operation can be carried out entirely at \MN.
Users can also add more customized APIs
by writing a small offload module that handles these customized network requests
by executing a combination of \sys\ basic APIs.

\ulineitpara{Step 3: Separating fast/slow paths separation with asynchronous decoupling support.}
Similar to the \sys\ virtual memory system, certain application can benefit from
the separation of their offload into a fast path in hardware and a slow path in software.
We offer the same asynchronous communication framework between offloads' fast and slow paths as Step 2 in \S\ref{sec:memory}
for them to decouple the generation and the consumption of their own entities.

\ulineitpara{Step 4: Managing request ordering and data sharing with DVMA's uniform support.}
When an application is dispersed across \CN\ and \MN\ and across hardware and software, 
it is usually hard to reason about and enforce data sharing and request ordering semantics. 
%(\eg, synchronization and consistency level) 
%that application can freely define.
\sys\ offers a uniform mechanism for applications to easily express these semantics.
Each hardware offload, software offload, and regular thread at \CN\ in a collection is 
treated as a thread in \sys.
\sys\ performs the same dependency checks as described in Step 4 of \S\ref{sec:memory} for requests within each offload,
and \sys's synchronization primitives work across all threads.
%We achieve this global uniform synchronization by \fixme{XXX}
