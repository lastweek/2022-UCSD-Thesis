
\documentclass[10pt,times,twocolumn]{z2-article}
%\documentclass[11pt,times,twocolumn]{article} %\documentclass[11pt]{article} 

%\usepackage{hyperref,url}%,bibunits}
%\urlstyle{rm}
\usepackage[usenames]{color}
\usepackage{times}
\usepackage{oneinchmargins}
\usepackage{tightenum}
\usepackage{enumitem}
\usepackage{ulem}
\usepackage[numbers,sort]{natbib}

\usepackage[hidelinks]{hyperref}

%

\usepackage{setspace}
%\usepackage{epsf}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{times}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage[compact]{titlesec}
\usepackage{url}
\usepackage{nonfloat}
\usepackage{subcaption}
\urlstyle{rm}
%\usepackage{algorithmic}
% \titlespacing*{\section}{0em}{-0ex}{-2ex}
% \titlespacing*{\subsection}{0em}{-0ex}{-2ex}
% \titlespacing*{\subsubsection}{0em}{-0ex}{-2ex}
%\setstretch{0.98}

\usepackage{multirow}

\usepackage{tikz}
\usetikzlibrary{calc}
\newcommand*\circled[1]{\tikz[baseline=-3pt]{
            \node[shape=circle,draw,inner sep=1pt,minimum size=10pt] (char) {\small #1};}}


\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


%\setstretch{2.2}

\setlength{\rightmargin}{0.0in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.4in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}


  \let\oldthebibliography=\thebibliography
  \let\endoldthebibliography=\endthebibliography
  \renewenvironment{thebibliography}[1]{%
    \begin{oldthebibliography}{#1}%
      \setlength{\parskip}{0ex}%
      \setlength{\itemsep}{0ex}%
  }%
  {%
    \end{oldthebibliography}%
  }

%\setlength{\rightmargin}{0.0in}
%\setlength{\topmargin}{-0.5in}
%\setlength{\textheight}{9.6in}
%\setlength{\textwidth}{7.0in}
%\setlength{\oddsidemargin}{-0.275in}
%\setlength{\evensidemargin}{-0.275in}

%\newcommand{\ignore}[1]{}
%\setlength{\rightmargin}{-0.3in}
%\setlength{\topmargin}{-0.6in}
%\setlength{\textheight}{9.8in}
%\setlength{\textwidth}{7.5in}
%\setlength{\oddsidemargin}{-0.55in}
%\setlength{\evensidemargin}{-0.55in}

%\setlength{\topmargin}{-0.5in}
%\setlength{\textheight}{9.2in}
%\setlength{\textwidth}{6.9in}
%\setlength{\oddsidemargin}{-0.25in}
%\setlength{\evensidemargin}{-0.25in}


\usepackage{listings}
  \usepackage{courier}
 \lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{red},
    		frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false      % Leerzeichen in Strings anzeigen ?        
 }
 \lstloadlanguages{% Check Dokumentation for further languages ...
         %[Visual]Basic
         %Pascal
         C
         %C++
         %XML
         %HTML
         %Java
 }

\sloppy
\input{macros}
%\newcommand{\ignore}[1]{}
\input{remark}
%% Leave this on, so we can see them!!!
\remarktrue
\newcommand{\shortenum}{\vspace*{-0.1in}}
%\newcommand{\shortsec}{\vspace*{-0.2in}}
%\newcommand{\sparagraph}[1]{\vspace*{-0.2in}\paragraph{#1}}
%\newcommand{\sparagraph}[1]{\vspace*{-0.15in}\paragraph{#1}}
\newcommand{\sparagraph}[1]{\vspace*{0.0in}\paragraph{#1}}

% add below for confidential distribution
%\usepackage{draftwatermark}
%\SetWatermarkText{DO NOT DISTRIBUTE}
%\SetWatermarkScale{0.4}

\begin{document}

\pagestyle{plain}


%\input{summary}
%\clearpage
%\pagestyle{myheadings}
%\pagenumbering{arabic}

%\newenvironment{smallitemize}{\begin{list}{$\bullet$}{\topsep0.0in\itemsep0.0in\parsep0.0in\partopsep0.0in\itemindent0.1in\leftmargin0.1in}}{\end{list}}
%\newenvironment{smallitemize}{\begin{list}{$\bullet$}{\topsep0.1in\itemsep0.1in\parsep0.1in\partopsep0.1in\itemindent0.1in\leftmargin0.1in}}{\end{list}}
%\newenvironment{smallitemize}{\begin{list}{$\bullet$}{\topsep0.05in\itemsep0.05in\parsep0.0in\partopsep0.05in\itemindent0.05in\leftmargin0.05in}}{\end{list}}
%{\topsep{0in}\itemsep{0.1in}\itemindent{0.1in}}
%%%%%%%%%%%%%%%%%%%%%%%%



\newcommand{\mm}{mm$^2$}
\newcommand{\figtitle}[1]{\textbf{#1}}
\newcommand{\us}{$\mu$s}
\newcommand{\yilun}[1]{{\color{green}\textbf{#1}}}

\definecolor{pink}{rgb}{1.0,0.47,0.6}
\newcommand{\laura}[1]{{\color{pink}\textbf{#1}}}
\newcommand{\arup}[1]{{\color{yellow}\textbf{#1}}}
\newcommand{\hungwei}[1]{{\color{purple}\textbf{#1}}}


\newcommand{\note}[2]{\fixme{$\ll$ #1 $\gg$ #2}}

\newcommand{\myitem}[1]{\item \textbf{#1}}

\newcommand{\yiying}[1]  {\noindent{\color{purple} {\bf \fbox{Yiying}     {\it#1}}}}
\newcommand{\yizhou}[1]  {\noindent{\color{blue} {\bf \fbox{Yizhou}     {\it#1}}}}

\newcommand{\fixme}[1]   {\noindent{\color{red} {\bf \fbox{FIXME}     {\it#1}}}}

twocolumn[
begin{@twocolumnfalse}
begin{center}
{largebf lego: A Disseminated, Distributed OS for Hardware Resource Disaggregation}
%{largebf Splitkernel: A Decomposed OS Architecture for Hardware Resource Disaggregation}
end{center}
%vspace{-0.1in}
centerline{Yizhou Shan, Yutong Huang, Yilun Chen, Yiying Zhang}
centerline{em Purdue University}
smallskip

bigskip
end{@twocolumnfalse}
]

thispagestyle{plain}

\section*{Abstract}

The monolithic server model where a server is the unit of deployment, operation, and failure 
is meeting its limits in the face of several recent hardware and application trends. 
To improve heterogeneity, elasticity, resource utilization, and failure handling in datacenters, 
we believe that datacenters should break monolithic servers into {\em disaggregated, network-attached} hardware components. 
Despite the promising benefits of hardware resource disaggregation, 
no existing OSes or software systems can properly manage it.

We propose a new OS model called {\em \splitkernel} to manage disaggregated systems. 
Splitkernel disseminates traditional OS functionalities into loosely-coupled {\em \microos{}s},
each of which runs on and manages a hardware component.
%The basic idea of \splitkernel\ is simple: when hardware is disaggregated, 
%the OS should be also. 
Using the \splitkernel\ model, we built {\em \lego}, 
a new OS designed for hardware resource disaggregation. 
\lego\ appears to users as a set of distributed servers.
Internally, \lego\ cleanly separates processor, memory, and storage devices 
both at the hardware level and the OS level.
%\lego\ runs a {\em \microos} at each hardware component and appears to applications as a set of distributed servers. 
We implemented \lego\ from scratch and evaluated it by emulating hardware components using commodity servers. 
Our evaluation results show that \lego's performance is comparable to monolithic Linux servers,
while largely improving resource packing and failure rate over monolithic clusters.
% improves performance per dollar by 
%33\% to 66\%
%over Linux on popular datacenter applications like TensorFlow, 
%while providing better heterogeneity, elasticity, and failure isolation.




\if 0
Recently, there is emerging interest in breaking monolithic servers in datacenters into disaggregated hardware components connected via network.
This disaggregated architecture offers the flexibility that is lacking in current datacenters:
resource components can be used, added, reconfigured, removed, grouped dynamically;
they can be heterogeneous and can even fail independently.
The disaggregated hardware architecture brings an interesting question:
{\em How to build a software system to manage and virtualize the disaggregated hardware components?}
Neither traditional OSes built for monolithic computers or distributed systems designed for a cluster of monolithic servers 
can handle the distributed nature of disaggregated hardware resources. 

We propose a new OS model called {\em \splitkernel} to manage disaggregated systems.
The basic idea of \splitkernel\ is simple: 
\textit{when hardware is disaggregated, the operating system should be also}.  
%However, building a \splitkernel\ is far more challenging than simply splitting an existing OS into separated pieces and run them on different hardware components.
%For example, \splitkernel\ needs to handle component and network failure 
%to ensure that the whole system can still function properly after failure.
Using the \splitkernel\ model, we built \lego, 
a new OS designed for disaggregated architecture. 
\lego\ runs a {\em \microos} at each hardware component
and appears to applications as a set of distributed servers.
%\lego\ manages 
%Internally, all components communicate with RPC using a customized RDMA-based network stack.
%\lego\ uses a combination of process checkpointing and data replication to sustain the failure of different types of hardware component.
We evaluated \lego\ by emulating hardware components using commodity monolithic servers.
Our evaluation results show that \lego\ improves performance per dollar by 7\% to 66\% over Linux,
while providing better heterogeneity, elasticity, and failure isolation.
%and adds only 20\% performance overhead to monolithic servers running Linux
%even when it completely separates processor, memory, and storage across network.
\fi


\section{Introduction}
\label{sec:introduction}


%scale and fail components independently

%motivated by recent hardware development in HBM and PIM, 
%we propose to separate memory performance from memory capacity.

For many years, the unit of deployment, operation, and failure in datacenters has been a {\em monolithic server},
one that contains all the hardware resources 
that are needed to run a user program
(typically a processor, some main memory, and a disk or SSD).
This monolithic architecture is meeting its limitations in the face of 
several issues and recent trends in datacenters.

%heterogeneity
Modern datacenters host increasingly heterogeneous hardware~\cite{sigarch-dc,Putnam14-FPGA,TPU,DPU}.
%and new hardware innovations appear at an increasing speed~\cite{Intel3DXpoint,Ahn15-PIM,Bojnordi12}.
%XXX better bib for the above
However, designing new hardware that can fit into monolithic servers and deploying them in datacenters
is a painful and cost-ineffective process 
that often limits the speed datacenters can adopt new hardware.
%
%elasticity
Packaging hardware in a server also makes it difficult to add, remove, or change 
hardware components after deployment~\cite{FB-Wedge100}. 
Pins, space, and power in a server can limit the scalability and capacity of certain hardware~\cite{HP-MemoryEvol,ITRS14,MemoryWall95}.
%such as the {\em capacity wall} main memory is facing
%Moreover, monolithic servers have poor hardware failure isolation. 
%When a component like memory controller fails, the whole server is unusable.
%
%resource utilization
Finally, 
datacenters face a difficult bin-packing problem of fitting applications to physical machines.
Since a process can only use processor and memory in the same machine, 
%only memory local to a node can be allocated to the processes running on the node, 
it is hard to achieve full memory and CPU resource utilization~\cite{Barroso-COMPUTER,Quasar-ASPLOS,PowerNap}.
%At the same time, datacenter applications have diverse and dynamic requirements for different hardware resources. 
%Some require huge amount of memory~\cite{Ongaro11-RamCloud};
%some demand varying amount of various hardware resources dynamically;
%yet some benefit from specialized hardware~\cite{TensorFlow}.
%Unfortunately, with monolithic servers, 
%it is difficult to fully-utilize all available hardware resources because of the physical machine boundary.
%A user process, a VM, or a container can only run on one physical machine and utilize the local hardware resources.
%Moreover, applications cannot easily use specialized hardware,
%and memory-hunger applications are facing memory capacity wall~\cite{HP-MemoryEvol}.
%heterogeneous applications are difficult to utilize all available hardware resources;

%In a monolithic server architecture, %the processor-to-memory ratio is limited and 
%it is difficult to group, add, remove, reconfigure, or reorganize hardware components. %, and

In short, datacenters' current monolithic-server architecture 
cannot fully support the growing hardware heterogeneity in datacenters
%It cannot 
or provide hardware elasticity, failure isolation, and efficient resource utilization.
%we need a flexible datacenter system that can quickly
%adapt to changes in hardware and applications. 
%fit for these needs, since it is difficult to group, add, remove, reconfigure, or
%reorganize hardware resources in monolithic servers.

We believe that datacenters should break monolithic servers
%We envision a more generalized future datacenter architecture that takes a step further by breaking 
%hardware resources into fully %independent, possibly heterogeneous components.
and organize hardware devices like CPU, DRAM, and disks 
as {\em independent, failure-isolated, network-attached} components,
each having its own controller to manage its hardware.
%and can communicate with other components through a fast network.
This {\em hardware resource disaggregation} architecture %({\em \dcrack}) 
is enabled by recent advances in network technologies~\cite{IB-RTT,GenZ,Mellanox-ConnectX6,OpenCAPI,Omni-Path,ccix} 
and the trend towards increasing processing power in hardware controller~\cite{Willow,Ahn15-PIM,Bojnordi12}.
%Recently, there is an emerging trend to move towards a {\em disaggregated} hardware
%architecture that breaks monolithic servers into hardware resource pools
%that are connected with a fast, scalable network~\cite{costa15-r2c2,FireBox-FASTKeynote}.
Hardware resource disaggregation greatly improves heterogeneity, hardware elasticity, 
resource utilization, and failure isolation,
%since hardware components are loosely-coupled and connected via a general network.
since each hardware component can have its own design and operate or fail on its own.
With these benefits, this new architecture has already attracted early attention 
from academia and industry~\cite{OCP,HP-TheMachine,FireBox-FASTKeynote,Lim09-disaggregate,Nitu18-EUROSYS,dRedBox-DATE}.
%largely increases flexibility, allowing
%different components to be easily grouped, reconfigured, added, or removed.
%Another benefit is the possibility of fine-grain failure isolation, which 
%allows applications to continue to run even when a software or hardware component fails.
%Finally, decomposed racks are good for scalability. 
%There is no central controller or networking device and thus does not limit the scale of a rack.
%Each component will have its own controller to manage its hardware and can
%communicate with other components through a fast network.


Hardware resource disaggregation completely shifts the paradigm of computing
and presents a key challenge to system builders:
{\em How to manage and virtualize the distributed, disaggregated hardware components?}

OSes built for monolithic computers assume local accesses to hardware resources 
and cannot handle their distributed nature after disaggregation. 
Datacenter distributed systems are built for managing clusters of servers, not individual hardware components. 
When hardware components spread across the network, OS and distributed systems designed for monolithic servers fall short.
%They do not handle traditional OS operations that are now across distributed components. % in decomposed racks.
Clearly, we need a new system for hardware resource disaggregation.

\if 0
We propose \lego, the {k\em first} OS designed for resource disaggregation. 
%a new OS to manage and virtualize hardware components in a disaggregated datacenter.
The basic idea of \dcos\ is simple:
{\em When hardware is decomposed, the operating system should be also.}
We will build \lego\ from scratch by splitting traditional OS services into {\em micro-OS services}, or {\em component managers}, 
and deploy them on different hardware components.
These component managers can be added, restarted, or reconfigured dynamically without affecting the rest of \dcos.
In addition to managing individual hardware components, 
\lego\ also globally manages the distributed set of hardware components, 
their communication, resource allocation, and failures.
\lego\ provides applications with an interface that is similar to traditional distributed monolithic servers
or to a set of none-cache-coherent processors, 
and it hides its internal resource disaggregation nature
in order to support existing datacenter applications.
\fi

We propose {\em \splitkernel}, an OS architecture for hardware resource disaggregation (Figure~\ref{fig-architecture}).
The basic idea is simple: \textit{When hardware is disaggregated, the OS should be also}.  
A \splitkernel\ breaks traditional operating system functionalities into loosely-coupled {\em \microos{}s},
each running at and managing a hardware component
(\eg, a memory component's \microos\ runs at its hardware memory controller and allocates, protects, and virtualizes its hardware memory resources).
%and runs them on different hardware components. 
Monitors in a \splitkernel\ can be heterogeneous and can be added, removed, 
and restarted dynamically without affecting the rest of the system.

We choose not to support a general coherent view across different components in \splitkernel.
Splitkernel does not assume or rely on a coherent network across hardware components.
All \microos{}s in a \splitkernel\ communicate with each other via {\em network messaging} only.
With our targeted scale, explicit message passing is much more efficient in network bandwidth consumption 
than the alternative of implicitly maintaining cross-component coherence.
%Building a \splitkernel\ is not as simple as just splitting an existing OS into separated pieces. 

Each \splitkernel\ \microos\ can operate on its own for its own functionality and
only communicates with other \microos{}s when there is a need to access resources there.
Globally, \splitkernel\ only performs two tasks: 
orchestrating resource allocation across components 
and handling failure of components.

Following the \splitkernel\ model, 
we built \lego, the {\em first} OS designed for hardware resource disaggregation.
%\lego\ is a distributed, loosely-coupled, failure-independent OS,
%designed and developed from scratch for disaggregated hardware architecture.
%\lego\ runs a {\em component manager} at each hardware component
\lego\ is a distributed OS that appears to applications as a set of virtual servers (called {\em \vnode{}s}).
A \vnode\ can run on multiple processor, memory, and storage components
and one component can host resources for multiple \vnode{}s.
%We use a two-level resource management mechanism for

\lego\ cleanly separates OS functionalities into %{\em micro OS services},
three types of {\em \microos{}s},
process \microos, memory \microos, and storage \microos. %to manage processes, memory, and storage data.
%Each \microos\ runs at the hardware component that it manages.
\lego\ \microos{}s share no or minimal states
and use a customized RDMA-based network stack to communicate with each other.
%\lego\ does not rely on or use any hardware-provided coherence across different hardware components.
%All \microos{}s in \lego\ communicate through explicit messaging over a customized RDMA-based network layer.
%manager, memory manager, and storage manager.
%\lego\ uses coarse-grained, global resource management to allocate, schedule, and coordinate across components. 
%Its design follows several principles.
%\lego\ currently consists of three types of \microos{}s to manage CPU, memory, and storage devices.

\if 0
\lego\ cleanly separates the functionalities of different component managers,
with zero or only minimal dependencies across them. 
This clean separation not only makes it easy to deploy, add, and remove components
but is also essential to ensuring failure independence. 
Our approach is to build each \lego\ manager as a {\em stateless}
service and managers communicate with requests that contain all the information needed to fulfill them.
For example, our storage manager is built in the NFS stateless server style~\cite{sun-nfs}.
\lego\ processor and memory managers are also cleanly separated 
in that memory managers manage all physical memory and virtual memory addresses, their allocation, deallocation, and mappings,
and processor only views virtual memory addresses assigned by memory managers. 
\fi

The biggest challenge and our focus in building \lego\ is the separation of processor and memory and their management.
Modern processors and OSes assume all hardware memory units including main memory, page tables, and TLB are local.
Simply moving all memory hardware and memory management software to behind network will not work.
%The simple separation of moving all memory function units behind network
%to manage disaggregated and distributed memory components
%and to provide good application performance when memory accesses are across the network. 

Based on application properties and hardware trends, 
we propose a hardware plus software solution that cleanly separates processor and memory functionalities,
while meeting application performance requirements.
\lego\ moves all memory hardware units to the disaggregated memory components
and organizes all levels of processor caches as virtual caches that are accessed using virtual memory addresses. 
To improve performance, \lego\ uses a small amount (\eg, 1\GB) of DRAM
organized as a virtual cache below current last-level cache.

\lego\ process \microos\ manages application processes and the extended DRAM-cache.
Memory \microos\ manages all virtual and physical memory space allocation and address mappings. 
\lego\ uses a novel two-level distributed virtual memory space management mechanism,
which ensures efficient foreground memory accesses and balances load and space utilization at allocation time.
Finally, \lego\ uses a space- and performance-efficient memory replication scheme to handle memory failure.
%with the observation that most applications already handle storage failure and our disaggregated architecture 
%mainly adds failure probabilities in memory.
%Replication in memory also has the benefits of sustain memory component failure in addition to processor failure. 
%We also port several drivers from Linux and implement both a customized, efficient network layer and the TCP/IP network stack.
%Overall, our goal is to run any binaries in \lego\ without any modifications. Hence we can
%port current distributed software ecosystem to \lego\ more smoothly.

We implemented \lego\ from scratch on the x86-64 architecture.
\lego\ is fully backward compatible with Linux ABIs
by supporting all common Linux system call APIs.
To evaluate \lego, we emulate disaggregated hardware components using commodity servers.
For example, to emulate a memory component, we only enable two cores of a server.
To emulate a processor component, we limit the accessible physical memory of a server
and use it as \lego{}' software-managed cache. 
%We are using InfiniBand as our high-speed network connection.
We evaluated \lego\ with microbenchmarks, the PARSEC benchmark~\cite{PARSEC}, Filebench~\cite{Filebench},
and two unmodified datacenter applications, TensorFlow~\cite{TensorFlow}
and an open-source MapReduce system called Phoenix~\cite{Ranger07-HPCA}.
Our evaluation results show that running unmodified applications with \lego\ on a disaggregated cluster 
outperforms running them on a memory-restricted monolithic server with local SSD/DRAM swap (by 0.84\x\ to 3.0\x) 
and with network swap (by 0.97\x\ to 3.1\x). 
At the same time, \lego\ can improve resource packing by up to \fixme{XXX\x}
and system mean time to failure by 18\% to 49\%,
while potentially providing better support for hardware heterogeneity and elasticity.
%improves perf/\$ by 33\% to 66\% over monolithic Linux server, 
%with a typical disaggregated cluster ,
%while providing better support for heterogeneity, elasticity, resource utilization, and reliability.
% XXX \lego{}' performance is still worse than Linux and 
%we identified network delay being a major bottleneck.

Overall, this work makes the following contributions:

\begin{itemize}

%\vspace{-0.1in}
\item We propose the concept of \splitkernel, an OS model that works for hardware resource disaggregation.

%\vspace{-0.1in}
\item We built \lego, the first OS that runs on and manages a disaggregated hardware cluster.
%designed for disaggregated systems.

%\vspace{-0.1in}
\item We propose a new hardware architecture to cleanly separate processor and memory hardware functionalities, 
while preserving most of the performance of monolithic server architecture.

%\vspace{-0.1in}
\item We presented a practical method to emulate disaggregated hardware and evaluate OS built for hardware resource disaggregation.
%and evaluated \lego\ with microbenchmarks and unmodified datacenter applications.
Our evaluation results of \lego\ shed light on future software and hardware directions in resource disaggregation.

%\vspace{-0.1in}

\end{itemize}

\lego\ is publicly available at {\small {\em {\url{http://LegoOS.io}}}}.\\

The rest of the paper is organized as follows.
\S{}2 motivates hardware resource disaggregation and a new OS for it.
\S{}3 presents the concept of \splitkernel.
We then discuss the detailed design and implementation of \lego\ in \S{}4 and \S{}5.
\S{}6 presents our evaluation results of \lego.
Finally, we cover related works in \S{}7 and conclude in \S{}8.

\section{Disaggregate Hardware Resource}
%\section{Disaggregate Hardware and OS}
\label{sec:motivation}

This section %discusses the limitations of monolithic servers
%in face of several datacenter trends and 
motivates the hardware resource disaggregation architecture
and discusses the challenges in managing disaggregated hardware.
%and shortages of existing OSes for that.
%We will present a full monetary cost model of our proposed resource disaggregation architecture 
%in \S~\ref{sec:cost}.

\subsection{Limitations of Monolithic Servers}
\label{sec:monolimit}
\input{fig-resource-anal}
% why we must change and this is doomed for future!
A monolithic server has served as the unit of deployment and operation in datacenters
for decades. % (Figure~\ref{fig-monolithic}). 
%Several recent trends in datacenters are challenging 
However, this long-standing {\em server-centric} architecture has several key limitations.
%We believe that these trends will continue to exist in the future
%and we question {\em should datacenters continue to use monolithic servers?}

\noindent{\textit{\uline{Bad support for heterogeneity.}}}
Driven by application needs, new hardware technologies are finding their ways into modern datacenters~\cite{sigarch-dc}.
Datacenters no longer host only commodity servers with CPU, DRAM, and hard disks. 
They include non-traditional and specialized hardware like GPGPU~\cite{GPU-google,GPU-aws}, 
%hardware technologies such as general-purpose GPU (GPGPU),
%specialized processors such as 
TPU~\cite{TPU}, 
%~\cite{DaDianNao,PuDianNao,PARD},
%and various FPGA and ASIC~\cite{Putnam14-FPGA},
DPU~\cite{DPU},
FPGA~\cite{Putnam14-FPGA,Amazon-F1,SmartNIC-nsdi18},
%new and specialized memory components like Oracle 
non-volatile memory~\cite{Intel3DXpoint,facebook-eurosys18},
%\yizhou{switch~\cite{FBOSS-SIGCOMM18,netcache-sosp17},}
%Program-In-Memory and Program-Near-Memory~\cite{Ahn15-PIM,Bojnordi12}.
%and new storage devices like 
and NVMe-based SSDs~\cite{everspin}.
%These hardware technologies have different performance, reliability, and energy-efficiency properties 
%that can benefit different applications.
%hardware and software heterogeneity
%
The monolithic server model tightly couples hardware devices with each other and with a motherboard.
As a result, making new hardware devices work with existing servers is a painful and lengthy process~\cite{Putnam14-FPGA}.
\if 0
System builders first need to design the interface for new devices to fit a specific bus or interconnect protocol.
They then have to find available slots in existing servers in a datacenter 
or purchase new servers to deploy the new devices.
Finally, system software like an OS may also need to change to utilize new hardware features.
\fi
With multiple new devices to be designed and deployed together, 
the problem becomes even worse, like {\em the matrix of hell}~\cite{Docker-PAAS}.
%
The current practice of making new hardware work is not only slow but also expensive.
%The common practice now is to build and deploy new servers for new hardware components. 
Datacenters often need to purchase new servers to host certain hardware.
Other parts of the new servers can go underutilized 
and old servers need to retire to make room for new ones.

\noindent{\textit{\uline{Poor hardware elasticity.}}}
%In a monolithic server architecture, %the processor-to-memory ratio is limited and 
It is difficult to add, move, remove, or reconfigure hardware components
after they have been installed in a monolithic server~\cite{FB-Wedge100}. %, and
Because of this rigidity, datacenters have to plan out server configurations in advance.
However, with today's speed of change in application requirements, such plans have to be adjusted frequently,
and when changes happen, it often comes with waste in existing server hardware.

%failure
Moreover, the failure unit of monolithic servers is coarse.
When a hardware component within a server fails, %(\eg, processor, memory chip, RAID controller), 
the whole server is often unusable and applications running on it can all crash.

\noindent{\textit{\uline{Hardware capacity wall.}}}
Pins, space, and power source in a monolithic server limit the capacity of hardware resource.
For example, the number of memory channels per socket~\cite{ITRS14} and DIMM density 
limit the memory capacity in a single server~\cite{HP-MemoryEvol,MemoryWall95,MemoryCube-ISCA17,Lim09-disaggregate}. 
As the number of cores per server increases, memory capacity is unable to keep pace.
Today's processor-to-memory ratio is already limiting the performance of datacenter applications~\cite{Ongaro11-RamCloud,page-graph}, 
forcing designers to resort to storage techniques~\cite{Zhang-FAST15-FlashGraph} 
or to scaling out~\cite{GU17-NSDI,Dragojevic14-FaRM,Dragojevic15-FaRM,Gonzalez12-OSDI}.
%Gonzalez-OSDI14-GraphX,Low-UAI10-GraphLab,Malewicz-SIGMOD10-Pregel,Kyrola-OSDI12-GraphChi}.
%Compounding the problem, data-intensive applications such as graph and deep learning~\cite{Gonzalez14-OSDI,Low12-VLDB,TensorFlow}
%require large amounts of memory.


%\subsubsection{Inefficient Resource Utilization}
\noindent{\textit{\uline{Inefficient resource utilization.}}}
With a server being the physical boundary of resource allocation, 
it is difficult to fully utilize all resources in a datacenter~\cite{Barroso-COMPUTER,Quasar-ASPLOS,PowerNap}.
We analyzed two production cluster traces: a 29-day Google one~\cite{GoogleTrace}
and a 12-hour Alibaba one~\cite{AliTrace}.
Figure~\ref{fig-resource-anal} plots the aggregated CPU and memory utilization in the two clusters.
For both clusters, only around half of the CPU and memory are utilized.
Interestingly, %our analysis of the Google cluster (Figure~\ref{fig-google-evict}) shows that 
a significant amount of jobs are being evicted at the same time in these traces
(\eg, evicting low-priority jobs to make room for high-priority ones~\cite{Borg}). % (Figures~\ref{fig-googleevict} and \ref{fig-alievict}).
One of the main reasons for resource underutilization in these production clusters is 
the constraint that CPU and memory for a job have to be allocated from 
the same physical machine.
%The diversity and dynamism of today's datacenter workloads
%make resource allocation even more difficult.
%As a result, datacenters need to host more servers,
%costing more %(both CapEx and OpEx) 
%to meet application requirements.

\subsection{Hardware Resource Disaggregation}
The server-centric architecture is a bad fit for the fast changing datacenter hardware, software, and cost needs.
%Why virtual resource disaggregation is not enough?
There is an emerging interest in utilizing resources beyond a local machine~\cite{Gao16-OSDI},
such as distributed memory~\cite{Dragojevic14-FaRM,Nelson15-ATC,Aguilera17-SOCC,Novakovic16-SOCC} and network swapping~\cite{GU17-NSDI}. 
These solutions improve resource utilization over traditional systems.
However, they cannot solve all the issues of monolithic servers (\eg, the first three issues in \S\ref{sec:monolimit}), 
since their hardware model is still a monolithic one.
To fully support the growing heterogeneity in hardware and to provide elasticity and flexibility at the hardware level, 
we should {\em break the monolithic server model.}% into flexible resource components.

We envision a {\em hardware resource disaggregation} architecture 
where hardware resources in traditional servers are decomposed into disseminated, loosely-coupled, network-attached {\em components}.
Each component has its own controller and network interface
and is an {\em independent, failure-isolated} entity.

%benefits
The disaggregated approach largely increases the flexibility of a datacenter.
Applications can freely use resources from any hardware component,
which makes resource allocation easy and efficient.
Different types of hardware resources can {\em scale independently}.
It is easy to add, remove, or reconfigure components.
New types of hardware components can easily be deployed in a datacenter ---
by simply enabling the hardware to talk to the network and adding a new network link to connect it.
Finally, hardware resource disaggregation enables fine-grain failure isolation, % because of decomposed hardware resources.
since one component failure will not affect the rest of a cluster.

Three hardware trends are making resource disaggregation feasible in datacenters.
First, network speed has grown by more than an order of magnitude and has become more scalable in the past decade % faster both in bandwidth and latency
with new technologies like Remote Direct Memory Access ({\em \rdma})~\cite{ibverbs} 
and new topologies and switches~\cite{FireBox-FASTKeynote,costa15-r2c2,Costa-WRSC14},
enabling fast accesses of hardware components that are disaggregated behind network.
%Intel OmniPath~\cite{OmniPath}.
InfiniBand will soon reach 200Gbps and sub-500 nanosecond speed~\cite{Mellanox-ConnectX6,Mellanox-ConnectX6-IB,Mellanox-Switch},
being only 2\x\ to 3\x\ slower than main memory bus in bandwidth.
With main memory bus facing a bandwidth wall~\cite{BW-Wall-ISCA09},
future network bandwidth (at line rate) is even projected to exceed local DRAM bandwidth~\cite{CacheCloud-hotcloud18}.
%These network developments will enable fast accesses to disaggregated hardware. % even when crossing network.
%XXX check if 200Gbps is already available

Second, network interfaces are moving closer to hardware components,
with technologies like Intel OmniPath~\cite{OmniPath},
RDMA~\cite{ibverbs},
and NVMe over Fabrics~\cite{NVMe-fabrics-Inteltalk,NVMe-fabrics}.
As a result, hardware devices will be able to access network directly 
without the need to attach any processors. 

Finally, hardware devices are incorporating more processing power~\cite{Ahn15-PIM,Bojnordi12,Mellanox-SmartNIC,Mellanox-SmartNIC2,Agilio-SmartNIC,Junwhan-ISCA17},
allowing application and OS logics to be offloaded to hardware~\cite{Willow,Kaufmann16-ASPLOS}.
On-device processing power will enable system software to manage disaggregated hardware components locally.

With these hardware trends and the limitations of monolithic servers,
we believe that datacenters should start to seek options in hardware resource disaggregation.
In fact, there have already been several initial hardware proposals in resource disaggregation~\cite{OCP},
including disaggregated memory~\cite{Lim09-disaggregate}, 
disaggregated flash~\cite{FlashDisaggregation,ReFlex},
new power state for disaggregated memory~\cite{Nitu18-EUROSYS},
Intel Rack-Scale System~\cite{IntelRackScale}, 
HP ``The Machine''~\cite{HP-TheMachine,HP-MemoryOS}, 
IBM Composable System~\cite{IBM-Composable},
and Firebox~\cite{FireBox-FASTKeynote}.


\subsection{OS for Resource Disaggregation}
\input{fig-architecture}
Despite various benefits hardware resource disaggregation promises, 
it is still unclear how to manage or utilize disaggregated hardware in a datacenter.
Unfortunately, existing OSes and distributed systems cannot work with this new architecture.
Single-node OSes like Linux view a server as the unit of management and assume all hardware components are local (Figure~\ref{fig-monolithic}).
They cannot handle hardware components that are separated by the network.
Multi-kernel solutions~\cite{Baumann-SOSP09,Barrelfish-DC,Helios-SOSP,fos-SOCC,Hive-SOSP} (Figure~\ref{fig-multikernel}) 
view different cores, processors, or programmable devices within a server separately 
by running a kernel on each core/device and using message passing to communicate across kernels.
However, multi-kernels still manage memory and storage as a whole in a monolithic server.
%disaggregated resource architecture well.
%Monolithic kernels and microkernels (Figure~\ref{fig-monolithic}) manage a monolithic server as a whole.
%Monolithic kernels usually tightly integrate all OS services and run them all in the kernel space, 
%Regardless, they both 
%More important, 
Other single-node OS proposals focus on improving flexibility~\cite{Exokernel,Drawbridge}, scalability~\cite{Corey-OSDI,Disco,K42-EUROSYS}, security~\cite{SealingOS}.
None of these existing OSes handle the distributed nature of hardware resource disaggregation.
%or deal with issues like distributed resource management, failure handling. 
%and require another distributed system to run
%and requires a distributed system on top to manage distributed servers.
%Moreover, when the processor fails, the memory is unusable without any software to manage it.
%for example, with Linux's model of ``viewing everything as a file''.
%Microkernel separates many OS services to run in the user space.

There have been various distributed OS proposals,
many of which date decades back~\cite{Amoeba-Experience,Sprite,MOSIX,V-System,Accent-SOSP,DEMOS-SOSP,Charlotte}.
%The idea of building distributed operating systems is not new and has been explored decades ago.
%Sprite~\cite{}, Amoeba~\cite{Amoeba-Status,Amoeba-Experience}, V~\cite{}, MOSIX~\cite{}, Accent~\cite{}, Charlotte~\cite{}, DEMOS are
Most of these distributed OSes manage a set of monolithic servers
instead of hardware components.
%XXX Amoeba
%and focus on building fast 
%Amoeba proposed 

%Barrelfish: different cores in one machine and across machine
%us: same architecture (only one) in one device and different across devices, hetereo across devices, 
%but homogeneous within device, non-cache coherent across devices, but coherent within devices

Hardware resource disaggregation is fundamentally different from the traditional monolithic server model.
A complete disaggregation of processor, memory, and storage 
means that when managing one of them, there will be no local accesses to the other two.
For example, processors will have no local memory or storage to store user or kernel data.
Memory and storage components will only have limited processing power. %not have no local memory to serve as cache.
If we run an OS only at processors, it will need to remotely manage all other components, causing high network overhead.
OS also needs to manage a distributed set of resources and handle their failure.
Clearly, hardware resource disaggregation calls for a redesign of OS.
%What makes today's disaggregated os different is advances in network and hardware heterogeneity?
%on the other hand, there are new constraints and challenges
%We believe that now is the time to reconsider how OS.. build a new OS
%\subsection{Challenges in Managing Disaggregated Resources}
%Hardware resource disaggregation completely shifts the paradigm of computer hardware and software systems
We identified the following key challenges in building an OS to manage a disaggregated cluster,
%and presents many new challenges,
some of which have previous been identified~\cite{HP-MemoryOS}.

\begin{itemize}
\item How to deliver good performance when application execution involves the access of network-partitioned disaggregated hardware
and currently network is still slower than local buses?

\item How to locally manage individual hardware components with limited hardware resources?

%\item How to communicate across components?

\item How to manage distributed hardware resources efficiently?

\item How to handle the failure of a component without affecting other components and running applications?

\item What abstraction should be exposed to users and how to support existing datacenter applications?

\end{itemize}


\if 0
\noindent{\textit{\uline{Latency performance.}}} %of accessing components over network.}}}
Although network bandwidth is now approaching and could exceed PCIe and memory bus bandwidth in the
future, network latency is still XXX slower.
How can a disaggregated cluster deliver good performance
%When hardware is disaggregated across network,
%it is inevitable to involve network communication during application execution.
%The biggest challenge in building a \splitkernel\ is the separation of processors and memory.
%With current network still being ~5\x\ slower in bandwidth and ~50\x\ slower in latency than memory bus, 
%if all memory accesses are cross the network, 
%application performance will be significantly impacted.

\noindent{\textit{\uline{How to manage distributed hardware resources.}}}
We envision the scale of a disaggregated cluster to be at least one rack, 
hosting hundreds of hardware components.
Managing them are different from traditional....
%To be fully flexible and elastic, applications should be able to utilize resources
%from any components.
%A \splitkernel\ thus needs an efficient mechanism to allocate and manage 
%hardware resources in a disaggregated cluster for different applications.

\noindent{\textit{\uline{Failure handling.}}} %How to handle hardware failures?}}}
Hardware resource disaggregation has both positive and negative implications in reliability.
The failure of disaggregated hardware will not affect other hardware.
But the total number of network-attached components can increase after disaggregation (\S\ref{sec:cost})
and each hardware component can host resources for multiple applications.
%The reliability of a disaggregated cluster largely depends on how
%software manages it and such failure handling should not come with a high 
%space or performance overhead.

\noindent{\textit{\uline{How to support existing datacenter applications?}}}
%Being able to easily port existing datacenter applications is crucial to the adoption of a \splitkernel.
%However, all current datacenter applications are designed to run on monolithic servers.
%To provide complete transparency and backward compatibility, 
%\splitkernel\ should hide the resource disaggregation nature. % in a disaggregated OS.

\noindent{\textit{\uline{Managing individual hardware component.}}} %How to manage individual hardware components with hardware constraints?}}}
%To support heterogeneity and failure independence, 
Each component in a disaggregated cluster should operate on its own as much as possible.
Thus, each \microos\ should manage its own hardware component
without involving other \microos{}s or network communication to them.
A major challenge in achieving this goal is that no component is a full-fledged server. 
They lack hardware resources that OSes and applications traditionally assume to be local.

\fi
\section{The Splitkernel OS Architecture}
\label{sec:design}

We propose {\em \splitkernel}, a new OS architecture for resource disaggregation. 
Figure~\ref{fig-architecture} illustrates \splitkernel's overall architecture.
As suggested by its name, the main idea of \splitkernel\ is to disseminate an OS 
into pieces of different functionalities.
Apart from this main idea, we propose three concepts in \splitkernel:
each disseminated piece of \splitkernel\ runs at the hardware component it manages;
all components communicate by message passing over a non-coherent network;
and \splitkernel\ globally manages resources and component failures.
%and push each of them to run on a hardware component.
%\textit{{when hardware is disaggregated, the OS should be also}}.
%This section defines the \splitkernel\ architecture and its abstraction to users.
%All hardware components in a \splitkernel\ are connected to a general network and 
Splitkernel is a general OS architecture we propose for hardware resource disaggregation 
and there can be many types of implementation of \splitkernel.
Further, we make no assumption on the specific hardware or network type in a disaggregated cluster \splitkernel\ runs on.
Below, we describe the four key concepts of \splitkernel\ in details.
%hardware model

%\subsection{Architecture and Abstraction Overview}
\noindent{\textit{\uline{Split OS functionalities.}}}
Splitkernel breaks traditional OS functionalities into {\em \microos{}s}.
Each \microos\ manages a hardware component, virtualizes and protects its physical resources.
\microos{}s in a \splitkernel\ are loosely-coupled and 
they communicates with other \microos{}s to access their resources. 
%deploys them at individual hardware components.
%\noindent{\textit{\uline{Use hints to improve performance.}}
%\noindent{\textit{\uline{Cleanly separate OS functionalities with no or minimal state sharing.}}}
%Resource disaggregation makes it possible to flexibly deploy heterogeneous hardware devices. 
%Hardware resource disaggregation enables different hardware components to run and fail on their own.
%To retain the benefits of disaggregated hardware, 
For each \microos\ to operate on its own with minimal dependency on other \microos{}s,
\splitkernel\ uses a stateless design by sharing no or minimal {\em states}, or metadata,
across \microos{}s.
%However, a clean separation is hard, especially when every hardware component has limited amount of resources.
%Sometimes, it even requires hardware changes (\S\ref{sec:hardware}).
%However, we can only obtain the benefits 
%if the software managing these hardware devices also provides good support for flexibility, elasticity, and heterogeneity.
%To achieve the same level of hardware disaggregation in software, 


\noindent{\textit{\uline{Run \microos{}s at hardware components.}}}
We expect each non-processor hardware component in a disaggregated cluster to have a controller that 
can run a \microos.
A hardware controller can be a low-power general-purpose core, an ASIC, or an FPGA.
Each \microos\ in \splitkernel\ can use its own implementation to manage the hardware component it runs on.
This design makes it easy to integrate heterogeneous hardware in datacenters:
to deploy a new hardware device, its developers only need to build the device,
implement a \microos\ to manage it and communicate with the rest of \splitkernel\,
and attach the device to the network. 
Similarly, it is easy to reconfigure, restart, and remove hardware components with \splitkernel.
%A \microos\ can be added, restarted, or stopped dynamically without affecting the rest of the OS.

\noindent{\textit{\uline{Message passing over non-coherent network.}}}
Unlike other proposals of disaggregated systems that rely on coherent network hardware~\cite{HP-TheMachine},
we choose not to require coherence across components at the hardware or network layer
mainly because maintaining coherence at these layers for our targeted cluster scale 
would consume significant network bandwidth for coherence traffic.
Instead, all communication across components in \lego\ is through {\em network messaging}.
Splitkernel still retains the cache coherence guarantee that hardware already provides within a device (\eg, cache coherence across cores in a CPU).

\noindent{\textit{\uline{Global resource management and failure handling.}}}
One hardware component can host resources for multiple applications and its failure can affect all these applications.
In addition to managing individual components, \splitkernel\ also needs to 
globally manage resources and failure.
To minimize performance and scalability bottleneck,
\splitkernel\ only involves global resource management occasionally for coarse-grained decisions, 
%\splitkernel\ uses a two-level resource management mechanism, 
%where a global manager performs coarse-grained decisions 
while individual \microos{}s make their own fine-grained decisions.
\splitkernel\ handles component failure by adding redundancy for recovery.
%\noindent{\textit{\uline{Handle failure with minimal space and performance overhead.}}}
%Isolating and recovering from failure is important
%but should not come at the cost of high performance or memory space overhead during failure-free periods. 
%For example, a full replication on every memory access would be costly both in space and in performance.
%Our idea is to reduce full replication costs by performing partial, best-effort replication
%and re-use the already existing application-provided failure handling mechanisms.



%include abstraction??? XXX
\if 0
\splitkernel appears to users as a distributed set of 
%\noindent{\textit{\uline{Appear as a cluster of nodes.}}}
A disaggregated cluster can appear as one giant machine by hiding all its distributed components from users~\cite{Amoeba-Experience};
it can also expose all the disaggregated hardware components to users.
We choose an abstraction in between: users view a distributed set of nodes that \splitkernel\ exposes,
and \splitkernel\ hides the hardware disaggregation nature from users.
Most existing datacenter applications are built with the knowledge of distributed servers.
Maintaining the distributed view can seamlessly support all these existing distributed datacenter applications.
Moreover, it lets us reuse application-provided failure handling and message passing (\S\ref{sec:failure}).

\splitkernel\ not only manages individual hardware components but also orchestrates the communication
among components, manages their failure, and globally allocates resources across hardware components.
Combining all \microos{}s, \splitkernel\ is a distributed OS that manages all hardware components in a disaggregated cluster.
%in a disaggregated datacenter.
\fi

% XXX benefits
%Unlike many tightly-coupled existing OSes, \splitkernel's design of disaggregating OS functionalities
%not only makes it easy to deploy, add, and remove components
%but is also essential to achieving heterogeneity and ensuring failure independence. 
%No additional distributed system layer or hardware controllers are needed.
%, nor is any hardware controller.
%Figure~\ref{fig-architecture} illustrates the overall architecture of \splitkernel. 

%can have different types of memory, e.g., some use 4K page, some use huge page, some use segmentation
%different TLB impl
%applications can choose different memory device for their own needs
%can fail independently


%Internally, \splitkernel\ consists of a distributed set of disaggregated hardware components
%and can map different amount of hardware resources as one ``node'' to the application dynamically.

%However, as we will show in Section~\ref{sec:design}, 
%a clean separation of OS functionalities is hard and can sometimes come with tradeoffs in performance.


\if 0
\splitkernel\ is far from just decomposing an OS. 
A main challenge it needs to solve is to handle failure 
With the scale of \dcrack, failures are inevitable
Our goal is to 

contrary to current practice of not replicating memory, we believe that memory can and should be replicated. 
more memory space with disaggregated memory, no memory wall
has to handle memory failure
memory access already cross network, replication will not add too much cost
can also be used to improve parallelism
Given our planned scale of hardware components, failures are inevitable.
%a hardware component may fail, the software running on it may crash, or the network to it may fail or be too slow. 
When monolithic computers experience a hardware or OS crash, they typically terminate entire applications~\cite{Depoutovitch10-otherworld}.
However, many datacenter applications require high availability~\cite{MongoDB} or are long running~\cite{TensorFlow},
and it would be unacceptable to terminate a whole application when one part of the disaggregated system fails.
Since an application running on disaggregated architecture
can be using a set of hardware components and one component can host multiple applications, 
a component failure should not affect application execution or make the whole system unavailable.
\splitkernel\ needs to isolate failures and quickly recover from them. %offer even under failure scenarios.
Moreover, such approach should not come at a high cost of space overhead.
In particular, a memory space overhead of 2\x\ (\eg, the result of a full two-way memory content replication) 
can be prohibitive to datacenters that desire low monetary costs.
\fi

%We will seek various ways to reduce this overhead while still delivering our failure guarantees.

%We envision the scale of disaggregated architecture to have at least thousands of components.
%With this scale, failure is inevitable. 

%It will first need an efficient mechanism to collect resource utilization information,
%to execute resource management tasks, and to communicate across hardware components.
%Such mechanism should be low latency, consume minimal network bandwidth, and use small amount of space to store metadata.
%\lego\ then needs to deploy a set of good policies of global resource allocation and scheduling.



\if 0
\noindent{\textit{\uline{Minimize network communication}}
delay allocation
prefetching
piggy-back

\noindent{\textit{\uline{Layered Abstraction}}
our design only specifies the abstraction of each layer and interface between components
how each service is implemented is flexible and can be heterogeneous.

\noindent{\textit{\uline{Separate Policy from Mechanism}
Global Resource Management
different types of resources can register with the global resource manager
\fi


\if 0
For example, memory protection should be enforced at the memory side for a clean separation of functionality.
However, performing an early protection check at the processor side can avoid sending
unnecessary traffic (protection violations) to memory.
As another example, 
To cleanly separate components, we will build each manager 
as a {\em stateless} service and different managers communicate
with requests that contain all the information needed to fulfill them.
For example, a memory access request or a storage I/O request sent from a processor will 
include all the information the memory component or the storage component will need to fulfill the requests,
without the need to store any historical states at these components.
This stateless design will also make failure handling easier, 
since during recovery components do not need to restore states. 

The first decision we made to build \lego\ is to cleanly separate 
pluggable, heterogeneous implementation

clean separation from hardware to system software
%\noindent{\textit{\uline{Stateless Control}
long-running, non-user-interactive applications

processor only sees virtual memory addresses

user space code only running on processors, 
all other components run only one mode that is privileged, equivalent to kernel mode.

the decision of using only virtual memory address at processors has its own tradeoffs.

\noindent{\textit{\uline{Run OS services at hardware}}
Our next idea of \splitkernel\ is to run micro-OS services at individual hardware.
%why

challenge: limited hardware resources
%\noindent{\textit{\uline{How to meet hardware constraints?}}
%The \dcrack\ architecture 
Running \splitkernel\ services on hardware components raise new design and implementation challenges,
because each type of these components have their new constraints.
Processors will only have limited memory, 
while other types of resource components like memory and storage devices will only have limited processing power.
There will also be no local persistent storage for processors or memory to use.

%\noindent{\textit{\uline{How to cleanly separate OS services?}}
Different hardware components have different constraints.
For example, memory components will have plenty of memory but has limited processing power,
usually just a memory controller,
while processors will have limited amount of memory.
The disaggregated OS component manager should be designed with these constraints in mind.
Different managers should also be cleanly separated in their functionalities
and should have zero or minimal dependencies.
\fi

\if 0
\subsection{Challenges}
There are three main challenges in building a disaggregated OS:

\noindent{\textit{\uline{How to provide good application performance when hardware accesses are across the network?}}
Current datacenters and future racks will be able to host thousands of hardware components~\cite{HP-Moonshot,HP-m800},
the scale that we target to support.
This scale offers applications great parallelism.
However, the disaggregated approach inevitably adds a performance cost in network communication.
Managing thousands of distributed components may also create performance overhead. %could also introduce performance overhead. %There is also 
How to minimize or hide these costs is crucial to the success of \lego.


\noindent{\textit{\uline{How to support existing datacenter applications?}}
%All current datacenter applications are designed to run on monolithic servers.
%To provide complete transparency and backward compatibility, 
%we need to hide the resource disaggregation nature in a disaggregated OS.
%However, doing so can lead to reduced flexibility in resource management. 
%Applications need a transparent and user-familiar abstraction.
There is a host of popular datacenter applications~\cite{TensorFlow,Gonzalez14-OSDI,Malewicz10-SIGMOD,Kyrola12-OSDI,Low10-UAI,Low12-VLDB,Gonzalez12-OSDI,MongoDB}.
Being able to easily port these existing applications and efficiently run them will assist in the adoption of \lego.
%\myitemit{Q5: How to support existing datacenter applications?}
%Applications need a transparent and user-familiar abstraction.
However, all current datacenter applications are designed to run on monolithic servers.
To provide complete transparency and backward compatibility, 
we need to hide the resource disaggregation nature in a disaggregated OS.
%However, doing so can lead to reduced flexibility in resource management. 
%Workload or jobs will be distributed across different running instances in different
%computers. With a new unified-interface exposed by disaggregated operating system to represent
%all available hardware components, the model to run distributed systems is changed.

\fi
\section{\lego\ Design}
\label{sec:design}

Based on the \splitkernel\ architecture,
we built {\em \lego}, the first OS designed for hardware resource disaggregation.
\lego\ is a research prototype that demonstrates the feasibility of \splitkernel,
but it is not the only way to build a \splitkernel\ OS.
%but it demonstrates a 
%is a new OS architecture that can have different implementations.
This section first describes the abstraction \lego\ exposes to users
and then introduces the disaggregated hardware model we propose for \lego.
Next, we explain the design of \lego' process, memory, and storage \microos{}s.
We end this section with the discussion of \lego' global resource management and failure handling mechanisms.

Overall, \lego\ achieves the following design goals:

\begin{itemize}

%\vspace{-0.1in}
%\item Great support for hardware disaggregation which in turn offers heterogeneity, elasticity, and flexibility.
%\vspace{-0.1in}

\item Clean separation of processor, memory, and storage functionalities.
%\vspace{-0.1in}

\item Runs at hardware components and fits well to device constraints.
%requires small hardware resources and can be easily deployed on hardware. 
%\vspace{-0.1in}

\item Comparable performance as monolithic Linux servers.
%\vspace{-0.1in}

\item Efficient resource management and memory failure handling, both in space and in performance. % and performance-efficient memory replication scheme.
%\vspace{-0.1in}

\item Easy-to-use, backward compatible user interface.
%\vspace{-0.1in}

\item Supports common Linux system call interfaces.
%\vspace{-0.1in}
%\vspace{-0.1in}

\end{itemize}

%To achieve the above design goals, we XXX several design principles in \lego.

\subsection{Abstraction and Usage Model}
\lego\ exposes a distributed set of {\em virtual nodes}, or {\em \vnode}, to users.
From users' point of view, a \vnode\ is like a virtual machine. 
Multiple users can run in a \vnode\ and each user can run multiple processes.
Each \vnode\ has a unique ID, a unique virtual IP address, %({\em \vip}),
and its own storage mount point. % ({\em \vmount}).
\lego\ protects and isolates the resources given to each \vnode\ from others.
Internally, one \vnode\ can run on multiple processors, multiple memory components,
and multiple storage components.
At the same time, each hardware component can host resources for more than one \vnode.
The internal execution status is transparent to \lego\ users;
they do not know which physical components their applications run on.
%why

%As discussed in Section~\ref{sec:principles}, we will 
\lego\ provides a non-cache-coherent interface across processors.
Thus, we do not support writable memory sharing across processors
and do not split a process across multiple processors
(\lego\ allows read-only sharing, since it will not cause coherence issues).
Using process as the unit of deployment across processors is sufficient. % and desired for existing datacenter applications,
Among the applications we studied, none use any inter-process {\em writable} memory sharing
Other studies~\cite{OVC} also show that inter-process sharing of writable in-memory data is rare.
Although \lego\ does not share (writable) memory across processors, 
different processors can use separate physical memory spaces on the same memory component.

One of the initial decisions we made when building \lego\ is to support Linux system call interface 
and unmodified Linux ABI,
because doing so can greatly ease the adoption of \lego.
Distributed applications that run on Linux can seamlessly run on a \lego\ cluster
by requesting a set of \vnode{}s from \lego\ and use their virtual IP addresses to communicate.
%Applications that fit well to \lego\ are computation intensive 
%or exhibit fairly good memory locality;
%many datacenter applications fall into this category~\cite{TensorFlow,Gonzalez12-OSDI,XXX}.
%We do not target storage-intensive applications currently and leave the 
%optimization of disaggregated storage for future work.

\input{fig-lego-arch}
\subsection{Hardware Architecture}
\label{sec:hardware}
%\splitkernel\ views each hardware component as an independent unit connected to a global network
%and it is not specific to any hardware device architecture.
%Splitkernel supports hardware heterogeneity and allows different hardware architectures to work in a disaggregated cluster.
To demonstrate the feasibility of hardware resource disaggregation,
we propose a processor and memory architecture designed 
within today's network, processor, and memory performance and hardware constraints
(Fig~\ref{fig-hw-arch}).

\if 0
There are three reasons why we think this hardware model will be a viable one in the near future.
First, hardware devices are having increasing amount of built-in processing power.
cite programmable NICs, integrated NICs, PIM, smart SSD

Although we push for heterogeneity. there are a few common properties we think 
hardware devices can use and benefit from.
\fi

\noindent{\textit{\uline{Separating processor and memory functionalities.}}}
%To separate processor and memory functionalities, 
\lego\ moves all hardware memory functionalities to memory components 
(e.g., page tables, TLBs) and leaves {\em only} caches at processor side. 
%This separation of processor and memory hardware functionalities offers ** benefits.
With a clean separation of processor and memory hardware units, 
the allocation and management of memory can be completely transparent to processors.
%and transparent to processors. 
%can be flexibly allocated at memory component and 
Each memory component can choose their own memory allocation technique
and virtual to physical memory address mappings (\eg, segmentation). 
A side benefit of moving TLB to memory is that processor cache accesses would not 
need to wait for TLB results~\cite{Kaxiras-ISCA13}, potentially making processors run faster.
%reason for moving TLB: TLB miss cost would be high and cite ASPLOS'18 paper on why TLB miss can be frequent

\noindent{\textit{\uline{Processor virtual caches.}}}
After moving all memory functionalities to memory, %including memory address mapping to memory, 
processors will only see virtual addresses and have to use virtual memory addresses to access its caches. 
%Since processors only see virtual memory addresses and there is no address translation at processor side, 
%Because of this, all processor caches can only be virtual.
Because of this, \lego\ organizes all levels of processor caches as {\em virtual caches}~\cite{Goodman-ASPLOS87,Wang-ISCA89},
\ie, virtually-indexed and virtually-tagged caches.

A virtual cache has two potential problems, commonly known as synonyms and homonyms~\cite{CacheMemory82}.
Synonyms happens when a physical address maps to multiple virtual addresses (and thus multiple virtual cache lines) 
as a result of memory sharing across processes,
and the update of one virtual cache line will not reflect to other lines that share the data.
%Under such situation, 
%If an update to one virtual cache line is not reflected to its synonyms, 
%other processes will not be able to see the update of this shared address.
Since \lego\ does not allow writable inter-process memory sharing,
it will not have the synonyms problem.
The homonyms problem happens when two address spaces use the same virtual address for their own different data.
%and it can happen when two address spaces use the same virtual address for different, independent data.
Similar to previous solutions~\cite{OVC}, we solve homonyms by storing an address space ID (ASID) with each cache line,
and differentiate a virtual address in different address spaces using ASID.
% must be reflected in all others, which could be cached in different places.
%We solve this problem by detecting and making a duplication of shared memory pages. 
%\yizhou{find if any previous papers also do this and add citations here}

\noindent{\textit{\uline{Separating memory for performance and for capacity.}}}
%Extended cache, victim cache, and kernel physical memory.}
%study results
Previous studies~\cite{Gao16-OSDI,GU17-NSDI} and our own show that today's network speed 
cannot meet application performance requirements if all memory accesses are behind the network. 
Fortunately, many modern datacenter applications exhibit strong memory access temporal locality.
%from our application study and others~\cite{XXX}.
%We performed a study on the memory access patterns of a set modern datacenter applications.
%including TensorFlow, PowerGraph, and 
For example, we found 90\% of memory accesses in PowerGraph~\cite{Gonzalez12-OSDI} go to just 0.06\% of total memory
and 95\% go to 3.1\% of memory
(22\% and 36\% for TensorFlow~\cite{TensorFlow} respectively
and 5.1\% and 6.6\% for Phoenix~\cite{Ranger07-HPCA}).
%PG 90% 0.0063G 95% 0.301G 100% 9.68G
%TF 90% 0.608G 95% 0.968G 100% 2.7G
There is a clear separation of applications' working sets that are frequently accessed 
and the rest of their memory footprints that are large but rarely accessed.
% XXXgive TF mem access pattern
%batch size

%These findings demonstrate that memory content can separated into two categories:
%those that are accessed frequently and thus affecting application performance, or {\em performant memory},
%and those that have a large footprint but infrequently accessed, or {\em capacity memory}.
With this dual pattern of accessing in-memory data, we propose to separate hardware memory into two categories and organize them differently:
a small amount of fast memory (\eg, 1\GB) at processor
and a much larger memory behind the network.
%and to leave a the former with processor and move the latter to the disaggregated, network-attached memory.
The processor side's memory can be regular DRAM 
or the on-die HBM~\cite{HBM-JEDEC,Knights-Landing}. 
%that is packaged on die~\cite{Knights-Landing}.
The disaggregated memory uses regular DRAM.
Without local memory's capacity wall, each disaggregated memory can have large capacity (\eg, 1\TB).
%or just with normal DRAM.

Different from previous proposals~\cite{Lim09-disaggregate}, 
we propose to organize the processor-side's DRAM/HBM as cache rather than main memory
for a clean separation of processor and memory functionalities.
We place this cache under the current processor Last-Level Cache (LLC)
and call it extended cache, or {\em \excache}.
\excache\ serves as another layer in the memory hierarchy between LLC and memory behind the network.
Like all other levels of cache at \lego\ processor, \excache\ is a virtual, inclusive cache.
%\yizhou{can you think of the benefits of organizing it as cache rather than memory?}
\excache\ can serve hot memory accesses fast, while disaggregated memory can provide the capacity applications desire. 

\if 0
is still needed in the near future
cite OSDI study paper
benefits of using a small DRAM as cache instead of as memory:
saving metadata and overhead of accessing metadata.
For example, instead of performing page table walk in DRAM organized using pages, 
we can obtain cache index with simple, static calculation and no additional DRAM accesses.
software-managed last-level cache
cite software-managed caches
\fi

Finally, we use a small amount of DRAM at processor for \lego{}' own kernel data usages,
accessed directly with physical memory addresses. 
%\lego\ accesses this memory directly with physical physical addresses.
%This kernel physical memory is small.
\lego\ ensures that all its own data fits in this space to avoid going to disaggregated memory.
%The processor-local kernel physical memory makes \lego{}' own kernel process run fast.
%and since it is a separate hardware, it also prevents the Meltdown security issue~\cite{meltdown}.
%networking
%cite R2C2, etc.

With our design, processors do not need any address mappings:
\lego\ accesses all processor-side DRAM/HBM using physical memory addresses
and does simple calculation to locate the \excache\ set for a memory access.
%and use physical address directly for kernel physical memory.
We use software~\cite{softvm-HPCA97,Tsai-ISCA17} (\lego) to manage \excache\ and the kernel physical memory,
although they can all be implemented in hardware too.
%\fixme{fill in benefits of software managed cache}
%We will detail our management of these caches soon in this Section
%and our hardware emulation of them in Section~\ref{sec:procimpl}. 

\subsection{Processor Management}
Processors run user programs and \lego\ {\em process \microos{}}
to manage CPU cores and \excache. 
%limited memory space for kernel, care taken to make kernel's metadata small

%no shared memory across processes
%all threads of the same process run on one processor

%\subsubsection{Spawning New Processes}
%process creation
%fork and exec


\subsubsection{Process Management and Scheduling}
At every processor, \lego\ uses a simple local thread scheduling model 
that targets datacenter applications 
(we will discuss global scheduling in \S~\ref{sec:grm}).
%\lego\ minimal scheduling and hardware interrupts
\lego\ dedicates a small amount of cores for kernel background threads 
(currently two to four)
and uses the rest of the cores for application threads.
When a new process starts, \lego\ uses a global policy to choose a processor to run it at (\S~\ref{sec:grm}).
Afterwards, \lego\ schedules new threads the process spawns on the same processor 
by choosing the cores that host fewest threads.
After assigning a thread to a core, 
we let it run to the end with no scheduling or kernel preemption under common scenarios.
For example, we do not use any network interrupts 
and let threads busy wait on the completion of outstanding network requests, 
since a network request in \lego\ is fast 
(\eg, fetching an \excache\ line from remote memory takes around 6.5\mus).
%performs a round-robin scheduling on the core.
%Compared to the low-latency network access (XXX\mus), 
%the overhead of scheduling a thread off and on a core is high. 
\lego\ improves the overall processor utilization in a disaggregated cluster,
since it can freely schedule processes on any processors without considering memory allocation.
Thus, we do not push for perfect core utilization when scheduling individual threads
and instead aim to minimize scheduling and context switch performance overheads.
%with no or minimal scheduling to improve performance.
Only when a processor has to schedule %more than one thread on a core 
%(\eg, when a process runs
more threads than its cores will
\lego\ do preemptive scheduling of threads on a core.

%partition cores vertically, 
%since our model is that one processor is mostly used by just one application
%processor can be smaller than now (not many cores) 
%and/or more specialized
%modern datacenters are already like that

\subsubsection{Software-Managed Extended Cache}
\lego\ process \microos\ configures and manages \excache.
During the processor's boot time, \lego\ configures the set associativity of \excache\
and its cache replacement policies.

When an \excache\ miss happens, 
\lego\ process \microos\ fetches the line from remote memory and inserts it to \excache.
%and inserts it into a proper \excache\ set
%(see \S~\ref{} for an optimization we did to reduce this overhead).
If the \excache\ set is full, \lego\ will first evict a line in the set.
\lego\ currently uses a per-set LRU eviction policy.
For each \excache\ set, it maintains an LRU list and evicts 
a least-recently-used clean line first (and then dirty LRU one),
similar to the traditional CLOCK algorithm.
%\lego\ can easily change to other replacement policies. 
%In fact, we also implemented a per-set FIFO replacement mechanism, for experimental comparison.
\lego\ throws away an evicted line if it is clean
and writes it back to remote memory components if it is dirty.

To maximize parallelism while reducing scheduling overhead,
we let the thread that causes the \excache\ miss perform all eviction and write-back tasks.
\lego\ only uses one background kernel thread to sweep all \excache\ sets and adjust their LRU lists.

\subsubsection{Supporting Linux Syscall Interface}
One of our early decisions is to support Linux ABIs for backward compatibility
and easy adoption of \lego.
To meet this goal, we implemented most common Linux system call APIs. 
The difficulty we met is that Linux has a stateful and tightly-coupled design,
while \lego\ aims at clean separation of OS functionalities with a stateless design.
For example, many entities are implemented as ``files'' 
and exposed to applications with file descriptors ({\em fd}).
To solve this problem, we maintain a {\em state session} indirection layer~\cite{Sandberg-NFS-85}
at each processor to store Linux states and translate the stateful Linux interface to stateless \lego\ interface.


\subsection{Memory Management}
\lego\ lets a process address space span multiple memory components
to achieve efficient memory space utilization and good parallelism.
Memory management is crucial to \lego{}' performance,
resource utilization, elasticity, and reliability.
We use memory components for three types of data:
anonymous memory (\ie, heaps, stack), 
memory-mapped files, and storage buffer cache.
\lego\ {\em memory \microos{}}
manages both the virtual and physical memory address spaces,
their allocation, deallocation, and address mappings.
It also performs the actual memory read and write.
No user processes run on memory components 
and they run completely in the kernel mode
(same is true for storage components). 
%There is only o

%no context switching

%stores all application processes' memory information and mapping 

\if 0
Apart from addressing and protection, the memory layer also needs to handle memory allocation and deallocation. 
We will use a global memory resource manager to decide which memory component to allocate from (Section~\ref{sec:resource}); 
this memory component will then perform memory allocation locally.

Each individual memory component has the freedom to choose their own forms of virtual memory mapping, permission tables, and 
physical memory allocation strategies, providing a great support for flexibility and heterogeneity.

separate physical and virtual memory mgmt

we only need very few threads to achieve good memory performance (two in our experiments)
\fi

\lego\ decomposes the virtual memory and physical memory management
in both mechanism and policy
and performs different management tasks at three locations:
a global memory resource manager ({\em \gmm}),
each process' {\em home memory} component,
and memory components that host data for the process.
A home memory component can also host process data.

\subsubsection{Virtual Memory Space Management}
%Memory components in \lego\ manages
%Processors only request virtual memory areas ({\em vma}s) from memory (at 
%each memory stores its own vma
%only shared states is the vregion array
%when fail, easy to recover
We propose a two-level approach to manage distributed virtual memory spaces,
where home memory makes coarse-grained decision on high-level virtual memory allocation
and regular memory components perform fine-grained virtual memory allocation.
This approach minimizes network communication during both normal memory accesses and virtual memory operations,
while ensuring good load balancing and memory utilization.
Figure~\ref{fig-dist-vma} demonstrates the data structures used. % in virtual memory space management.

At the higher level, we split each virtual memory address space into coarse-grained, fix-sized {\em virtual regions},
or {\em \vregion{}s} (\eg, of 1\GB).
Each \vregion\ that contains allocated virtual memory addresses (an active \vregion) is {\em owned} by a memory component.
The owner of a \vregion\ handles all memory accesses and virtual memory requests within the \vregion.
%We use a static array to store the memory component IDs in charge of active \vregion{}s for fast accesses.
%With the help of \vregion\ array, processors can directly send memory access requests to the 

The lower level stores user process virtual memory area (vma) information,
such as virtual address ranges and permissions, in vma trees.
The owner of an active \vregion\ stores one vma tree for it,
with each node in the tree being one vma.
% for  active \vregion\ stores one vma tree at the owner memory component of the \vregion.
A user-perceived virtual memory range can split across multiple memory components,
but only one memory component owns a \vregion.
%\lego\ supports the Linux \brk, \mmap, \munmap, and \mremap\ system calls,
%and thus keeps the virtual memory area ({\em vma}) concept 
%where each vma is a virtual memory address range that has a certain permission.

\lego\ assigns a home memory component to each running process at process start time.
The home memory accepts and oversees all system calls related to virtual memory space allocation, 
\eg, \brk, \mmap, \munmap, and \mremap.
\vregion\ owners perform the actual virtual memory allocation and vma tree set up.
Home memory can also be the owner of \vregion{}s.
Home memory does not maintain any information of memory that belong to \vregion{}s owned by other memory components.
%It does not maintain any information about other memory components' physical memory allocation status.
It only keeps the information of which memory component owns a \vregion\ (in a {\em \vregion\ array})
and how much free virtual memory space is left in each \vregion.
These metadata can be easily reconstructed after a home memory failure.
%which oversees virtual memory address set up.

When an application wants to allocate a virtual memory space,
%or maps a new virtual address range in an inactive \vregion,
the processor forwards the allocation request 
to the home memory (\circled{1} in Figure~\ref{fig-dist-vma}).
The home memory uses its own information of available virtual memory space in \vregion{}s
to find one or multiple \vregion{}s that best fit the requested amount of virtual memory space.
If a new \vregion\ is needed, the home memory contacts the \gmm\ (\circled{2} and \circled{3}) 
to find a candidate memory component to own the \vregion.
\gmm\ makes this decision based on available physical memory space and access load on different memory components (\S~\ref{sec:grm}).
If the candidate memory is not the home memory, the home memory next forwards the request to that memory (\circled{4}),
which then performs local virtual memory area allocation and sets up the proper vma tree. 
Afterwards, the processor directly sends memory access requests to the owner of the \vregion\ where the memory access falls into
(\eg, \circled{a} and \circled{c} in Figure~\ref{fig-dist-vma}).


%Benefits and discussion
%good load balancing and resource utilization while minimize metadata and network traffic
\lego{}' mechanism of distributed virtual memory management is efficient and it cleanly separates memory operations from processors.
%To efficiently manage virtual memory space and minimize network communication, 
Processors directly hand over memory-related system call requests to memory components.
% not perform any memory-related operations.
Processors only cache a copy of the \vregion\ array for fast memory accesses.
To fill a cache miss or to flush a dirty cache line, 
the processor looks up the cached \vregion\ array to find its owner memory component and sends the request to it.
% XXX To avoid contacting \gmm\ and making decision on every new vma allocation/deallocation.
%Each owner of a \vregion\ {\em promises} to 
%Only the memory in charge of a \vregion\ stores its vma tree.
%avoid store information about every vma

%All memory components including the home memory can store memory data for any process.


%The home node handles memory-related operations (syscalls)
%including \mmap, \munmap, \mremap, and program loading.

%with one processor, two memory components, and one 


\subsubsection{Physical Mem Mgmt and Address Mapping}
Each memory component manages the physical memory allocation for data that falls into the
\vregion\ that it owns.
Each memory component can choose their own way of physical memory allocation
and own mechanism of virtual-to-physical memory address mapping.
We use a simple allocation algorithm and an efficient address mapping mechanism, 
as will be discussed in \S~\ref{sec:memimpl}.
We delay physical memory allocation to as late as possible, 
\ie, when a memory page contains dirty data that has to be written to memory.
We detail our optimization technique to delay physical allocation next.

%zerofill
\subsubsection{Optimization on Memory Accesses}
\label{sec:zerofill}
With our strawman memory design, 
all \excache\ misses will go to memory components.
We soon found that a large performance overhead in running real applications 
is caused by filling empty \excache, \ie, {\em cold misses}.
To reduce the performance overhead of cold misses, we propose a new technique 
to avoid accessing the disaggregated memory on first memory accesses.

The basic idea is simple: since the initial content of anonymous memory 
(non-file-backed memory) is zero, %undefined and can be any data, 
we can directly allocate a cache line with empty content
in \excache\ for the first access to 
anonymous memory instead of going to memory
(we call such cache lines {\em processor-local lines}).
Before processor-local lines are evicted, they only lives in processor \excache;
no memory components are aware of them
and will not allocate any physical memory or establish any virtual-to-physical
memory mapping.
For proper memory protection, 
processor \microos\ records the protection information of newly created anonymous memory regions
and checks access permission at the time of allocating processor-local lines.
% to catch stray write.
%The permission is cached at processor when anonymous region was created.
%we do not have the danger of other processors also accessing.
%Thus, it is safe to just have the anonymous memory in \excache.

When a processor-local cache line becomes dirty and needs to be flushed, 
the processor \microos\ sends it to its owner memory, which then
allocates physical memory space and establishes a virtual-to-physical memory mapping.
Essentially, \lego\ {\em delays physical memory allocation until write time}.
Notice that it is safe to only maintain processor-local lines at a processor \excache\ 
without any other processors knowing them, 
since processors in \lego\ do not share any memory
and other processors will not access this data.

%Similarly, if a 

% XXX violation of stateless design principle of \splitkernel.

\subsection{Storage Management}
\lego\ supports a hierachical file interface that is backward compatible with POSIX 
through its \vnode\ abstraction. 
Users can store their directories and files under their \vnode{}s' mount points
and perform normal read, write, and other accesses to them.

\lego\ implements core storage functionalities at storage components.
To cleanly separate storage functionalities, \lego\ uses a stateless storage server design, 
where each I/O request to the storage server contains all the information needed to 
fulfill this request, \eg, full path name, absolute file offset,
similar to the server design in NFS v2~\cite{Sandberg-NFS-85}.

While \lego\ supports a hierachical file use interface,
internally, \lego\ storage \microos\ treats (full) directory and file paths just as unique names of a file
and place all files of a \vnode\ under one internal directory at the storage component.
To locate a file, \lego\ storage \microos\ maintains a simple hash table with the full paths of files (and directories) as keys.
From our observation, most datacenter applications only have a few files.
Thus, a simple hash table for a whole \vnode\ is sufficient to achieve good lookup performance.
Using a non-hierachical file system implementation largely reduces the complexity of \lego{}' file system,
making it possible for a storage \microos\ to fit in storage devices controllers that have limited processing power~\cite{Willow}.
%\lego\ uses a simple file system with {\em flat naming} \yizhou{within each \vnode,}
%instead of traditional hierarchical file systems.
%All files in one \vnode\ are placed under the same mount directory. 
%and built a simple file system with flat naming.

\lego\ places the storage buffer cache at memory components
rather than storage ones, because storage devices have limited amount of internal memory.
\lego\ memory \microos\ manages the storage buffer cache by simply performing insertion, lookup, and deletion of buffer cache entries.
For simplicity and to avoid coherence traffic, we currently place the buffer cache of one file
under one memory component.

%storage cache only at storage component, because we don't have hardware coherence across memory components
%when different memory cache the same data, it will create cache coherence issues.
%second because most apps we see and plan to support are using memory-mapped files
%could support read-only memory-side cache


\subsection{Global Resource Management}
\label{sec:grm}
\lego\ uses a two-level resource management mechanism.
At the higher level, \lego\ uses three global resource managers for processor, memory, and storage resources, 
{\em \gpm, \gmm}, and {\em \gsm}.
These global managers perform coarse-grained global resource allocation and load balancing,
and they can run on one normal Linux machine.
Global managers only maintain approximate resource usage and load information.
They update their information either when they make allocation decisions 
or by periodically asking \microos{}s in the cluster.
At the lower level, each \microos\ can employ its own policies and mechanisms to manage its local resources.
%Global managers maintain resource utilization information at a coarse granularity.
%\microos{}s only contact global managers when they need to acquire a 

For example, processor \microos{}s allocate new threads locally 
and only ask \gpm\ when they need to create a new process.
\gpm\ chooses the processor that has the least amount of threads based on its maintained approximate information.
Memory \microos{}s allocate virtual and physical memory space on their own.
Only home memory asks \gmm\ when it needs to allocate a new \vregion.
\gmm\ maintains approximate physical memory space usages and memory access load by periodically asking memory components
and chooses the memory with least load among all the ones that have at least \vregion\ size of free physical memory.
%\gmm\ maintains the coarse information of how much physical memory space 

%migration?
%load balancing

\subsection{Reliability and Failure Handling}
\label{sec:failure}
After disaggregation, processor, memory, and storage components can all fail independently.
To decide how to handle failure in \lego,
%We made this decision based on the observation 
we first observe that most modern datacenter applications
already provide storage data reliability (usually through some form of redundancy).
%Thus, we leave storage reliability handling to applications.
Next, we observe that \lego\ decreases the probability of processor failure,
since it uses fewer processor components than the number of machines in a traditional cluster (\S\ref{sec:cost}).
Therefore, currently \lego\ only handles memory component failure and leaves processor and storage failure handling to applications.
Further, since losing in-memory data will not affect user persistent data,
we propose to provide memory reliability in a best-effort manner.
%targeted usage of \lego\ in modern datacenter applications. 

A naive approach to handle memory failure is to perform a full replication of memory content over two or more memory components.
This method would require at least 2\x\ memory space,
making the monetary and energy cost of providing reliability prohibitively high (the same reason why RAMCloud~\cite{Ongaro11-RamCloud} does not replicate in memory).
Instead, we propose a space- and performance-efficient approach to provide in-memory data reliability in a best-effort way.

We use one primary memory, one secondary memory, and a backup file in storage for each vma.
A memory component can serve as the primary for some vma and the secondary for others.
The primary stores all memory data and metadata. %, as discussed in \S~\ref{sec:}.
\lego\ maintains a small append-only log at the secondary memory component
and also replicates the vma tree there.
When processor flushes a dirty \excache\ line, 
\lego\ sends the data to both primary and secondary in parallel (step \circled{a} and \circled{b} in Figure~\ref{fig-dist-vma})
and waits for both to reply (\circled{c} and \circled{d}).
In the background, the secondary memory flushes the backup log to a storage component,
which writes it to an append-only file.

If the flushing of a backup log to storage is slow and the log is full, 
we will skip replicating application memory.
If the primary fails during this time, \lego\ simply reports an error to application.
Otherwise when a primary memory fails, we can recover memory content 
by replaying the backup logs on storage and in the secondary memory.
When a secondary memory fails, we do not reconstruct anything 
and start replicating to a new backup log on another memory component.

%To improve application performance, we treat memory replication as best effort.
%XXX polishing

%have a way to tell whether or not memory is replicated (and can thus recover from memory failure)

%We do not use the secondary memory for foreground reads for two reasons.
%First, 
%1) no need for further load balancing, since our memory design already spreads across many memory components and have good load balancing
%2) don't want backup's mapping be contending for cache space at memory with primary data

\if 0
%\subsection{Performance Optimization}
%first access to anon pages
\subsection{Putting It All Together}
give an example of memory operation here?

\subsection{Putting It All Together}
%\subsection{Discussion}
benefits
cost
energy
hetero


\fi
\section{\lego\ Implementation}
\label{sec:impl}

We built \lego\ from scratch
and implemented it in C on the x86-64 architecture.
\lego\ can run on commodity, off-the-shelf machines 
and support most commonly-used Linux system call APIs.
%run unmodified Linux ABIs.
Apart from being a proof-of-concept of the \splitkernel\ OS architecture,
our current \lego\ implementation can also be used on existing datacenter servers to reduce the energy cost,
with the help of techniques like Zombieland~\cite{Nitu18-EUROSYS}.
Currently, \lego\ has 206K SLOC,
with 56K SLOC for drivers.


%but ended up using many Linux data structure implementation and interfaces
%good for backward compatibility (drivers)

%\lego\ is a research prototype to demonstrate and validate our design of \splitkernel.
%Although it runs on off-the-shelf servers and support key Linux syscalls and several datacenter applications,
%we do not intend it to be used in large scale or in commodity environments.

%goals
%validate as much design as possible
%working system on real, off-the-shelf machines
%can run existing unmodified applications


\subsection{Hardware Emulation}
Since there is no real resource disaggregation hardware,
we emulate disaggregated hardware components using commodity servers 
by limiting their internal hardware usages.
For example, to emulate controllers for memory and storage components, 
we limit the usable cores of a server to two.
To emulate processors, we limit the amount of usable main memory of a server
and configure it as \lego\ software-managed \excache.

%cache tag and bits stored ana managed by \lego.
%separate DRAM space used for kernel data structures
%We cannot avoid the hardware page table walk in X86.
%We also emulate \excache\ and \vicache\ using main memory as will be discussed in \S\ref{sec:procimpl}.

\subsection{Network Stack}
We implemented three network stacks in \lego.
The first is a customized RDMA-based RPC framework we implemented based on LITE~\cite{Tsai17-SOSP}
on top of the Mellanox mlx4 InfiniBand driver we ported from Linux.
Our RDMA RPC implementation registers physical memory addresses with RDMA NICs 
and thus eliminates the need for NICs to cache physical-to-virtual memory address mappings~\cite{Tsai17-SOSP}.
The resulting smaller NIC SRAM can largely reduce the monetary cost of NICs,
further saving the total cost of a \lego\ cluster. % (\S~\ref{sec:cost}).
All \lego\ internal communications use this RPC framework.
%efficient RPC style communication with zero copy support
For best latency, we use one dedicated polling thread at RPC server side to keep polling incoming requests.
Other thread(s) execute the actual RPC functions. 
%To reduce CPU utilization at memory and storage, we use unsignaled RPC reply. 
%XXX add more details here if we have paper space, eg nowait opt

The second network stack is our own implementation of the socket interface directly on RDMA.
The final stack is a traditional socket TCP/IP stack we adapted from lwip~\cite{lwip} 
on our ported e1000 Ethernet driver.
Applications can choose between these two socket implementations 
and use virtual IPs for their socket communication.

\subsection{Processor Monitor}
\label{sec:procimpl}
%\noindent{\underline{\em Core management and thread scheduling.}}

%\noindent{\underline{\em \excache\ and \vicache\ management.}}
We reserve a contiguous physical memory region during kernel boot time
and use fixed ranges of memory in this region as \excache, tags and metadata for these caches, and kernel physical memory. 

We organize \excache\ into virtually indexed sets with a configurable set associativity.
Since x86 (and most other architectures) uses hardware-managed TLB and walks page table directly after TLB misses, 
we have to use paging and the only chance we can trap to OS is at page fault time. 
We thus use paged memory to emulate \excache, 
with each \excache\ line being a 4\KB\ page.
A \excache\ miss would cause a page fault and trap to \lego.

To minimize the overhead of context switches,
we use the application thread that faults on a \excache\ line miss
to perform \excache\ replacement.
Specifically, this thread will identify the set to insert the missing line
using its virtual memory address,
evict a line in this set if it is full,
and if needed, flush a dirty line to memory 
(via a \lego\ RPC call to the owner memory component of the \vregion\ this line is in).
We piggy-back the request of dirty line flush and new line fetching
in one RPC call to minimize the network round trip needed to complete a \excache\ line miss.
%which then inserts the new cache line (a page) into a set in \excache\ 
%by calculating the set index using its virtual memory address.

\lego\ maintains an LRU list for each \excache\ set 
and uses a background thread to sweep all entries in \excache\ and adjust LRU lists.
%Since \excache\ is critical to \lego{}' performance, 
%we use a dedicated core to keep running the sweep thread
%and use a fine-grained locking mechanism to minimize the overhead of adjusting LRU lists.
%The \vicache\ has a small number of entries and is fully associative for best hit rate.
%On a \vicache\ hit, \lego\ moves the cache line back into \excache.
%On a miss, \lego\  if the cache line is dirty,
%and if so, writes it back to memory.
%If the cache line is clean, we 
%Otherwise, \lego\ simply throws it away.
%Otherwise, we write it back to memory.

\subsection{Memory Monitor}
\label{sec:memimpl}

%\noindent{\underline{\em Fitting to memory hardware.}
%only small processor, wimpy cores
We use regular machines to emulate memory components 
by limiting usable cores to two. %(currently to two core per memory).
We dedicate one core to busy poll network requests 
and the other core to perform all memory operations. 
Doing so not only fits our targeted memory device model,
but also eliminates the need for any synchronization in memory \microos.
\lego\ stores 
%manages address mapping in software with no hardware mapping caches like TLB.
application data, application memory address mappings, vma trees, and \vregion\ arrays all
in main memory of the emulating machine. 

\lego\ memory \microos\ loads an application executable from storage components 
to the memory component, handles application virtual memory address allocation requests,
allocates actual physical memory at the component,
and builds hash tables for the mapping from application process virtual memory addresses to 
local physical memory addresses.
Our current implementation of \lego\ memory \microos\ is purely in software.
While we envision future disaggregated memory components to implement all these functionalities in hardware and
to have specialized hardware parts to store address mappings,
our current software implementation can still be useful for 
users that want to build software-managed disaggregated memory components.

\input{fig-associativity}
\subsection{Storage Monitor}
Since storage is not the focus of the current version of \lego,
we chose a simple implementation of building storage \microos\ on top of the Linux {\em vfs} layer as a loadable Linux kernel module.
\lego\ creates a normal file over vfs as the mount partition for each \vnode\
and issues vfs file operations to perform \lego\ storage I/Os.
Doing so is sufficient to evaluate \lego, while largely saving our implementation efforts on storage device drivers and layered storage protocols.
%\lego{}' current design and implementation of storage components is simple. 
We leave exploring other options of building \lego\ storage to future work.
%For open, processor directly sends request (uid, mode, flags, filepath) to storage component, storage will check permission and create file using mode if O_CREATE set, then return with 0 or -Errno; For write, what real happen is processor trying to copy content from user buffer, if it not in LLC, trigger LLC miss and sends memory with LLC miss and bring the pages from memory to LLC, then processor sends the write request with whole content to memory, then memory just send the same content to storage, storage reply with retval to memory, memory returns same retval to processor. (It works, but I think the content should not send from memory to processor, then back); For read, what real happen is processor send the read request to memory, memory component redirect it to storage, storage reply with return value plus whole content to memory, memory copy the content to buffer in user space, then it reply with processor the same return value and content, processor copy the content to the buffer in user space in LLC; For mmap, when processor read from a page not in LLC, an LLC miss happen and an LLC miss request send to memory, memory handle this fault but sending read request to storage and storage reply with return value plus few pages content (if prefetch enabled), memory alloc pages and copy these contents to them and build the page table mapping from user missing virtual address to kernel virtual address of these pages, finally, memory reply one one page that page fault happened to fill processor's LLC.


\subsection{Experience and Discussion}
%TODO if have room
We built \lego\ from scratch to have a clean design and implementation that 
can fit the \splitkernel\ model
and to evaluate the efforts needed in building different \microos{}s.
We spent a lot of engineering efforts in porting drivers
and eventually decided to re-use common algorithms and data structures in Linux
to easily port Linux drivers.
Another significant chunk of engineering efforts went into 
the implementation Linux system calls, since our implementation needs 
to fit well with \lego{}' design while maintaining Linux ABI.
Currently, \lego\ supports 113 syscalls, 15 pseudo-files,
and 10 vectored syscall opcodes. 
Similar to the findings in ~\cite{tsai-eurosys16}, we found that implementing these Linux interfaces
are sufficient to run many unmodified datacenter applications.

When we started building \lego, we had a clear goal of sticking to
the principle of ``clean separation of functionalities''.
However, we later found several places where performance could be improved 
if this principle is relaxed.
For example, processors need to ask the home memory at every memory access to 
know which memory component it falls to, if processors do not cache the \vregion\ array.
%if memory components are aware of the \excache\ organization,
%they can allocate virtual memory addresses based on their indices into \excache\ sets.
Another example is the implementation of \mremap.
When data in the \mremap\ region is in \excache, 
\lego\ needs to move it to another set if the new virtual address does not fall into the 
current set.
If memory components are \excache-aware, they can choose the new virtual memory address
to fall into the same set as the current one.
Our strategy is to relax the clean-separation principle only by giving ``hints'', %~\cite{XXX},
and only for frequently-accessed,
performance-critical operations.



\if 0
%We have ported Mellanox 4 drivers, the e1000 driver, lwip socket, TCP, UDP, IP network stack
%supporting most file systems system calls, 
%process-related system calls, 
%futex, 
%signal-related system calls.
We built \lego\ from scratch
%built from scratch pros and cons
driver is painful
backward compatibility with Linux, stateful

sacrifice stateless for perf
e.g., \vregion\ array at processor

Linux ABI, syscalls is hard

research prototype, room for improvement
\fi
%\input{tbl-cost-model}
\input{fig-latency-excache-m}
\section{Evaluation}
\label{sec:results}

This section presents the performance evaluation of \lego\ using micro- and macro-benchmarks and two unmodified real applications.
We also quantitatively analyze the resource packing and failure rate of \lego.
We ran all experiments on a cluster of 10 machines, each with two Intel Xeon CPU E5-2620 2.40GHz
processors, 128 GB DRAM, and one 40 Gbps Mellanox ConnectX-3 InfiniBand network adapter;
a Mellanox 40 Gbps InfiniBand switch connects all of the machines. 

\subsection{Micro- and Macro-benchmark Results}
\noindent{\textit{\uline{Network performance.}}}
Network communication is at the core of \lego{}' performance.
Thus, we evaluate \lego{}' network performance first before evaluating \lego\ as a whole.
Figure~\ref{fig-net-latency} plots the average latency of sending messages with socket-over-InfiniBand (IPoIB) in Linux,
\lego{}' implementation of socket on top of InfiniBand (Lego-Sock-o-IB), and \lego{}' implementation of RPC over InfiniBand (Lego-RPC-IB).
\lego\ use Lego-RPC-IB for all its internal network communication across components and use Lego-Sock-o-IB for 
all application-initiated socket network requests.
Both \lego{}' networking stacks significantly outperform Linux's.
%which partially explain why \lego\ outperforms nbdX in \S\ref{sec:appresults}.

\noindent{\textit{\uline{PARSEC results.}}}
We evaluated \lego\ with a set of workloads from the PARSEC benchmark suite~\cite{PARSEC},
including Blackscholes, FreqMine, StreamCluster, Swaptions, and x264.
These workloads are a good representative of datacenter applications, 
ranging from machine-learning algorithms to streaming processing ones.
Figure~\ref{fig-parsec} presents the slowdown of \lego\ 
over single-node Linux with enough memory.
\lego\ uses a fixed \excache\ size of 128\MB\ for these tests and 1, 2, and 4 memory components.
The two streaming workloads, StreamCluster and x264, perform the best because of their 
memory access pattern in batches (each batch around 95\MB\ to 110\MB).
Blackscholes and Freqmine have a larger resident memory size of around 630\MB.
This result suggest that stream processing is a good fit to \lego.
Finally, for these PARSEC workloads, the number of memory components have no or small effects.

\noindent{\textit{\uline{Filebench results.}}}
\fixme{TODO. We are working on getting Filebench results with its varmail, fileserver, and webserver workloads.}

\subsection{Application Performance}
\label{sec:appresults}
We evaluated \lego{}' performance with two real, unmodified applications, 
TensorFlow~\cite{TensorFlow} and Phoenix~\cite{Ranger07-HPCA}, a single-node multi-threaded implementation of MapReduce~\cite{DeanEtAl04-MapReduce}.
TensorFlow's experiments use the MNIST dataset~\cite{MNIST-DS} and Phoenix's use a Wikipedia dataset~\cite{Wiki-DS}.
Unless otherwise stated, the base configuration used for all TensorFlow experiments
is 128\MB\ 64-way \excache, one processor, one memory component, and one storage component.
The base configuration for Phoenix is the same as TensorFlow's with the exception that the base \excache\ size is 512\MB,
The total amount of virtual memory addresses touched in TensorFlow is 2.7\GB\ and 1.75\GB\ for Phoenix.
The total resident memory (minimal amount of memory needed to run an application without swapping) 
for TensorFlow and Phoenix is 0.5\GB\ and 1.7\GB.
Our default \excache\ sizes are set as roughly 25\% of total resident memory.
We ran Phoenix with four threads and use two thread settings for TensorFlow: single thread (TF-1) and four threads (TF-4).

\noindent{\textit{\uline{\excache\ size.}}}
Figures~\ref{fig-tf}, \ref{fig-tf4}, and \ref{fig-phoenix} plot the TensorFlow and Phoenix run time comparison across 
\lego, two remote paging systems (InfiniSwap~\cite{GU17-NSDI} and Mellanox nbdX~\cite{nbdX}), 
single-node Linux (v4.9.47) with a swap file in a local high-end NVMe SSD, and Linux with a swap file in local ramdisk.
All values are calculated as a slowdown to running the applications on a single-node Linux that have enough local resources (main memory, CPU cores, and SSD).
For systems other than \lego, we change the main memory size to the same size of \excache\ in \lego, with rest of the memory on swap file. 
With roughly 25\% resident memory, the overhead of single-threaded TensorFlow, four-thread TensorFlow, and Phoenix on \lego\ are 
29\%, 52\%, and 3.5\x\ compared with monolithic Linux that has all the memory needed for the workload.
\fixme{We are still trying to improve multi-threaded \lego\ performance.}
%\yizhou{But for multi-threaded TF, we saw a 3.5x slowdown. The reason is simple: with multiple threads,
%the program execution becomes much faster, which leads to higher rate of ExCache conflicts.}
%\fixme{How to explain the difference between TF-4 and Phoenix? They are both 4 threads. The explanation is not convincing.}

\lego{}' performance is significantly better than swapping to SSD and to remote memory 
largely because of our efficiently-implemented network stack, simplified code path compared with Linux paging subsystem,
and the optimization technique proposed in \S\ref{sec:zerofill}.
Surprisingly, it is similar or even better than swapping to local memory, even when \lego{}' memory accesses are across network.
This is mainly because ramdisk goes through buffer cache and incurs memory copies between the buffer cache and the in-memory swap file.
%because ramdisk 
%We suspect this is due to the overhead in ramdisk.
%As expected, with less main memory and with smaller \excache, Linux and \lego{}' performances both decrease.

\if 0
Comparing TensorFlow with Phoenix, \lego\ works better with TensorFlow.
This is because TensorFlow has a repeating pattern of memory accesses during its computation.
It first loads the whole dataset into memory. 
Afterwards, it performs computation on a rotating {\em batch\_size} of in-memory data (\eg, 100 images).
This pattern fits well with \lego{}' memory model: a small set of memory with frequent accesses and a large amount of total memory.
\fi

\lego{}' performance results are not easy to achieve and we went through rounds of design and implementation refinement.
Our network stack and RPC optimizations yield a total improvement of up to 50\%.
For example, we made all RPC server (memory components') replies {\em unsignaled} to save memory components' processing time
and to increase its request handling throughput.
Another optimization we did is to piggy-back dirty cache line flush and cache miss fill into one RPC.
The optimization of first anonymous memory access (\S\ref{sec:zerofill}) improves \lego{}' performance further by up to 5\%.

\input{fig-memory-rep}
%\input{tbl-os-comp}
\noindent{\textit{\uline{Number of memory components and replication.}}}
Next, we study the effect of number of memory components and memory replication.
Figure~\ref{fig-mem-rep} plots the performance slowdown as the number of memory components increases from one to four.
Using more memory components improves performance by up to 3\%.
A main reason why the improvement is small and unstable is because these applications perform
most memory allocation in the beginning of their run.
At that time, \lego\ allocates \vregion\ owners with limited information in memory access load.
Migrating allocated data across memory components can potentially solve this problem,
an improvement we leave for future.
%We currently do not support memory migration and leave it as a planned future work.

We also evaluated \lego{}' memory replication in Figure~\ref{fig-mem-rep}.
Overall, our best-effort replication mechanism is almost free in performance (0.1\% to 1\%) and in space (a static 1\MB\ overhead).
We also tested synchronous replication where foreground waits for the write back of backup memory log.
The synchronous replication scheme has an overhead of 1.2\%.

\input{tbl-failure.tex}
\noindent{\textit{\uline{\excache\ configuration.}}}
We now present the results of how \excache\ configuration affects application performance.
Figure~\ref{fig-associativity} plots the performance slowdown and number of \excache\ misses as \excache\ associativity increases.
With higher associativity (\ie, each \excache\ set is bigger), the performance of TensorFlow improves slowly,
but surprisingly, the performance of Phoenix drops.
To understand the cause, we collected the number of \excache\ misses. 
TensorFlow's misses drop as associativity increases, 
while Phoenix's misses stay similar across different associativity.
We found our LRU-list implementation is a major reason why performance decreases as \excache\ associativity increases and \excache\ misses stay the same.
We use a per-set LRU list and lock it when inserting, deleting, and adjusting a \excache\ line's metadata entry in the list.
With multiple threads contending for a set, the locking overhead increases as a set becomes bigger.
We plan to solve this problem by changing the per-set LRU list to a per-core and per-set list in the future.
%which we suspect is because it evenly distribute accesses within its working sets.
%1) the workloads we tested are not sensitive to set associativity mostly because their wss are evenly distributed (which is not a bad thing, cos with small associativity the cost of tag comparison is lower), 2) with higher associativity we pay a higher lock contention overhead, something we can improve by changing to per-core LRU list, for future work

\subsection{Resource Packing}
\label{sec:cost}
To analyze how multiple applications run on \lego\ and how \lego\ allocates resources for them,
we use a simple scenario of running one TensorFlow instance and two Phoenix instances together in three settings:
1) run each instance separately on a monolithic Linux machine with 8 cores and 2\GB\ memory (three machines in total);
2) run one TensorFlow and one Phoenix on one Linux machine of the same type and 
run one Phoenix on a second Linux machine of the same type;
%2) run all three instances together on one Linux machine of the above type;
3) run the three instances on two \lego\ processors (each with 8 cores) and two \lego\ memory components (each with 2\GB\ memory).
The resident memory of a TensorFlow instance is around 0.5\GB\ and that of Phoenix is around 1.7\GB.

In the first setting, all three instances have enough memory and will not trigger any swap. 
We use the performance of TensorFlow and Phoenix in this setting (\ie, each running on a single-node Linux with enough resources)
as the baseline of performance comparison.
This setting achieves the best performance but it comes with the cost of high memory resource waste (2.1/6 = 35\%),
and it requires three full machines.
The second setting uses two Linux machines. 
The first machine's memory is fully utilized while 1.7\GB\ memory on the second machine is utilized, yielding a total of 7.5\% resource waste.
Running one TensorFlow and one Phoenix on the first machine results in memory swap (total resident memory is 2.2\GB, which exceeds the 2\GB\ main memory size),
slowing down TensorFlow's performance by 27\% and Phoenix's performance by 31\%).

With the third setting of a \lego\ cluster, \lego{}' GPM and GMM dynamically allocates CPU cores and memory to 
the three running instances. 
Since \lego\ does not split a process across processors, one TensorFlow process and one Phoenix process
run on one processor, and the other Phoenix process runs on a second processor.
The memory of these three processes are spread across the two memory components,
resulting in total memory utilization of 3.9/4 = 98\% without any memory swapping.
The TensorFlow and Phoenix that run together on the first processor incur run time slowdown of 
5.9\x\ and 1.8\x, while the Phoenix that runs alone on the second processor has a slowdown of 1.5\x.
\fixme{still trying to get better results of co-running TensorFlow and Phoenix.}

This experiment demonstrates \lego{}' advantages in efficient resource packing.
\lego\ can allocate memory from any memory component regardless of where a process runs at.
In contrast, monolithic OSes like Linux have to allocate memory from the same machine where
a running process is at, resulting in either high memory wastes or performance overhead due to memory swapping.

\if 0
Before presenting \lego{}' performance results, we first discuss the estimated monetary cost of 
\lego{}' hardware system and how it compares with monolithic servers.
Table~\ref{tbl-cost} summarizes the market price of a packaged monolithic server 
with similar configuration as the servers we use in our experiments (see above)
and the price of hardware devices in it. 
We list InfiniBand NIC and switch prices separately.
We also estimate the number of components a disaggregated cluster needs to provide the same 
resource as 100 monolithic servers.
We assume that without DIMM/channel bottleneck or other bus bottlenecks, 
each memory component can host twice as much DRAM and every storage component can host two SSDs.

Overall, a disaggregated cluster costs only 50\% of a traditional server cluster.
With fewer CPU and DRAM, it will also significantly save energy cost.
We remind readers that our calculation here is only a rough estimation based on off-the-shelf market price
and on the results of CPU and memory being half utilized in current datacenters.
For example, we assume a 100\% resource utilization with a disaggregated cluster
and calculate the amount of CPU and DRAM it needs to be half of a regular cluster.
We also include a 25\% of disaggregated memory as processor-local \excache.
Although our calculation may not be accurate, but the conclusion and the trend are clear:
with better resource utilization, the total owning and operating costs of a disaggregated cluster is much lower than a monolithic server cluster. 
\fi

\subsection{Failure Analysis}
\label{sec:failure-results}
Finally, we provide a qualitative analysis on the failure rate of a \lego\ cluster compared to a monolithic server cluster.
Table~\ref{tbl-failure} summarizes our analysis.
We use two metrics to measure the failure rate of a cluster: 
1) Mean Time To Permanent (hardware) Failure (MTTPF), the mean time to the permanent failure of a server in a monolithic cluster
or a component in a \lego\ cluster;
2) Mean Time To Failure (MTTF), the mean time to all types of hardware failure (permanent and transient).
%that includes all failures that lead to machine outage, not only
%those that necessitate a hardware replacement.
MTTPF indicates the frequency of hardware replacement in a cluster, 
while MTTF implies application failures that are caused by hardware issues.
For MTTF, we further take into consideration the fault tolerance in a monolithic cluster and in \lego\ (\eg, redundant storage).

To calculate MTTPF (or MTTF) of a monolithic server, we first obtain the replacement frequency of different hardware devices in a server
(CPU, memory, disk, NIC, motherboard, case, power supply, fan, CPU heat sink, and other cables and connections)
from the real world (the COM1 and COM2 clusters in \cite{Failure-Disk-FAST07}).
For \lego, we envision every component to have a NIC and a power supply, 
and in addition, a processor to have CPU, fan, and heat sink, a memory component to have memory, and a storage component to have a disk.
We further assume both a monolithic server and a \lego\ component to fail when any hardware devices in them fails
and the devices in them fail independently.
Thus, the MTTPF (or MTTF) can be calculated using the harmonic mean ({\em HM}) 
of the MTTPF (or MTTF) of each device.
% serial systems of components having
%independent exponentially distributed reliabilities.

\begin{equation}
MTTPF = \frac{HM_{i=0}^n(MTTPF_i)}{n}
\end{equation}

where $n$ includes all devices in a machine/component. MTTF is calculated similarly.

Further, when calculating MTTPF and MTTF of \lego, we estimate the amount of components needed in \lego\ 
to run the same applications as a monolithic cluster.
Our estimated worst case for \lego\ is to use the same amount of hardware devices 
(\ie, assuming same resource utilization as monolithic cluster).
\lego{}' best case is to achieve full resource utilization 
and thus requiring only about half of CPU and memory resources 
(since average CPU and memory resource utilization in monolithic server clusters is around 50\%~\cite{GoogleTrace,AliTrace}).

With better resource utilization and simplified hardware components (\eg, no motherboard),
\lego\ improves MTTPF by 17\% to 49\% compared to an equivalent monolithic server cluster.

\if 0
Best case:
\begin{equation}
MTTF_{Lego} = \frac{HM(MTTF_P, MTTF_M, MTTF_S)}{3}
\end{equation}

Worst case:
\begin{equation}
MTTF_{Lego} = \frac{HM(MTTF_P/2, MTTF_M/2, MTTF_S)}{3}
\end{equation}
\fi

\fixme{TODO: still need to find per-device MTTF numbers}.
\section{Related Work}
\label{sec:related}
%Below we contrast \lego\ to several related works.

%\boldparagraph{Resource Disaggregation.}
\paragraph{Hardware Resource Disaggregation.}
There have been a few hardware disaggregation proposals from academic and industry,
%break the traditional server-centric rack architecture into disaggregated resources.
including Firebox~\cite{FireBox-FASTKeynote}, HP "The Machine"~\cite{HP-TheMachine,HP-MemoryOS}, dRedBox~\cite{dRedBox-DATE},
and IBM Composable System~\cite{IBM-Composable}.
%Firebox~\cite{FireBox-FASTKeynote} is an ongoing project at Berkeley which aims to 
%use custom chips and new instruction set architecture to organize disaggregated racks.
%{\em The Machine} from HP~\cite{HP-TheMachine,HP-MemoryOS} is another ongoing project that builds racks with disaggregated resources and focuses on the usage of SoCs, persistent memory, and a memory-centric OS.
These systems are at their early stage and do not provide or disclose software implementations.
Moreover, they do not consider heterogeneity or failure issues in disaggregated cluster.
%\yizhou{check if they do handle failures.}
%We believe that we are the first to propose a distributed OS for resource disaggregation.
%are likely to adopt the resource pool architecture and 
%use simple high-radix fiber-optic or RDMA network to connect resource pools.
%Our proposed work decomposes racks further into independent resource components 
%and designs a new network layer for this architecture.

\paragraph{Memory Disaggregation and Remote memory.}
Lim \etal\ first proposed the concept of hardware disaggregated memory
with two models of disaggregated memory: using it as network swap device 
and transparently accessing it through memory instructions~\cite{Lim09-disaggregate,Lim12-HPCA}.
Their hardware models still use a monolithic server at the local side. 
\lego's hardware model separates processor and memory completely. %is different.

Another set of recent projects utilize remote memory without changing 
monolithic servers~\cite{Dragojevic14-FaRM,Nelson15-ATC,remote-region-atc18,GU17-NSDI,Novakovic16-SOCC,hotpot-socc17}.
For example, InfiniSwap~\cite{GU17-NSDI} transparently swaps local memory to remote memory via RDMA.
These remote memory systems help improve the memory resource packing in a cluster.
However, as discussed in \S\ref{sec:motivation}, unlike \lego, these solutions cannot solve other limitations 
of monolithic servers like the lack of hardware heterogeneity and elasticity. 

\paragraph{Storage Disaggregation.}
Cloud vendors usually provision storage at different physical machines\cite{deepview-nsdi18,url:aws-storage,url:vmware-vSAN}.
Remote access to hard disks is a common practice, because their high latency and low throughput
can easily hide network overhead~\cite{petal-asplos96,blizzard-nsdi14,Parallax-hotos15,Legtchenko-hotstorage17}.
While disaggregating high-performance flash is a more challenging task~\cite{FlashDisaggregation,url:facebook-lighting}.
Systems such as ReFlex~\cite{ReFlex}, Decibel~\cite{decibel-nsdi17}, and PolarFS~\cite{PolarFS-VLDB18},
tightly integrate network and storage layers to minimize software overhead in the face of fast hardware.
Although storage disaggregation is not our main focus now,
we believe those techniques can be realized in future \lego\ easily.


%\boldparagraph{Operating system design.}
%The concept of distributed operating systems dates back to early 90s. %late 80s and early 90s, with two OSes, Sprite and Amoeba.

\paragraph{Single-Node OSes.}
Multi-kernel OSes like Barrelfish~\cite{Baumann-SOSP09,Barrelfish-DC}, Helios~\cite{Helios-SOSP}, Hive~\cite{Hive-SOSP}, and fos~\cite{fos-SOCC}
treat different cores or programmable devices on their own and run a kernel on each.
Multi-instance OSes like Popcorn~\cite{popcorn-eurosys15} and Pisces~\cite{Pisces-hpdc15} run multiple Linux kernel instances
together on one machine to provide heterogeneous support and performance isolation.
Although these OSes look more like a distributed systems than traditional single-node OSes,
they still sit in a monolithic server and do not deal with issues that can arise in a distributed cluster.
Moreover, none of them separates memory from processor. 

%Several new single-node OSes have been proposed in the recent years in response to various hardware advances.
%Barrelfish~\cite{Baumann-SOSP09,BarrellfishDC} targets many-core computers. 
%It treats each core as an independent entity and replicates kernel states across cores.
%All inter-core communication is through explicit message passing.
%Helios~\cite{Helios} targets heterogeneous processors and uses the satellite kernel concept.
%\lego\ is a new distributed OS in response to a set of datacenter hardware advances.

%fos: single system image of distributed nodes

%Helios sattlelite kernel
%"sattlelite kernel for heterogeneous processors, communication via remote message passing
%Helios introduces satellite kernels, which export a single, uniform set of OS abstractions across CPUs of disparate architectures and performance characteristics. Satellite kernels allow developers to write applications against familiar operating system APIs and abstractions. In addition, Helios extends satellite kernels to NUMA architectures, treating NUMA as a shared-nothing multiprocessor. Each NUMA domain, which consists of a set of CPUs and co-located memory, runs its own satellite kernel and independently manages its resources"

\paragraph{Distributed OSes.}
There have been several distributed OSes built in late 80s and early 90s~\cite{Amoeba-Status,Amoeba-Experience,Sprite,MOSIX,V-System,Accent-SOSP,DEMOS-SOSP,Charlotte}.
%Sprite ... and Amoeba ...~\cite{comparison}
%Sprite 
%distributed client nodes, server nodes, single OS. RPC, shared memory (single address space), maintains client cache coherence, dist (hierarchical) file system, process migration (still need to go back to home machine after migration)
%10-100 workstations, one central file server. 1 user/workstation. homogeneous. sequential workloads. syscall not message passing. 
%distributed file system emphasizes caching (both at server and client) and scalability, block-based
%Migration states (transfer, forward, ignore, global id). residue. migration schemes: stop-and-copy, pre-copy, lazy, sprite server
%
Many of them aim to appear as a single machine to users and focus on improving inter-node IPCs. 
%Moreover, they do not work with 
Among them, the most closely related one is Amoeba~\cite{Amoeba-Status,Amoeba-Experience}.
It organizes a cluster into a shared process pool and disaggregated specialized servers.
Unlike Amoeba, \lego\ further separates memory from processors and different hardware components are
loosely coupled and can be heterogeneous instead of as a homogeneous pool.
There are also few emerging proposals to build distributed OSes in datacenters~\cite{Wolfgang-hotcloud18,Schwarzkopf-apsys13}, 
\eg, to reduce the performance overhead of middleware.
\lego\ achieves the same benefits of minimal middleware layers by only 
having \lego\ as the system management software for a disaggregated cluster
and using the lightweight \vnode\ mechanism.
%abstraction, which
%further provides strong protection and performance isolation.

%a distributed OS designed in the early 90s, builds on the architecture of a pool of processors, specialized servers, and a set of user monitors.
%Its goal is to appear to applications as a single time-shared machine. 
%It uses a microkernel design and runs OS services such as file system at its specialized servers. 
%Because \lego\ builds on the rack-scale architecture of thousands of independent hardware components and targets large-scale datacenter applications,
%it is more focused on performance at large scale, failure isolation, and network optimization.
%Plus, unlike Amoeba, \lego\ has a decomposed memory layer and has to deal with remote memory access issues.
%Compared to Amoeba, \lego\ faces a different set of challenges, hardware and applications.
%10-100 CPUs per user. goal is to appear as a single time-shared machine to applications and run parallel applications. architecture: a shared process pool, specialized servers (File, DB, etc.), monitors, one gateway to WAN
%microkernel on each node, kernel-managed user threads, memory management using segments, thread communication between two nodes with RPC (user-to-user, user-to-kernel, kernel-to-kernel). 
%Most of the traditional operating system services (such as the directory server) in Amoeba are implemented as server processes,
%object-based with capability, e.g., capability for each memory segment by memory server, capability for each process for process server
%the choice of file system is not dictated by the operating system. The file system runs as a collection of server processes
%difference: scale, failure, hetero, applications(abstraction)


\if 0
%\boldparagraph{Remote and distributed memory systems.}
There is also increasing interest in memory-based or memory-centered systems.
%DSM
%Researchers and system builders have developed a host of software and hardware \dsm\ systems in the past few 
%decades~\cite{Gibbons91-SPAA,Kontothanassis97-ISCA,Lo94-AC,Kessler89-ACM,Stumm90-IEEE,Black89-COMPCON,Keleher92-ISCA,Fleisch89-SOSP,Delp:1988:AIM:59505,Bisiani90-ISCA,Ramachandran91-Wiley,Zhou92-IEEE,Bennett90-PPOPP,Zhou92-IEEE,Stumm90-IEEE,Stumm90-IPDPS}.
%However, usage of these \dsm\ systems has been limited, 
%mainly because their high network and software overheads are not acceptable for most memory-based applications.
%Recently, with the increasing popularity of low-latency RDMA networks in data centers~\cite{Kalia14-SIGCOMM,Wei15-SOSP,Dragojevic14-NSDI}, 
%there is a new interest in \dsm~\cite{Nelson15-ATC} to support modern data-intensive applications.
Grappa~\cite{Nelson15-ATC} is a recent DSM system that supports 
modern data-parallel applications.
Instead of fetching remote memory to a local cached copy,
Grappa executes functions at the remote side.
%RAMCloud~\cite{Ongaro11-RamCloud} is a storage system that 
%replicates data in the storage layer and doesn't maintain any replica in memory.
\lego\ is a full-scale OS, whose decomposed architecture results in various different design decisions in its memory layer. 
\fi
\section{Discussion and Conclusion}
\label{sec:conclude}

We presented \lego, the first OS designed and built from scratch for hardware resource disaggregation.
\lego\ demonstrated the feasibility of resource disaggregation and its advantages in perf/\$,
heterogeneity, and elasticity, all without changing Linux ABIs.
\lego\ and resource disaggregation in general can help the adoption of new hardware
and thus encourage more hardware and system software innovations.  

\lego\ is a research prototype and has a lot of room for improvement.
For example, our initial investigation in load balancing 
shows that memory allocation policies across memory components can affect application performance.
However, since we do not support memory data migration yet, 
the benefit of our load-balancing mechanism is small.
We leave memory migration for future work.
In general, large-scale resource management of a disaggregated cluster is 
an interesting and important topic, but is out side of the scope of this paper.

\if 0
organizing as cache
no coherence across processors or memory
users know the nature of distributed computation, but not that memory, storage are disaggregated
help hardware and system software innovations
Linux
user interface programming model
new abstraction
focus on mechanism, leave policy for future
\fi
\section*{Acknowledgments}

We would like to thank the anonymous reviewers and our shepherd Michael Swift
for their tremendous feedback and comments, which have
substantially improved the content and presentation of this paper.
We are also thankful to Sumukh H. Ravindra for his contribution in the early
stage of this work.
This material is based upon work supported by the National
Science Foundation under the following grant: NSF 1719215.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not 
necessarily reflect the views of NSF or other institutions.

  %%%%%%%%%%%%%%%%%%%%%%%%5
%\clearpage
%\pagestyle{empty}
%\input{department}
%\clearpage






%\setstretch{1}
%\bibliography{ws1,ws2,asplos,fpga,full,hot-chips,immd4,microchip,other,BuildingStuff}

\setstretch{0.8}
\titlespacing*{\section}{0em}{1ex}{1ex}
\begin{small}

%\bibliographystyle{ieeetr}
\begin{spacing}{0.3}
\bibliographystyle{abbrv}
%\bibliographystyle{mbt_dj}
\bibliography{all-defs,all,personal,all-confs,local,paper}
\end{spacing}
\end{small}

%\clearpage
%\input{bio}
%\clearpage
%\input{budget}
%\clearpage
%\input{facilities}

\end{document}







