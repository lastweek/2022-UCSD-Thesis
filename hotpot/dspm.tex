\section{Distributed Shared Persistent Memory}
\label{sec:dspm}

The datacenter application and hardware trends described in Section~\ref{sec:motivation} 
clearly point to one promising direction of using \nvm\ in datacenter environments --- 
as distributed, shared, persistent memory (\dsnvm).
A \dsnvm\ system manages a distributed set of \nvm{}-equipped machines  
and provides the abstraction of a global virtual address space and a data persistence interface to applications.
This section gives a brief discussion on the \dsnvm\ model.

\subsection{\dsnvm\ Benefits and Usage Scenarios}
\dsnvm\ offers low-latency, shared access to vast amount of durable data in distributed \nvm,
and the reliability and high availability of these data.
Application developers can build in-memory data structures with the global virtual address space 
and decide how to name their data and when to make data persistent.

Applications that fit \dsnvm\ well have two properties:
accessing data with memory instructions and making data durable explicitly.
We call the time when an application makes its data persistent a {\em commit point}.
There are several types of datacenter applications that meet the above two descriptions and can benefit from running on \dsnvm.

First, applications that are built for single-node \nvm\
can be easily ported to \dsnvm\ and scale out to distributed environments.
These applications store persistent data as in-memory data structures 
and already express their commit points explicitly.
Similarly, storage applications that use memory-mapped files also fit \dsnvm\ well,
since they operate on in-memory data and explicitly make them persistent at well-defined commit points (\ie, \msync).
Finally, \dsnvm\ fits shared-memory or DSM-based applications that desire to incorporate durability.
These applications do not yet have durable data commit points,
but we expect that when developers want to make their applications durable, 
they should have the knowledge of when and what data they want make durable.

\subsection{\dsnvm\ Challenges}
\label{sec:challenges}
Building a \dsnvm\ system presents several new challenges.

First, {\em what type of abstraction should \dsnvm\ offer to support both direct memory accesses and data persistence (Section~\ref{sec:abstraction})}?
To perform native memory accesses, application processes should use virtual memory addresses. 
But virtual memory addresses are not a good way to {\em name} persistent data.
\dsnvm\ needs a naming mechanism that applications can easily use to retrieve their in-memory data after reboot or crashes (Section~\ref{sec:naming}).
Allowing direct memory accesses to \dsnvm\ also brings another new problem:
pointers need to be both persistent in \nvm\ and consistent across machines (Section~\ref{sec:addressing}).

Second, {\em how to efficiently organize data in \dsnvm\ to deliver good application performance (Section~\ref{sec:data})?}
To make \dsnvm's interface easy to use and transparent, 
\dsnvm\ should manage the physical \nvm\ space for applications and handle \nvm\ allocation.
\dsnvm\ needs a flexible and efficient data management mechanism to deliver good performance to different types of applications.

Finally, {\em \dsnvm\ needs to ensure both distributed cache coherence and data reliability at the same time} (Section~\ref{sec:xact}).
The former requirement ensures the coherence of multiple cached copies at different machines under concurrent accesses and is usually enforced in a distributed memory layer.
The latter provides data reliability and availability when crashes happen and is implemented in distributed storage systems or distributed databases.
\dsnvm\ needs to incorporate both these two different requirements in one layer in a correct and efficient way.
%Note that PM is attached to main memory bus directly, hence we assume PM share the same CPU cache coherence mechanism with DRAM.
%Hotpot focus on cache coherence among different cached copies across nodes.
