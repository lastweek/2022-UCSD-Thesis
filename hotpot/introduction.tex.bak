\section{Introduction}
\label{sec:introduction}

%scalability

Next-generation non-volatile memories ({\em NVMs}), 
such as 3DXpoint~\cite{Intel3DXpoint}, 
phase change memory ({\em PCM}),
spin-transfer torque magnetic memories ({\em STTMs}),  and the memristor
will provide byte addressability, persistence, high density, and DRAM-like performance~\cite{NVMDB}.
%latency that is within 
%an order of magnitude of 
%DRAM~\cite{NVMDB}. %,hosomi2005novel,johnson2004phase,lee2011fast,yang2013memristive,Lee10-pcmquest,lee2010phase,pcmdataasheet,qureshi2010morphable}.
These developments are poised to radically alter the landscape of memory and storage technologies
and have already inspired a host of research 
projects~\cite{Bailey10-OSImpl,Coburn11-ASPLOS, sosp09:bpfs, Dulloor14-EuroSys, hotos09:mogul, Volos11-ASPLOS, Xiaojian11-SC,Zhang15-Mojim,Octopus}.
However, most previous research on NVMs has focused on using them in a single machine environment.
Even though NVMs have the potential to greatly improve the performance and reliability of large-scale applications,
it is still unclear how to best utilize them in distributed, datacenter environments. 
%while most mission-critical data resides in distributed, datacenter environments. 
%For NVMs to succeed as a first-class storage technology, it must find its way to these environments.
%provide the reliability and availability that these storage systems require~\cite{Pinkerton14-Talk}.
%
%The benefits of \nvm{}s are especially apparent in datacenter environments,
%where many large-scale applications require fast access to vast amounts of persistent data.
%However, using traditional distributed storage systems designed for slower storage devices on NVMs will result
%in excessive software and network overheads that outstrip NVMs' low latency performance~\cite{Zhang15-Mojim}.

This paper takes a significant step towards the goal of using NVMs in distributed datacenter environments.
We propose {\em Distributed Shared Persistent Memory (\dsnvm)},
a framework that provides a global, shared, and persistent memory space 
using a pool of machines with NVMs attached at the main memory bus (\S\ref{sec:dspm}).
%to form a global, shared, and persistent memory space.
Applications can perform native memory load and store instructions to access both local and remote data in this global memory space 
and can at the same time make their data persistent and reliable.
\dsnvm\ can benefit both single-node persistent-data applications that want to scale out efficiently
and shared-memory applications that want to add durability to their data.

%benefits
%one layer
% one form
%It is clear that modern datacenter applications need to store, access, and share 
%more data than is feasible with a single machine.
%To meet these application needs, we propose using \nvm\ in  datacenter environments 
%as distributed, shared, persistent memory (\dsnvm).
%%\dsnvm\ is a perfect response to these three datacenter trends.
%\dsnvm\ can meet all these application needs. % and fit perfectly in datacenter environments.
%A \dsnvm\ system manages a distributed set of \nvm{}-equipped machines  
%and provides an abstraction that lets applications access and share persistent data in these \nvm{}s.
Unlike traditional systems with separate memory and storage layers~\cite{Larchant,Perdis00,Larchant94,PerDis},
we propose to use just one layer that incorporates both distributed memory and 
distributed storage in \dsnvm.
\dsnvm's one-layer approach eliminates the performance overhead of data marshaling and unmarshaling,
and the space overhead of storing data twice. 
%By using only one layer that provides both distributed persistent storage 
%and distributed shared memory access, 
With this one-layer approach, \dsnvm\ can provide the low-latency performance, 
vast persistent memory space, data reliability, and high availability
that many modern datacenter applications demand. 
%It will also provide an easy-to-use abstraction that can meet modern datacenter applications' demand 
%for shared memory access, data reliability, and high availability. 


%challenge
Building a \dsnvm\ system presents its unique challenges.
Adding ``Persistence'' to Distributed Shared Memory (DSM)
is not as simple as just making in-memory data durable.
Apart from data durability, \dsnvm\ needs to provide two key features that DSM does not have:
persistent naming and data reliability.
%Since our targeted usage of \dsnvm\ in datacenters is to store application persistent data,
%\dsnvm\ needs to provide an easy-to-use abstraction 
%to deliver the same data reliability guarantees as datacenter distributed storage systems.
%XXX
Applications should be able to easily
name, close, and re-open their in-memory data structures 
and directly access them via memory loads and stores.
%to access their data like in memory,
User data should also be reliably stored in NVM and sustain various types of failures; %(\eg, to have $N$ copies of persistent data).
they need to be consistent both within a node and across distributed nodes after crashes.
To make it more challenging, 
\dsnvm\ has to deliver these guarantees without sacrificing application performance
in order to preserve the low-latency performance of NVMs.
%and to support the large scale of datacenter applications.

\if 0
While adding these two things, we cannot
For example, how do applications access and use \dsnvm? 
What type of addresses should \dsnvm\ use? 
What naming mechanisms can applications use to store and retrieve their persistent data in \dsnvm?
How to scale \dsnvm\ to many nodes in data centers?
%How to ensure the coherence of multiple copies of data under concurrent accesses to \dsnvm?
How to ensure data reliability and high availability when errors happen in \dsnvm?
How to minimize performance overhead and exploit the most from \nvm's low latency performance?
\fi

We built {\em \hotpot}, the first \dsnvm\ system, in the Linux kernel.
\hotpot\ offers low-latency, transparent memory access, data persistence, reliability, and
high availability to datacenter applications.
It exposes a global virtual memory address space to a user application
and uses a new persistent naming mechanism that is both easy-to-use and efficient (\S\ref{sec:abstraction}).
Internally, \hotpot\ uses a flexible mechanism to organize and manage data in \dsnvm\
and a set of adaptive resource management techniques to achieve 
better performance and scalability (\S\ref{sec:data}).

To efficiently provide data reliability with distributed shared memory access, 
we propose two main ideas in \hotpot.
The first is to integrate distributed memory caching and data replication 
by imposing {\em morphable} states on memory pages in \dsnvm.

In DSM systems, when an application on a node ({\em actively}) accesses shared data in remote memory,
DSM caches these data copies in its local memory for fast accesses
and later evicts them when reclaiming local memory space.
Like DSM, \hotpot\ also caches application-accessed data in local memory
and ensures the coherence of multiple cached copies on different nodes.
%But instead of throwing the cached copies away during eviction, 
But \hotpot\ also uses them as persistent data replicas, ensures their {\em crash-consistent} state,
and avoids evicting them.
%for better data reliability.

On the other hand, unlike distributed storage systems, which {\em creates} extra data replicas 
to meet user-specified reliability requirements, 
\hotpot\ makes use of data copies that {\em already exist} in the system when
they were fetched to a local node due to application memory accesses.
 
In essence, every local copy of data serves two simultaneous purposes.
First, applications can access it locally without any network delay.
Second, by placing the fetched copy in persistent memory ({\em \nvm}), it can be treated as a persistent replica 
for data reliability.

This seemingly-straightforward integration is not simple. 
Maintaining wrong or outdated versions of data can result in inconsistent data.
To make it worse, these inconsistent data will be persistent in \nvm.
We carefully designed a set of protocols to deliver data reliability and crash consistency guarantees 
to applications while integrating memory caching and data replication (\S\ref{sec:xact}).
%and provides several consistency modes (\S\ref{sec:xact}).
%Applications can use this transactional interface both to push updated data to all the cached copies in \hotpot\ 
%and to replicate data for reliability and high availability.

\input{fig-share-anal}
Our second idea is to exploit application behaviors and intentions in the \dsnvm\ setting. 
Unlike traditional memory-based applications, persistent-data-based applications,
\dsnvm's targeted type of application, have well-defined data {\em commit points}
where they specify what data they want to make persistent.
When a process in such an application makes data persistent,
it usually implies that the data can be {\em visible} outside the process (\eg, to other processes or other nodes). 
\hotpot\ utilizes these data commit points to also push updates to cached copies on distributed nodes
to avoid maintaining coherence on every \nvm\ write. %~\cite{XXX},
%it is not necessary to maintain memory coherence for each memory access;
%\hotpot\ treats these coherence copies as persistent data replicas
%and makes extra redundant copies to meet the degree of replication specified by users.
%Conducting memory coherence events only at data commit points 
Doing so greatly improves the performance of \hotpot, 
while still ensuring correct memory sharing and data reliability.
%delivering the sharing, reliability, and availability required by \hotpot\ applications. 

%As another example, we employ a replica placement and load balancing technique that relies heavily on application behavior.  


\if 0
\hotpot\ transparently supports memory load and store accesses to both local and remote \nvm\ 
by intercepting the OS page fault handler.
\hotpot\ also supports pointers in \nvm\ and allows applications to use these pointers directly on any \hotpot\ nodes
without any pointer marshaling and unmarshaling.
To let applications name their persistent datasets, \hotpot\ uses a flat namespace with much smaller 
metadata overhead than traditional file system namespaces.

There are no fixed locations for data
eviction: let application access determine
for cold data, goal is to minimize thrashing and minimize impact on hot data, foreground perf
\fi

To demonstrate the benefits of \hotpot, we ported a NoSQL database, MongoDB~\cite{MongoDB}, to \hotpot\
and built a distributed graph engine similar to PowerGraph~\cite{Gonzalez12-OSDI} on \hotpot\ (\S\ref{sec:app}). 
%We evaluated \hotpot\ with micro-benchmarks, macro-benchmarks, and a real-world large-scale graph.
Our MongoDB evaluation results show that \hotpot\ outperforms a \nvm-based system~\cite{Zhang15-Mojim} by up to 3.1\x{}, 
two NVM-based file systems~\cite{Dulloor14-EuroSys,Octopus} by up to 787\x{}, and a DRAM-based file system by up to 53\x{}. 
\hotpot\ outperforms PowerGraph by 2.3\x{} to 5\x{}, a recent DSM system~\cite{Nelson15-ATC} by 1.3\x{} to 3.2\x{},
and two DSM systems that we built by TODO.
%even though PowerGraph adopts sophisticated graph partition techniques 
%and uses two dedicated threads per machine for communication. 
%52\% and 66\% on two real-world graph datasets.
Moreover, \hotpot\ delivers stronger data reliability and availability guarantees than these alternative systems.

Overall, this paper makes the following key contributions:

\begin{itemize}
\item We are the first to introduce the Distributed Shared Persistent Memory (DSPM) model
and among the first to build distributed persistent memory systems.
The DSPM model provides direct and shared memory accesses to a distributed set of \nvm{}s 
and is an easy and viable way for datacenter applications to use \nvm.

\item We propose a one-layer approach to build \dsnvm\ by 
integrating memory coherence and data replication.
The one-layer approach avoids the performance cost of two or more indirection layers.

\item We design two distributed data commit protocols with different consistency levels 
and corresponding recovery protocols to 
ensure data durability, reliability, and availability.

\item We build the first \dsnvm\ system, \hotpot, in the Linux kernel, 
and two traditional kernel-level DSM systems as comparison points to \hotpot. 
We open source \hotpot\ and the two DSM systems.

\item We demonstrate \hotpot's performance benefits and ease of use with two real datacenter applications
and extensive microbenchmark evaluation. 
We compare \hotpot\ with five existing file systems and distributed memory systems, 
and two DSM systems that we built.

\end{itemize}

The rest of the paper is organized as follows.
Section 2 provides some background on PM and recent trends toward DSPM.
We discuss the benefits and challenges of DSPM in Section 3.
Section 4 presents the architecture and abstraction of Hotpot.
We then discuss Hotpot's data management in Section 5.
In Section 6, we present Hotpot's transaction design,
and Section 7 discuesses the network layer.
Section 8 presents detailed evaluation of Hotpot.
We cover related work in Section 9, and conclude in Section 10.







\if 0
In summary, we make the following contributions:
\begin{itemize}[noitemsep]
\item We introduce the DSPM model, which provides direct and shared memory access to a distributed set of PMs.
\item We integrate memory coherence and data replication in one layer.
\item We build the first DSPM system, \hotpot, and demonstrated its benefits with modern data-intensive applications.
\end{itemize}


The rest of the paper is organized as follows. Section 2 discusses the background and recent trends toward DSPM.
Section 3 presents the architecture and detailed design of Hotpot. Section 4 presents the ported applications and
evaluation of Hotpot. Section 5 discusses related work. And section 6 concludes.

\dsnvm{}s require both memory coherence {\em and} data replication.
Because applications can access and share \nvm\ like memory, 
\dsnvm{}s need to provide the coherence across multiple copies of data at different nodes' \nvm{}s.
At the same time, applications can also treat these data in \nvm{}s as persistent data, 
which necessitates data replication to provide reliability and high availability.

We propose to integrate memory coherence and data replication in \dsnvm, 
in order to meet the above requirements and to reduce the performance overhead 
caused by redundant indirection~\cite{Nameless}.
When an application process on a node accesses a remote \nvm\ page,
\dsnvm\ will fetch the memory page to the \nvm\ in the local node. 
This local copy serves two simultaneous purposes.
First, the application process can access it locally without any network delay.
Second, by placing the fetched copy in \nvm, it can be treated as a persistent replica 
of the remote \nvm\ page.
To further reduce performance overhead, we will design a set of new mechanisms
to handle memory coherence and data replication at the same time without
extra indirection.
\fi


\if 0
In summary, we make the following contributions.

%\begin{itemize}
\begin{itemize}[noitemsep]
\item We are the first to introduce the \dsnvm\ model, which provides direct and shared memory access to a distributed set of \nvm{}s.

\item To the best of our knowledge, we are the first to integrate memory coherence and data replication.

\item We built the first \dsnvm\ system, \hotpot,
and demonstrated its benefits with modern data-intensive applications.

\end{itemize}
\fi



















\if 0
A unique challenge among these issues is the problem of memory coherence and data replication. 

Although both memory coherence and data replication involve multiple copies of data
and provide various levels of consistency across those copies,
these two techniques have several fundamental differences.
%including different . 
For example, 
when application processes or threads share data and cache multiple copies of that data, 
memory coherence techniques passively maintain consistency across these copies.
Data replication, on the other hand, proactively makes replicas of data to provide redundancy.
Coherence techniques are designed for volatile data,
and clean cached data can be safely thrown away,
while data replication is designed for persistent data and losing data in storage systems is catastrophic.
\fi


%\hotpot\ the abstraction of 
%Applications running on \dsnvm\ directly access \nvm\ with memory load and store 
%instructions for both local and remote \nvm.
%We give full control of how users want to use PM to applications.
% which provides both the ease and performance of direct 
%memory accesses 
%and the reliability and availability of distributed storage systems.

%how to ensure different versions (replica, coherence) do not affect each other, 
%and can convert from each other
%targeted applications


%Combining coherence and replication is not straightforward and has many challenges
%one challenge we met is
%For example, in traditional data replication, once a data has been replicated, 

%one potential problem with \dsnvm\ is


\if 0
Extending this basic idea, we will build a \dsnvm\ system, \hotpot, 
to provide applications with easy-to-use abstraction, native memory accesses, 
low-latency performance, data reliability, consistency, and availability.
We will implement \hotpot\ in Linux on a real Infiniband-based cluster in the PI's lab
and evaluate it with micro-benchmarks and modern data center applications.

Memory coherence and data replication are two important techniques in memory and storage systems.

Memory coherence (or cache coherence) guarantees the consistency of multiple copies of cached data through concurrent accesses.
%for example, by pushing changes from one copy to other copies of the data.
Over the past few decades, researchers have proposed many coherence 
protocols~\cite{Gharachorloo90-ISCA,Gibbons91-SPAA,Kontothanassis97-ISCA,Katz85-ISCA,Gamsa99-OSDI,Srbljic97-IEEE,Mellor-Crummey91-ACM,Tartalja95-HICSS,Gharachorloo90-ISCA,Keleher92-ISCA,Lenoski90-ISCA,Dubois88-IEEE,Li89-ACM,Tomasevic94-IEEE}
%with different tradeoffs of consistency level, performance, and scalability~\cite{}.
and these have been used extensively in local and distributed shared memory systems.

Data replication adds redundancy to persistent data to provide data reliability and availability
in the event of device or machine failure.
Many different data replication techniques  
%offer different levels of data consistency, availability, and performance
are used in local and distributed storage
systems~\cite{AdyaEtAl-Farsite,calder11-azure,DeCandia+07-Dynamo,Ghemawat03-GoogleFS,Gray96-Danger,HellersteinEtAl94-Coding,KubiEtAl00-Ocean,PattersonEtAl88-RAID,Petersen97-Bayou,Rhea03-Pond,Rowstron01-PAST,vanRenesse04-OSDI,Zhong08-Replication}.
%where we intentionally create multiple copies of the data to provide reliability and availability.

Both memory coherence and data replication involve multiple copies of data
and provide various levels of consistency across those copies.
However, the two techniques have several fundamental differences. 

First, memory coherence and data replication have different rationales and use cases.
When application processes or threads share data and cache multiple copies of that data, 
memory coherence techniques passively maintain consistency across these copies.
Data replication, on the other hand, proactively makes replicas of data to provide redundancy.
Second, coherence techniques are designed for volatile data, 
while data replication is designed for persistent data.
Clean cached data can be safely thrown away, while losing data in storage systems is catastrophic.
Finally, memory coherence and data replication operate at different levels of granularity:  
memory coherence granularity exists at the level of a cache line or a memory page, 
while data replication provides granularity at the level of data blocks.


Because of these differences, memory coherence and data replication have always been viewed as orthogonal techniques,
and design decisions reflect that distinction. 
For example, most memory coherence implementations take place in hardware, because memory coherence events usually happen with every memory access
and thus require very low performance overhead.
%Low-latency memory coherence events are thus critical to application performance, 

On the other hand, data replication usually happens when applications commit a set of data (\eg, with \fsync).
Since storage devices are much slower than memory, considerations such as transactional support and 
fast crash recovery often take priority over low-latency replication performance.
As a result,  data replication is most often implemented in user-level software.

%Thus, data replication needs to maintain a certain degree of redundancy for all the persistent data, 
%while there is no such requirement for memory coherence.
%For example, during cache or memory replacement, we can just through away clean data.
%But if we lose or through away a replica, then the replication degree cannot meet user requirements or sustain enough failure.
%there has always been a clean separation between memory and storage, 
%With the advent of \nvm, we raise the question if we should and how to combine coherence and replication.

Recently, emerging disruptive hardware technologies have challenged this long-standing separation between coherence and replication.
Next-generation fast, byte-addressable, non-volatile memories~\cite{NVMDB}
such as 3DXpoint~\cite{Intel3DXpoint}, phase change memory ({\em PCM}),
spin-transfer torque magnetic memories ({\em STTMs})~\cite{SamsungSTTM}, and the memristor~\cite{HP-TheMachine}
can be attached to the main memory bus to form persistent main memory ({\em \nvm}).
%\nvm{}s are blurring the line between memory and storage.

\nvm{}s require both memory coherence {\em and} data replication.
Because applications can access \nvm\ like memory, \nvm{}s need to provide memory coherence across multiple copies of data.
At the same time, applications can also store persistent data in \nvm, 
which necessitates data replication to provide reliability and high availability.
The need for both memory coherence and data replication is especially 
acute in distributed datacenter environments, where large-scale applications
access and share a vast amount of data that are distributed across nodes.
%These environments raise the need for both memory coherence and data replication.
%and the separation of memory coherence and data replication.
%Applications can access \nvm{}s with memory load and store instructions
%and at the same time use \nvm{}s to store persistent data.
%Such usage scenarios raise the need for both memory coherence and data replication.
%This paper explores the challenges of providing coherence and data replication to \nvm.

We introduce {\em Distributed Shared Persistent Memory (\dsnvm)}, 
a framework that uses a pool of machines with \nvm{}s to form a global, shared, and persistent memory space.
Applications can perform traditional memory load and store instructions to access both local and remote data in this global memory space 
and can at the same time make their data persistent and reliable.
\dsnvm\ provides an easy-to-use model that can potentially meet modern datacenter applications' demand for low-latency, shared memory access, data reliability, and high availability. 
A fundamental challenge of \dsnvm\ is efficiently maintaining memory coherence and
data replication, while providing performance that is close to local \nvm\ in spite of network delay.

We propose to integrate memory coherence and data replication in \dsnvm, 
in order to meet the above requirements.
When an application process on a node accesses a remote \nvm\ page,
\dsnvm\ will fetch the memory page to the \nvm\ in the local node. 
This local copy serves two simultaneous purposes.
First, the application process can access it locally without any network delay.
Second, by placing the fetched copy in \nvm, it can be treated as a persistent replica 
of the remote \nvm\ page.
%The only difference between coherent and redundant copies is that
%coherent copies have been accessed by applications and thus are visible to applications, 
%while redundant copies 
\fi

%The rest of the paper is organized as follows. 
%Section~\ref{}
